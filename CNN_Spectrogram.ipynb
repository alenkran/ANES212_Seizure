{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test CNN on spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4c42306910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seed = 42\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize data into something useable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_kfold(train_percent, k_fold=1):\n",
    "    features_og = np.load('features_nonsliding_ch.npy')\n",
    "    targets_og = np.load('targets_nonsliding_ch.npy')\n",
    "    print(sum(targets_og),targets_og.shape)\n",
    "\n",
    "    # Split data into train and test\n",
    "    split = np.arange(len(targets_og))\n",
    "    np.random.shuffle(split)\n",
    "    features_og = features_og[split]\n",
    "    targets_og = targets_og[split]\n",
    "    features_og_0 = features_og[np.where(targets_og==0)[0]]\n",
    "    features_og_1 = features_og[np.where(targets_og==1)[0]]\n",
    "    targets_og_0 = targets_og[np.where(targets_og==0)[0]]\n",
    "    targets_og_1 = targets_og[np.where(targets_og==1)[0]]\n",
    "    N_0 = len(targets_og_0)\n",
    "    N_1 = len(targets_og_1)\n",
    "    \n",
    "    features_og_train = np.vstack([features_og_0,features_og_1])\n",
    "    targets_og_train = np.vstack([targets_og_0,targets_og_1])\n",
    "    sample_list = []\n",
    "    for i in range(k_fold):\n",
    "        if (i+1)==k_fold:\n",
    "            temp1 = np.arange(i*N_0//k_fold,N_0, dtype=np.int64)\n",
    "            temp2 = np.arange(i*N_1//k_fold,N_1, dtype=np.int64)+N_0\n",
    "            temp2 = np.repeat(temp2,100)\n",
    "        else:\n",
    "            temp1 = np.arange(i*N_0//k_fold,(i+1)*N_0//k_fold, dtype=np.int64)\n",
    "            temp2 = np.arange(i*N_1//k_fold,(i+1)*N_1//k_fold, dtype=np.int64)+N_0\n",
    "            temp2 = np.repeat(temp2,100)\n",
    "        sample_list.append(np.hstack([temp1,temp2]))\n",
    "    \n",
    "    train_sampler_list = []\n",
    "    test_sampler_list = []\n",
    "    for i in range(k_fold):\n",
    "        temp = np.where(np.arange(len(sample_list), dtype=np.int64) != i)[0]\n",
    "        train =  np.hstack([sample_list[x] for x in temp])\n",
    "        test = sample_list[i]\n",
    "        train_sampler_list.append(SubsetRandomSampler(train))\n",
    "        test_sampler_list.append(SubsetRandomSampler(test))\n",
    "        \n",
    "    # Convert data to tensor dataset\n",
    "    features = torch.from_numpy(features_og_train).float()\n",
    "    targets = torch.from_numpy(targets_og_train).long()\n",
    "    targets = torch.squeeze(targets)\n",
    "    train = data_utils.TensorDataset(features, targets)\n",
    "    \n",
    "    return train_sampler_list, test_sampler_list, train\n",
    "    \n",
    "def split_train_test(train_percent, k_fold=0):\n",
    "    features_og = np.load('features_nonsliding_ch.npy')\n",
    "    targets_og = np.load('targets_nonsliding_ch.npy')\n",
    "    print(sum(targets_og),targets_og.shape)\n",
    "\n",
    "    # Split data into train and test\n",
    "    split = np.arange(len(targets_og))\n",
    "    np.random.shuffle(split)\n",
    "    features_og = features_og[split]\n",
    "    targets_og = targets_og[split]\n",
    "    features_og_0 = features_og[np.where(targets_og==0)[0]]\n",
    "    features_og_1 = features_og[np.where(targets_og==1)[0]]\n",
    "    targets_og_0 = targets_og[np.where(targets_og==0)[0]]\n",
    "    targets_og_1 = targets_og[np.where(targets_og==1)[0]]\n",
    "    N_0 = len(targets_og_0)\n",
    "    N_1 = len(targets_og_1)\n",
    "    \n",
    "    features_og_train = np.vstack([features_og_0[:int(train_percent*N_0)],features_og_1[:int(train_percent*N_1)]])\n",
    "    targets_og_train = np.vstack([targets_og_0[:int(train_percent*N_0)],targets_og_1[:int(train_percent*N_1)]])\n",
    "    features_og_test = np.vstack([features_og_0[int(train_percent*N_0):],features_og_1[int(train_percent*N_1):]])\n",
    "    targets_og_test = np.vstack([targets_og_0[int(train_percent*N_0):],targets_og_1[int(train_percent*N_1):]])\n",
    "    print(sum(targets_og_train), sum(targets_og_test))\n",
    "    \n",
    "\n",
    "    # Balance dataset\n",
    "    # ~1/4000 seizure events\n",
    "    idx = np.hstack([np.where(targets_og_train == 0)[0], \n",
    "                     np.repeat(np.where(targets_og_train == 1)[0], 100)]) # Oversample\n",
    "    features = features_og_train[idx]\n",
    "    targets = targets_og_train[idx]\n",
    "\n",
    "    # Convert data to tensor dataset\n",
    "    features = torch.from_numpy(features).float()\n",
    "    targets = torch.from_numpy(targets).long()\n",
    "    targets = torch.squeeze(targets)\n",
    "    train = data_utils.TensorDataset(features, targets)\n",
    "\n",
    "    N = features.size()[0]\n",
    "    sample_list = np.arange(N, dtype=np.int64)\n",
    "    np.random.shuffle(sample_list)\n",
    "    percent_train = 1.0\n",
    "\n",
    "    #Training\n",
    "    n_training_samples = int(N*percent_train)\n",
    "    train_sampler = SubsetRandomSampler(sample_list[:n_training_samples])\n",
    "\n",
    "    #Validation\n",
    "    val_sampler = SubsetRandomSampler(sample_list[:n_training_samples])\n",
    "\n",
    "    #Test data\n",
    "    features = torch.from_numpy(features_og_test).float()\n",
    "    targets = torch.from_numpy(targets_og_test).long()\n",
    "    targets = torch.squeeze(targets)\n",
    "    test = data_utils.TensorDataset(features, targets)\n",
    "    return train, test, train_sampler, val_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(torch.nn.Module):\n",
    "    \n",
    "    #Our batch shape for input x is (3, 32, 32)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        #Input channels = 1, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(23, 18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(18 * 16 * 16, 64)\n",
    "        \n",
    "        #64 input features, 10 output features for our 10 defined classes\n",
    "        self.fc2 = torch.nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (3, 32, 32) to (18, 32, 32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        #Size changes from (18, 32, 32) to (18, 16, 16)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (18, 16, 16) to (1, 4608)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        x = x.view(-1, 18 * 16 *16)\n",
    "        \n",
    "        #Computes the activation of the first fully connected layer\n",
    "        #Size changes from (1, 4608) to (1, 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #Computes the second fully connected layer (activation applied later)\n",
    "        #Size changes from (1, 64) to (1, 10)\n",
    "        x = self.fc2(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader takes in a dataset and a sampler for loading (num_workers deals with system level memory) \n",
    "def get_train_loader(batch_size, train_sampler):\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers=2)\n",
    "    return(train_loader)\n",
    "\n",
    "#Test and validation loaders have constant batch sizes, so we can define them directly\n",
    "val_loader = torch.utils.data.DataLoader(train, batch_size=32, sampler=val_sampler, num_workers=2)\n",
    "\n",
    "def trainNet(net, train_sampler, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = get_train_loader(batch_size, train_sampler)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.data[0]\n",
    "            total_train_loss += loss_size.data[0]\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.6f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in val_loader:            \n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data[0]\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / max(1,len(val_loader))))\n",
    "        \n",
    "\n",
    "        print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 20\n",
      "learning_rate= 1e-07\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Christian/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/Christian/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 10% \t train_loss: 2.583366 took: 1.68s\n",
      "Epoch 1, 20% \t train_loss: 1.967699 took: 1.16s\n",
      "Epoch 1, 30% \t train_loss: 1.458107 took: 1.18s\n",
      "Epoch 1, 40% \t train_loss: 1.249286 took: 1.29s\n",
      "Epoch 1, 50% \t train_loss: 1.024508 took: 1.16s\n",
      "Epoch 1, 60% \t train_loss: 0.951950 took: 1.19s\n",
      "Epoch 1, 70% \t train_loss: 0.825036 took: 1.14s\n",
      "Epoch 1, 80% \t train_loss: 0.809808 took: 1.23s\n",
      "Epoch 1, 90% \t train_loss: 0.701772 took: 1.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Christian/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.63\n",
      "Epoch 2, 10% \t train_loss: 0.594906 took: 1.77s\n",
      "Epoch 2, 20% \t train_loss: 0.599846 took: 1.16s\n",
      "Epoch 2, 30% \t train_loss: 0.531406 took: 1.39s\n",
      "Epoch 2, 40% \t train_loss: 0.535186 took: 1.17s\n",
      "Epoch 2, 50% \t train_loss: 0.511298 took: 1.21s\n",
      "Epoch 2, 60% \t train_loss: 0.462963 took: 1.21s\n",
      "Epoch 2, 70% \t train_loss: 0.439777 took: 1.20s\n",
      "Epoch 2, 80% \t train_loss: 0.380659 took: 1.18s\n",
      "Epoch 2, 90% \t train_loss: 0.389825 took: 1.24s\n",
      "Validation loss = 0.37\n",
      "Epoch 3, 10% \t train_loss: 0.362626 took: 1.73s\n",
      "Epoch 3, 20% \t train_loss: 0.326635 took: 1.37s\n",
      "Epoch 3, 30% \t train_loss: 0.323807 took: 1.18s\n",
      "Epoch 3, 40% \t train_loss: 0.303603 took: 1.33s\n",
      "Epoch 3, 50% \t train_loss: 0.317406 took: 1.17s\n",
      "Epoch 3, 60% \t train_loss: 0.300134 took: 1.19s\n",
      "Epoch 3, 70% \t train_loss: 0.258814 took: 1.27s\n",
      "Epoch 3, 80% \t train_loss: 0.262742 took: 1.30s\n",
      "Epoch 3, 90% \t train_loss: 0.254640 took: 1.21s\n",
      "Validation loss = 0.23\n",
      "Epoch 4, 10% \t train_loss: 0.236902 took: 1.84s\n",
      "Epoch 4, 20% \t train_loss: 0.207751 took: 1.14s\n",
      "Epoch 4, 30% \t train_loss: 0.195732 took: 1.24s\n",
      "Epoch 4, 40% \t train_loss: 0.206894 took: 1.23s\n",
      "Epoch 4, 50% \t train_loss: 0.190862 took: 1.19s\n",
      "Epoch 4, 60% \t train_loss: 0.183493 took: 1.22s\n",
      "Epoch 4, 70% \t train_loss: 0.173205 took: 1.21s\n",
      "Epoch 4, 80% \t train_loss: 0.165101 took: 1.16s\n",
      "Epoch 4, 90% \t train_loss: 0.164744 took: 1.31s\n",
      "Validation loss = 0.15\n",
      "Epoch 5, 10% \t train_loss: 0.153121 took: 1.77s\n",
      "Epoch 5, 20% \t train_loss: 0.147069 took: 1.24s\n",
      "Epoch 5, 30% \t train_loss: 0.130890 took: 1.19s\n",
      "Epoch 5, 40% \t train_loss: 0.141928 took: 1.17s\n",
      "Epoch 5, 50% \t train_loss: 0.127699 took: 1.22s\n",
      "Epoch 5, 60% \t train_loss: 0.130278 took: 1.15s\n",
      "Epoch 5, 70% \t train_loss: 0.119105 took: 1.21s\n",
      "Epoch 5, 80% \t train_loss: 0.113816 took: 1.20s\n",
      "Epoch 5, 90% \t train_loss: 0.104380 took: 1.25s\n",
      "Validation loss = 0.11\n",
      "Epoch 6, 10% \t train_loss: 0.106332 took: 1.88s\n",
      "Epoch 6, 20% \t train_loss: 0.100688 took: 1.23s\n",
      "Epoch 6, 30% \t train_loss: 0.099748 took: 1.21s\n",
      "Epoch 6, 40% \t train_loss: 0.094200 took: 1.26s\n",
      "Epoch 6, 50% \t train_loss: 0.097481 took: 1.22s\n",
      "Epoch 6, 60% \t train_loss: 0.089355 took: 1.29s\n",
      "Epoch 6, 70% \t train_loss: 0.081493 took: 1.29s\n",
      "Epoch 6, 80% \t train_loss: 0.081084 took: 1.53s\n",
      "Epoch 6, 90% \t train_loss: 0.073743 took: 1.20s\n",
      "Validation loss = 0.08\n",
      "Epoch 7, 10% \t train_loss: 0.069279 took: 1.84s\n",
      "Epoch 7, 20% \t train_loss: 0.075833 took: 1.17s\n",
      "Epoch 7, 30% \t train_loss: 0.073068 took: 1.20s\n",
      "Epoch 7, 40% \t train_loss: 0.062502 took: 1.22s\n",
      "Epoch 7, 50% \t train_loss: 0.068309 took: 1.23s\n",
      "Epoch 7, 60% \t train_loss: 0.071620 took: 1.21s\n",
      "Epoch 7, 70% \t train_loss: 0.058871 took: 1.40s\n",
      "Epoch 7, 80% \t train_loss: 0.057762 took: 1.19s\n",
      "Epoch 7, 90% \t train_loss: 0.059235 took: 1.37s\n",
      "Validation loss = 0.06\n",
      "Epoch 8, 10% \t train_loss: 0.056437 took: 1.86s\n",
      "Epoch 8, 20% \t train_loss: 0.052368 took: 1.24s\n",
      "Epoch 8, 30% \t train_loss: 0.052442 took: 1.16s\n",
      "Epoch 8, 40% \t train_loss: 0.049000 took: 1.28s\n",
      "Epoch 8, 50% \t train_loss: 0.044568 took: 1.23s\n",
      "Epoch 8, 60% \t train_loss: 0.048906 took: 1.35s\n",
      "Epoch 8, 70% \t train_loss: 0.050257 took: 1.24s\n",
      "Epoch 8, 80% \t train_loss: 0.046007 took: 1.30s\n",
      "Epoch 8, 90% \t train_loss: 0.042441 took: 1.21s\n",
      "Validation loss = 0.04\n",
      "Epoch 9, 10% \t train_loss: 0.041483 took: 1.79s\n",
      "Epoch 9, 20% \t train_loss: 0.040842 took: 1.19s\n",
      "Epoch 9, 30% \t train_loss: 0.041501 took: 1.25s\n",
      "Epoch 9, 40% \t train_loss: 0.039356 took: 1.54s\n",
      "Epoch 9, 50% \t train_loss: 0.039502 took: 1.24s\n",
      "Epoch 9, 60% \t train_loss: 0.036986 took: 1.27s\n",
      "Epoch 9, 70% \t train_loss: 0.029119 took: 1.22s\n",
      "Epoch 9, 80% \t train_loss: 0.030200 took: 1.20s\n",
      "Epoch 9, 90% \t train_loss: 0.036338 took: 1.24s\n",
      "Validation loss = 0.03\n",
      "Epoch 10, 10% \t train_loss: 0.029472 took: 1.82s\n",
      "Epoch 10, 20% \t train_loss: 0.032679 took: 1.27s\n",
      "Epoch 10, 30% \t train_loss: 0.031274 took: 1.38s\n",
      "Epoch 10, 40% \t train_loss: 0.027843 took: 1.16s\n",
      "Epoch 10, 50% \t train_loss: 0.028119 took: 1.22s\n",
      "Epoch 10, 60% \t train_loss: 0.025337 took: 1.21s\n",
      "Epoch 10, 70% \t train_loss: 0.024056 took: 1.17s\n",
      "Epoch 10, 80% \t train_loss: 0.031149 took: 1.24s\n",
      "Epoch 10, 90% \t train_loss: 0.027894 took: 1.24s\n",
      "Validation loss = 0.02\n",
      "Epoch 11, 10% \t train_loss: 0.028357 took: 1.86s\n",
      "Epoch 11, 20% \t train_loss: 0.022247 took: 1.33s\n",
      "Epoch 11, 30% \t train_loss: 0.023028 took: 1.18s\n",
      "Epoch 11, 40% \t train_loss: 0.024196 took: 1.26s\n",
      "Epoch 11, 50% \t train_loss: 0.021805 took: 1.26s\n",
      "Epoch 11, 60% \t train_loss: 0.023337 took: 1.19s\n",
      "Epoch 11, 70% \t train_loss: 0.022901 took: 1.20s\n",
      "Epoch 11, 80% \t train_loss: 0.019352 took: 1.19s\n",
      "Epoch 11, 90% \t train_loss: 0.020438 took: 1.22s\n",
      "Validation loss = 0.02\n",
      "Epoch 12, 10% \t train_loss: 0.022203 took: 1.87s\n",
      "Epoch 12, 20% \t train_loss: 0.018673 took: 1.20s\n",
      "Epoch 12, 30% \t train_loss: 0.016848 took: 1.21s\n",
      "Epoch 12, 40% \t train_loss: 0.024832 took: 1.18s\n",
      "Epoch 12, 50% \t train_loss: 0.016264 took: 1.25s\n",
      "Epoch 12, 60% \t train_loss: 0.017310 took: 1.21s\n",
      "Epoch 12, 70% \t train_loss: 0.020573 took: 1.23s\n",
      "Epoch 12, 80% \t train_loss: 0.017625 took: 1.24s\n",
      "Epoch 12, 90% \t train_loss: 0.015081 took: 1.31s\n",
      "Validation loss = 0.02\n",
      "Epoch 13, 10% \t train_loss: 0.015290 took: 1.84s\n",
      "Epoch 13, 20% \t train_loss: 0.012431 took: 1.27s\n",
      "Epoch 13, 30% \t train_loss: 0.014595 took: 1.22s\n",
      "Epoch 13, 40% \t train_loss: 0.016578 took: 1.66s\n",
      "Epoch 13, 50% \t train_loss: 0.017841 took: 1.44s\n",
      "Epoch 13, 60% \t train_loss: 0.017156 took: 1.38s\n",
      "Epoch 13, 70% \t train_loss: 0.014740 took: 1.15s\n",
      "Epoch 13, 80% \t train_loss: 0.017419 took: 1.23s\n",
      "Epoch 13, 90% \t train_loss: 0.012362 took: 1.24s\n",
      "Validation loss = 0.01\n",
      "Epoch 14, 10% \t train_loss: 0.012291 took: 1.84s\n",
      "Epoch 14, 20% \t train_loss: 0.012519 took: 1.28s\n",
      "Epoch 14, 30% \t train_loss: 0.015231 took: 1.27s\n",
      "Epoch 14, 40% \t train_loss: 0.012213 took: 1.22s\n",
      "Epoch 14, 50% \t train_loss: 0.012857 took: 1.22s\n",
      "Epoch 14, 60% \t train_loss: 0.010680 took: 1.17s\n",
      "Epoch 14, 70% \t train_loss: 0.010759 took: 1.20s\n",
      "Epoch 14, 80% \t train_loss: 0.018781 took: 1.17s\n",
      "Epoch 14, 90% \t train_loss: 0.013096 took: 1.28s\n",
      "Validation loss = 0.01\n",
      "Epoch 15, 10% \t train_loss: 0.012500 took: 1.96s\n",
      "Epoch 15, 20% \t train_loss: 0.011732 took: 1.14s\n",
      "Epoch 15, 30% \t train_loss: 0.010180 took: 1.22s\n",
      "Epoch 15, 40% \t train_loss: 0.010766 took: 1.15s\n",
      "Epoch 15, 50% \t train_loss: 0.008794 took: 1.21s\n",
      "Epoch 15, 60% \t train_loss: 0.012770 took: 1.19s\n",
      "Epoch 15, 70% \t train_loss: 0.011361 took: 1.19s\n",
      "Epoch 15, 80% \t train_loss: 0.013226 took: 1.19s\n",
      "Epoch 15, 90% \t train_loss: 0.010352 took: 1.18s\n",
      "Validation loss = 0.01\n",
      "Epoch 16, 10% \t train_loss: 0.008271 took: 1.83s\n",
      "Epoch 16, 20% \t train_loss: 0.010049 took: 1.21s\n",
      "Epoch 16, 30% \t train_loss: 0.011010 took: 1.19s\n",
      "Epoch 16, 40% \t train_loss: 0.009727 took: 1.23s\n",
      "Epoch 16, 50% \t train_loss: 0.008836 took: 1.16s\n",
      "Epoch 16, 60% \t train_loss: 0.009823 took: 1.22s\n",
      "Epoch 16, 70% \t train_loss: 0.010470 took: 1.52s\n",
      "Epoch 16, 80% \t train_loss: 0.010857 took: 1.21s\n",
      "Epoch 16, 90% \t train_loss: 0.010409 took: 1.29s\n",
      "Validation loss = 0.01\n",
      "Epoch 17, 10% \t train_loss: 0.012712 took: 1.77s\n",
      "Epoch 17, 20% \t train_loss: 0.008731 took: 1.18s\n",
      "Epoch 17, 30% \t train_loss: 0.006766 took: 1.19s\n",
      "Epoch 17, 40% \t train_loss: 0.007542 took: 1.15s\n",
      "Epoch 17, 50% \t train_loss: 0.009311 took: 1.22s\n",
      "Epoch 17, 60% \t train_loss: 0.008071 took: 1.29s\n",
      "Epoch 17, 70% \t train_loss: 0.005983 took: 1.15s\n",
      "Epoch 17, 80% \t train_loss: 0.008370 took: 1.27s\n",
      "Epoch 17, 90% \t train_loss: 0.005996 took: 1.21s\n",
      "Validation loss = 0.01\n",
      "Epoch 18, 10% \t train_loss: 0.008010 took: 1.83s\n",
      "Epoch 18, 20% \t train_loss: 0.008016 took: 1.18s\n",
      "Epoch 18, 30% \t train_loss: 0.008693 took: 1.20s\n",
      "Epoch 18, 40% \t train_loss: 0.008474 took: 1.32s\n",
      "Epoch 18, 50% \t train_loss: 0.009080 took: 1.31s\n",
      "Epoch 18, 60% \t train_loss: 0.006608 took: 1.22s\n",
      "Epoch 18, 70% \t train_loss: 0.007668 took: 1.26s\n",
      "Epoch 18, 80% \t train_loss: 0.007495 took: 1.19s\n",
      "Epoch 18, 90% \t train_loss: 0.006165 took: 1.25s\n",
      "Validation loss = 0.01\n",
      "Epoch 19, 10% \t train_loss: 0.006631 took: 1.87s\n",
      "Epoch 19, 20% \t train_loss: 0.005270 took: 1.19s\n",
      "Epoch 19, 30% \t train_loss: 0.005760 took: 1.23s\n",
      "Epoch 19, 40% \t train_loss: 0.007284 took: 1.23s\n",
      "Epoch 19, 50% \t train_loss: 0.006437 took: 1.27s\n",
      "Epoch 19, 60% \t train_loss: 0.007045 took: 1.24s\n",
      "Epoch 19, 70% \t train_loss: 0.007650 took: 1.17s\n",
      "Epoch 19, 80% \t train_loss: 0.010511 took: 1.24s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, 90% \t train_loss: 0.007176 took: 1.22s\n",
      "Validation loss = 0.01\n",
      "Epoch 20, 10% \t train_loss: 0.005668 took: 1.87s\n",
      "Epoch 20, 20% \t train_loss: 0.008720 took: 1.33s\n",
      "Epoch 20, 30% \t train_loss: 0.006293 took: 1.19s\n",
      "Epoch 20, 40% \t train_loss: 0.004814 took: 1.28s\n",
      "Epoch 20, 50% \t train_loss: 0.006440 took: 1.23s\n",
      "Epoch 20, 60% \t train_loss: 0.006062 took: 1.19s\n",
      "Epoch 20, 70% \t train_loss: 0.004907 took: 1.23s\n",
      "Epoch 20, 80% \t train_loss: 0.005867 took: 1.27s\n",
      "Epoch 20, 90% \t train_loss: 0.004979 took: 1.16s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 328.44s\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train, test, train_sampler, val_sampler = split_train_test(0.85)\n",
    "# Build and train CNN\n",
    "CNN = SimpleCNN()\n",
    "trainNet(CNN, batch_size=4, n_epochs=25, learning_rate=1e-7)#1e-6, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 38.] (10379, 1)\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Christian/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/Christian/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:57: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 10% \t train_loss: 1.030834 took: 3.15s\n",
      "Epoch 1, 20% \t train_loss: 0.486802 took: 1.64s\n",
      "Epoch 1, 30% \t train_loss: 0.306088 took: 1.58s\n",
      "Epoch 1, 40% \t train_loss: 0.225894 took: 1.45s\n",
      "Epoch 1, 50% \t train_loss: 0.181125 took: 1.60s\n",
      "Epoch 1, 60% \t train_loss: 0.146279 took: 1.62s\n",
      "Epoch 1, 70% \t train_loss: 0.135309 took: 1.61s\n",
      "Epoch 1, 80% \t train_loss: 0.116394 took: 1.69s\n",
      "Epoch 1, 90% \t train_loss: 0.099511 took: 1.73s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Christian/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:76: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.07\n",
      "Training finished, took 21.66s\n",
      "Epoch 2, 10% \t train_loss: 0.064215 took: 3.11s\n",
      "Epoch 2, 20% \t train_loss: 0.050044 took: 1.62s\n",
      "Epoch 2, 30% \t train_loss: 0.055039 took: 1.53s\n",
      "Epoch 2, 40% \t train_loss: 0.044163 took: 1.50s\n",
      "Epoch 2, 50% \t train_loss: 0.033316 took: 1.41s\n",
      "Epoch 2, 60% \t train_loss: 0.037932 took: 1.51s\n",
      "Epoch 2, 70% \t train_loss: 0.028644 took: 1.55s\n",
      "Epoch 2, 80% \t train_loss: 0.034442 took: 1.49s\n",
      "Epoch 2, 90% \t train_loss: 0.023707 took: 1.38s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 42.85s\n",
      "Epoch 3, 10% \t train_loss: 0.019355 took: 3.01s\n",
      "Epoch 3, 20% \t train_loss: 0.017900 took: 1.64s\n",
      "Epoch 3, 30% \t train_loss: 0.021104 took: 1.57s\n",
      "Epoch 3, 40% \t train_loss: 0.015888 took: 1.65s\n",
      "Epoch 3, 50% \t train_loss: 0.010186 took: 1.41s\n",
      "Epoch 3, 60% \t train_loss: 0.013479 took: 1.51s\n",
      "Epoch 3, 70% \t train_loss: 0.010220 took: 1.53s\n",
      "Epoch 3, 80% \t train_loss: 0.008470 took: 1.56s\n",
      "Epoch 3, 90% \t train_loss: 0.011456 took: 1.51s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 64.16s\n",
      "Epoch 4, 10% \t train_loss: 0.006171 took: 3.11s\n",
      "Epoch 4, 20% \t train_loss: 0.007997 took: 1.69s\n",
      "Epoch 4, 30% \t train_loss: 0.008410 took: 1.62s\n",
      "Epoch 4, 40% \t train_loss: 0.006004 took: 1.50s\n",
      "Epoch 4, 50% \t train_loss: 0.006931 took: 1.52s\n",
      "Epoch 4, 60% \t train_loss: 0.005514 took: 1.44s\n",
      "Epoch 4, 70% \t train_loss: 0.004317 took: 1.52s\n",
      "Epoch 4, 80% \t train_loss: 0.003209 took: 1.60s\n",
      "Epoch 4, 90% \t train_loss: 0.004838 took: 1.54s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 85.85s\n",
      "Epoch 5, 10% \t train_loss: 0.003886 took: 3.14s\n",
      "Epoch 5, 20% \t train_loss: 0.002736 took: 1.50s\n",
      "Epoch 5, 30% \t train_loss: 0.003251 took: 1.56s\n",
      "Epoch 5, 40% \t train_loss: 0.004721 took: 1.51s\n",
      "Epoch 5, 50% \t train_loss: 0.003232 took: 1.57s\n",
      "Epoch 5, 60% \t train_loss: 0.003453 took: 1.56s\n",
      "Epoch 5, 70% \t train_loss: 0.002920 took: 1.45s\n",
      "Epoch 5, 80% \t train_loss: 0.002412 took: 1.64s\n",
      "Epoch 5, 90% \t train_loss: 0.001429 took: 1.59s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 107.46s\n",
      "Epoch 6, 10% \t train_loss: 0.002329 took: 3.50s\n",
      "Epoch 6, 20% \t train_loss: 0.001533 took: 1.62s\n",
      "Epoch 6, 30% \t train_loss: 0.002237 took: 1.49s\n",
      "Epoch 6, 40% \t train_loss: 0.001394 took: 1.43s\n",
      "Epoch 6, 50% \t train_loss: 0.001201 took: 1.53s\n",
      "Epoch 6, 60% \t train_loss: 0.001247 took: 1.62s\n",
      "Epoch 6, 70% \t train_loss: 0.003762 took: 1.58s\n",
      "Epoch 6, 80% \t train_loss: 0.001034 took: 1.57s\n",
      "Epoch 6, 90% \t train_loss: 0.000996 took: 1.42s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 129.36s\n",
      "Epoch 7, 10% \t train_loss: 0.000904 took: 3.27s\n",
      "Epoch 7, 20% \t train_loss: 0.002049 took: 1.65s\n",
      "Epoch 7, 30% \t train_loss: 0.000872 took: 1.53s\n",
      "Epoch 7, 40% \t train_loss: 0.000935 took: 1.58s\n",
      "Epoch 7, 50% \t train_loss: 0.000749 took: 1.55s\n",
      "Epoch 7, 60% \t train_loss: 0.001060 took: 1.50s\n",
      "Epoch 7, 70% \t train_loss: 0.000799 took: 1.56s\n",
      "Epoch 7, 80% \t train_loss: 0.000565 took: 1.59s\n",
      "Epoch 7, 90% \t train_loss: 0.000598 took: 1.59s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 151.34s\n",
      "Epoch 8, 10% \t train_loss: 0.000462 took: 3.22s\n",
      "Epoch 8, 20% \t train_loss: 0.000484 took: 1.71s\n",
      "Epoch 8, 30% \t train_loss: 0.000625 took: 1.43s\n",
      "Epoch 8, 40% \t train_loss: 0.000394 took: 1.59s\n",
      "Epoch 8, 50% \t train_loss: 0.000533 took: 1.51s\n",
      "Epoch 8, 60% \t train_loss: 0.000496 took: 1.63s\n",
      "Epoch 8, 70% \t train_loss: 0.000392 took: 1.56s\n",
      "Epoch 8, 80% \t train_loss: 0.001125 took: 1.48s\n",
      "Epoch 8, 90% \t train_loss: 0.000392 took: 1.58s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 173.13s\n",
      "Epoch 9, 10% \t train_loss: 0.000458 took: 3.32s\n",
      "Epoch 9, 20% \t train_loss: 0.000265 took: 1.69s\n",
      "Epoch 9, 30% \t train_loss: 0.000297 took: 1.55s\n",
      "Epoch 9, 40% \t train_loss: 0.000348 took: 1.56s\n",
      "Epoch 9, 50% \t train_loss: 0.000394 took: 1.39s\n",
      "Epoch 9, 60% \t train_loss: 0.000184 took: 1.58s\n",
      "Epoch 9, 70% \t train_loss: 0.000302 took: 1.59s\n",
      "Epoch 9, 80% \t train_loss: 0.000256 took: 1.58s\n",
      "Epoch 9, 90% \t train_loss: 0.000228 took: 1.62s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 194.96s\n",
      "Epoch 10, 10% \t train_loss: 0.000197 took: 3.04s\n",
      "Epoch 10, 20% \t train_loss: 0.000172 took: 1.64s\n",
      "Epoch 10, 30% \t train_loss: 0.000139 took: 1.60s\n",
      "Epoch 10, 40% \t train_loss: 0.000181 took: 1.53s\n",
      "Epoch 10, 50% \t train_loss: 0.000206 took: 1.58s\n",
      "Epoch 10, 60% \t train_loss: 0.000392 took: 1.44s\n",
      "Epoch 10, 70% \t train_loss: 0.000149 took: 1.60s\n",
      "Epoch 10, 80% \t train_loss: 0.000124 took: 1.55s\n",
      "Epoch 10, 90% \t train_loss: 0.000158 took: 1.61s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 216.56s\n",
      "[[ 543.    1.]\n",
      " [ 200.    0.]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 0.825569 took: 3.57s\n",
      "Epoch 1, 20% \t train_loss: 0.312306 took: 1.51s\n",
      "Epoch 1, 30% \t train_loss: 0.212874 took: 1.48s\n",
      "Epoch 1, 40% \t train_loss: 0.147747 took: 1.35s\n",
      "Epoch 1, 50% \t train_loss: 0.132260 took: 1.46s\n",
      "Epoch 1, 60% \t train_loss: 0.103067 took: 1.47s\n",
      "Epoch 1, 70% \t train_loss: 0.095159 took: 1.54s\n",
      "Epoch 1, 80% \t train_loss: 0.065534 took: 1.36s\n",
      "Epoch 1, 90% \t train_loss: 0.064074 took: 1.50s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 20.93s\n",
      "Epoch 2, 10% \t train_loss: 0.038911 took: 3.40s\n",
      "Epoch 2, 20% \t train_loss: 0.031440 took: 1.50s\n",
      "Epoch 2, 30% \t train_loss: 0.032735 took: 1.37s\n",
      "Epoch 2, 40% \t train_loss: 0.027014 took: 1.49s\n",
      "Epoch 2, 50% \t train_loss: 0.026450 took: 1.54s\n",
      "Epoch 2, 60% \t train_loss: 0.018106 took: 1.56s\n",
      "Epoch 2, 70% \t train_loss: 0.019091 took: 1.46s\n",
      "Epoch 2, 80% \t train_loss: 0.021660 took: 1.54s\n",
      "Epoch 2, 90% \t train_loss: 0.014152 took: 1.53s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 42.00s\n",
      "Epoch 3, 10% \t train_loss: 0.012331 took: 3.01s\n",
      "Epoch 3, 20% \t train_loss: 0.010755 took: 1.47s\n",
      "Epoch 3, 30% \t train_loss: 0.009853 took: 1.59s\n",
      "Epoch 3, 40% \t train_loss: 0.006714 took: 1.53s\n",
      "Epoch 3, 50% \t train_loss: 0.006389 took: 1.52s\n",
      "Epoch 3, 60% \t train_loss: 0.005688 took: 1.57s\n",
      "Epoch 3, 70% \t train_loss: 0.005556 took: 1.48s\n",
      "Epoch 3, 80% \t train_loss: 0.005435 took: 1.58s\n",
      "Epoch 3, 90% \t train_loss: 0.004044 took: 1.53s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 63.31s\n",
      "Epoch 4, 10% \t train_loss: 0.003561 took: 3.27s\n",
      "Epoch 4, 20% \t train_loss: 0.003118 took: 1.64s\n",
      "Epoch 4, 30% \t train_loss: 0.003286 took: 1.40s\n",
      "Epoch 4, 40% \t train_loss: 0.002870 took: 1.53s\n",
      "Epoch 4, 50% \t train_loss: 0.003735 took: 1.50s\n",
      "Epoch 4, 60% \t train_loss: 0.002702 took: 1.50s\n",
      "Epoch 4, 70% \t train_loss: 0.002127 took: 1.48s\n",
      "Epoch 4, 80% \t train_loss: 0.001805 took: 1.43s\n",
      "Epoch 4, 90% \t train_loss: 0.002822 took: 1.57s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 84.23s\n",
      "Epoch 5, 10% \t train_loss: 0.002013 took: 3.18s\n",
      "Epoch 5, 20% \t train_loss: 0.001304 took: 1.45s\n",
      "Epoch 5, 30% \t train_loss: 0.001462 took: 1.57s\n",
      "Epoch 5, 40% \t train_loss: 0.000875 took: 1.54s\n",
      "Epoch 5, 50% \t train_loss: 0.000837 took: 1.51s\n",
      "Epoch 5, 60% \t train_loss: 0.001164 took: 1.57s\n",
      "Epoch 5, 70% \t train_loss: 0.000973 took: 1.41s\n",
      "Epoch 5, 80% \t train_loss: 0.001259 took: 1.51s\n",
      "Epoch 5, 90% \t train_loss: 0.001185 took: 1.56s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 105.48s\n",
      "Epoch 6, 10% \t train_loss: 0.000905 took: 3.36s\n",
      "Epoch 6, 20% \t train_loss: 0.000796 took: 1.65s\n",
      "Epoch 6, 30% \t train_loss: 0.000616 took: 1.71s\n",
      "Epoch 6, 40% \t train_loss: 0.000802 took: 1.67s\n",
      "Epoch 6, 50% \t train_loss: 0.000421 took: 1.55s\n",
      "Epoch 6, 60% \t train_loss: 0.000626 took: 1.79s\n",
      "Epoch 6, 70% \t train_loss: 0.000441 took: 1.55s\n",
      "Epoch 6, 80% \t train_loss: 0.000442 took: 1.68s\n",
      "Epoch 6, 90% \t train_loss: 0.000349 took: 1.64s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 127.90s\n",
      "Epoch 7, 10% \t train_loss: 0.000289 took: 3.10s\n",
      "Epoch 7, 20% \t train_loss: 0.000457 took: 1.70s\n",
      "Epoch 7, 30% \t train_loss: 0.000302 took: 1.67s\n",
      "Epoch 7, 40% \t train_loss: 0.000273 took: 1.56s\n",
      "Epoch 7, 50% \t train_loss: 0.000239 took: 1.53s\n",
      "Epoch 7, 60% \t train_loss: 0.000301 took: 1.62s\n",
      "Epoch 7, 70% \t train_loss: 0.000262 took: 1.45s\n",
      "Epoch 7, 80% \t train_loss: 0.000408 took: 1.53s\n",
      "Epoch 7, 90% \t train_loss: 0.000170 took: 1.52s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 149.52s\n",
      "Epoch 8, 10% \t train_loss: 0.000179 took: 3.32s\n",
      "Epoch 8, 20% \t train_loss: 0.000174 took: 1.68s\n",
      "Epoch 8, 30% \t train_loss: 0.000132 took: 1.49s\n",
      "Epoch 8, 40% \t train_loss: 0.000112 took: 1.59s\n",
      "Epoch 8, 50% \t train_loss: 0.000131 took: 1.61s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, 60% \t train_loss: 0.000121 took: 1.57s\n",
      "Epoch 8, 70% \t train_loss: 0.000336 took: 1.56s\n",
      "Epoch 8, 80% \t train_loss: 0.000146 took: 1.43s\n",
      "Epoch 8, 90% \t train_loss: 0.000124 took: 1.59s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 171.14s\n",
      "Epoch 9, 10% \t train_loss: 0.000130 took: 3.10s\n",
      "Epoch 9, 20% \t train_loss: 0.000083 took: 1.68s\n",
      "Epoch 9, 30% \t train_loss: 0.000067 took: 1.63s\n",
      "Epoch 9, 40% \t train_loss: 0.000066 took: 1.46s\n",
      "Epoch 9, 50% \t train_loss: 0.000070 took: 1.54s\n",
      "Epoch 9, 60% \t train_loss: 0.000090 took: 1.58s\n",
      "Epoch 9, 70% \t train_loss: 0.000062 took: 1.56s\n",
      "Epoch 9, 80% \t train_loss: 0.000074 took: 1.53s\n",
      "Epoch 9, 90% \t train_loss: 0.000057 took: 1.42s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 192.30s\n",
      "Epoch 10, 10% \t train_loss: 0.000043 took: 3.30s\n",
      "Epoch 10, 20% \t train_loss: 0.000041 took: 1.64s\n",
      "Epoch 10, 30% \t train_loss: 0.000037 took: 1.55s\n",
      "Epoch 10, 40% \t train_loss: 0.000049 took: 1.47s\n",
      "Epoch 10, 50% \t train_loss: 0.000035 took: 1.54s\n",
      "Epoch 10, 60% \t train_loss: 0.000029 took: 1.52s\n",
      "Epoch 10, 70% \t train_loss: 0.000097 took: 1.60s\n",
      "Epoch 10, 80% \t train_loss: 0.000047 took: 1.58s\n",
      "Epoch 10, 90% \t train_loss: 0.000026 took: 1.58s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 213.91s\n",
      "[[ 1086.     2.]\n",
      " [  200.   200.]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 0.924501 took: 2.94s\n",
      "Epoch 1, 20% \t train_loss: 0.394821 took: 1.57s\n",
      "Epoch 1, 30% \t train_loss: 0.301373 took: 1.58s\n",
      "Epoch 1, 40% \t train_loss: 0.207076 took: 1.46s\n",
      "Epoch 1, 50% \t train_loss: 0.146520 took: 1.42s\n",
      "Epoch 1, 60% \t train_loss: 0.118056 took: 1.56s\n",
      "Epoch 1, 70% \t train_loss: 0.097123 took: 1.48s\n",
      "Epoch 1, 80% \t train_loss: 0.083919 took: 1.51s\n",
      "Epoch 1, 90% \t train_loss: 0.065782 took: 1.34s\n",
      "Validation loss = 0.07\n",
      "Training finished, took 20.59s\n",
      "Epoch 2, 10% \t train_loss: 0.048191 took: 3.43s\n",
      "Epoch 2, 20% \t train_loss: 0.051138 took: 1.61s\n",
      "Epoch 2, 30% \t train_loss: 0.040391 took: 1.60s\n",
      "Epoch 2, 40% \t train_loss: 0.032005 took: 1.57s\n",
      "Epoch 2, 50% \t train_loss: 0.038671 took: 1.44s\n",
      "Epoch 2, 60% \t train_loss: 0.025367 took: 1.58s\n",
      "Epoch 2, 70% \t train_loss: 0.027352 took: 1.60s\n",
      "Epoch 2, 80% \t train_loss: 0.017840 took: 1.69s\n",
      "Epoch 2, 90% \t train_loss: 0.012971 took: 1.52s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 42.80s\n",
      "Epoch 3, 10% \t train_loss: 0.014765 took: 3.25s\n",
      "Epoch 3, 20% \t train_loss: 0.018398 took: 1.50s\n",
      "Epoch 3, 30% \t train_loss: 0.010871 took: 1.58s\n",
      "Epoch 3, 40% \t train_loss: 0.011638 took: 1.55s\n",
      "Epoch 3, 50% \t train_loss: 0.009875 took: 1.53s\n",
      "Epoch 3, 60% \t train_loss: 0.007077 took: 1.51s\n",
      "Epoch 3, 70% \t train_loss: 0.013378 took: 1.45s\n",
      "Epoch 3, 80% \t train_loss: 0.008742 took: 1.61s\n",
      "Epoch 3, 90% \t train_loss: 0.010617 took: 1.56s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 64.40s\n",
      "Epoch 4, 10% \t train_loss: 0.008839 took: 3.44s\n",
      "Epoch 4, 20% \t train_loss: 0.006049 took: 1.69s\n",
      "Epoch 4, 30% \t train_loss: 0.008215 took: 1.54s\n",
      "Epoch 4, 40% \t train_loss: 0.004327 took: 1.52s\n",
      "Epoch 4, 50% \t train_loss: 0.005550 took: 1.57s\n",
      "Epoch 4, 60% \t train_loss: 0.007001 took: 1.54s\n",
      "Epoch 4, 70% \t train_loss: 0.004282 took: 1.57s\n",
      "Epoch 4, 80% \t train_loss: 0.005780 took: 1.51s\n",
      "Epoch 4, 90% \t train_loss: 0.003169 took: 1.42s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 86.02s\n",
      "Epoch 5, 10% \t train_loss: 0.002389 took: 3.17s\n",
      "Epoch 5, 20% \t train_loss: 0.006621 took: 1.64s\n",
      "Epoch 5, 30% \t train_loss: 0.005754 took: 1.59s\n",
      "Epoch 5, 40% \t train_loss: 0.003152 took: 1.52s\n",
      "Epoch 5, 50% \t train_loss: 0.002372 took: 1.41s\n",
      "Epoch 5, 60% \t train_loss: 0.001918 took: 1.57s\n",
      "Epoch 5, 70% \t train_loss: 0.001997 took: 1.65s\n",
      "Epoch 5, 80% \t train_loss: 0.002186 took: 1.53s\n",
      "Epoch 5, 90% \t train_loss: 0.001874 took: 1.55s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 107.46s\n",
      "Epoch 6, 10% \t train_loss: 0.002093 took: 3.22s\n",
      "Epoch 6, 20% \t train_loss: 0.002578 took: 1.64s\n",
      "Epoch 6, 30% \t train_loss: 0.001643 took: 1.52s\n",
      "Epoch 6, 40% \t train_loss: 0.001663 took: 1.50s\n",
      "Epoch 6, 50% \t train_loss: 0.001456 took: 1.47s\n",
      "Epoch 6, 60% \t train_loss: 0.001230 took: 1.49s\n",
      "Epoch 6, 70% \t train_loss: 0.001688 took: 1.54s\n",
      "Epoch 6, 80% \t train_loss: 0.002956 took: 1.55s\n",
      "Epoch 6, 90% \t train_loss: 0.001616 took: 1.62s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 129.11s\n",
      "Epoch 7, 10% \t train_loss: 0.002974 took: 2.83s\n",
      "Epoch 7, 20% \t train_loss: 0.001325 took: 1.82s\n",
      "Epoch 7, 30% \t train_loss: 0.001237 took: 1.78s\n",
      "Epoch 7, 40% \t train_loss: 0.001558 took: 1.54s\n",
      "Epoch 7, 50% \t train_loss: 0.001234 took: 1.68s\n",
      "Epoch 7, 60% \t train_loss: 0.000816 took: 1.55s\n",
      "Epoch 7, 70% \t train_loss: 0.001288 took: 1.51s\n",
      "Epoch 7, 80% \t train_loss: 0.000779 took: 1.49s\n",
      "Epoch 7, 90% \t train_loss: 0.000570 took: 1.54s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 151.30s\n",
      "Epoch 8, 10% \t train_loss: 0.001098 took: 2.97s\n",
      "Epoch 8, 20% \t train_loss: 0.001073 took: 1.70s\n",
      "Epoch 8, 30% \t train_loss: 0.000547 took: 1.53s\n",
      "Epoch 8, 40% \t train_loss: 0.000771 took: 1.49s\n",
      "Epoch 8, 50% \t train_loss: 0.000380 took: 1.55s\n",
      "Epoch 8, 60% \t train_loss: 0.000678 took: 1.56s\n",
      "Epoch 8, 70% \t train_loss: 0.000532 took: 1.58s\n",
      "Epoch 8, 80% \t train_loss: 0.000764 took: 1.73s\n",
      "Epoch 8, 90% \t train_loss: 0.001945 took: 1.58s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 173.18s\n",
      "Epoch 9, 10% \t train_loss: 0.000555 took: 3.48s\n",
      "Epoch 9, 20% \t train_loss: 0.000287 took: 1.50s\n",
      "Epoch 9, 30% \t train_loss: 0.000379 took: 1.67s\n",
      "Epoch 9, 40% \t train_loss: 0.000588 took: 1.71s\n",
      "Epoch 9, 50% \t train_loss: 0.000288 took: 1.59s\n",
      "Epoch 9, 60% \t train_loss: 0.000261 took: 1.51s\n",
      "Epoch 9, 70% \t train_loss: 0.000263 took: 1.70s\n",
      "Epoch 9, 80% \t train_loss: 0.000307 took: 1.65s\n",
      "Epoch 9, 90% \t train_loss: 0.000260 took: 1.51s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 195.32s\n",
      "Epoch 10, 10% \t train_loss: 0.000298 took: 3.40s\n",
      "Epoch 10, 20% \t train_loss: 0.000220 took: 1.80s\n",
      "Epoch 10, 30% \t train_loss: 0.000271 took: 1.70s\n",
      "Epoch 10, 40% \t train_loss: 0.000200 took: 1.60s\n",
      "Epoch 10, 50% \t train_loss: 0.000545 took: 1.63s\n",
      "Epoch 10, 60% \t train_loss: 0.000325 took: 1.43s\n",
      "Epoch 10, 70% \t train_loss: 0.000290 took: 1.72s\n",
      "Epoch 10, 80% \t train_loss: 0.000207 took: 1.61s\n",
      "Epoch 10, 90% \t train_loss: 0.000205 took: 1.53s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 217.66s\n",
      "[[ 1630.     2.]\n",
      " [  200.   400.]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 2.578684 took: 3.86s\n",
      "Epoch 1, 20% \t train_loss: 0.570934 took: 1.64s\n",
      "Epoch 1, 30% \t train_loss: 0.368844 took: 1.68s\n",
      "Epoch 1, 40% \t train_loss: 0.300660 took: 1.52s\n",
      "Epoch 1, 50% \t train_loss: 0.200168 took: 1.42s\n",
      "Epoch 1, 60% \t train_loss: 0.155387 took: 1.67s\n",
      "Epoch 1, 70% \t train_loss: 0.129932 took: 1.67s\n",
      "Epoch 1, 80% \t train_loss: 0.110310 took: 1.52s\n",
      "Epoch 1, 90% \t train_loss: 0.123498 took: 1.51s\n",
      "Validation loss = 0.09\n",
      "Training finished, took 22.16s\n",
      "Epoch 2, 10% \t train_loss: 0.064879 took: 3.32s\n",
      "Epoch 2, 20% \t train_loss: 0.063914 took: 1.73s\n",
      "Epoch 2, 30% \t train_loss: 0.042269 took: 1.69s\n",
      "Epoch 2, 40% \t train_loss: 0.047014 took: 1.67s\n",
      "Epoch 2, 50% \t train_loss: 0.030071 took: 1.59s\n",
      "Epoch 2, 60% \t train_loss: 0.033545 took: 1.65s\n",
      "Epoch 2, 70% \t train_loss: 0.053627 took: 1.70s\n",
      "Epoch 2, 80% \t train_loss: 0.028178 took: 1.44s\n",
      "Epoch 2, 90% \t train_loss: 0.020141 took: 1.68s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 44.54s\n",
      "Epoch 3, 10% \t train_loss: 0.020955 took: 3.45s\n",
      "Epoch 3, 20% \t train_loss: 0.015365 took: 1.68s\n",
      "Epoch 3, 30% \t train_loss: 0.016245 took: 1.53s\n",
      "Epoch 3, 40% \t train_loss: 0.027147 took: 1.55s\n",
      "Epoch 3, 50% \t train_loss: 0.008986 took: 1.43s\n",
      "Epoch 3, 60% \t train_loss: 0.011682 took: 1.59s\n",
      "Epoch 3, 70% \t train_loss: 0.010514 took: 1.61s\n",
      "Epoch 3, 80% \t train_loss: 0.009769 took: 1.58s\n",
      "Epoch 3, 90% \t train_loss: 0.012459 took: 1.65s\n",
      "Validation loss = 0.04\n",
      "Training finished, took 66.83s\n",
      "Epoch 4, 10% \t train_loss: 0.007518 took: 3.48s\n",
      "Epoch 4, 20% \t train_loss: 0.005694 took: 1.40s\n",
      "Epoch 4, 30% \t train_loss: 0.006953 took: 1.38s\n",
      "Epoch 4, 40% \t train_loss: 0.005180 took: 1.39s\n",
      "Epoch 4, 50% \t train_loss: 0.008921 took: 1.39s\n",
      "Epoch 4, 60% \t train_loss: 0.004873 took: 1.33s\n",
      "Epoch 4, 70% \t train_loss: 0.003659 took: 1.42s\n",
      "Epoch 4, 80% \t train_loss: 0.005874 took: 1.47s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, 90% \t train_loss: 0.004484 took: 1.43s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 87.42s\n",
      "Epoch 5, 10% \t train_loss: 0.003289 took: 2.86s\n",
      "Epoch 5, 20% \t train_loss: 0.004933 took: 1.54s\n",
      "Epoch 5, 30% \t train_loss: 0.003005 took: 1.39s\n",
      "Epoch 5, 40% \t train_loss: 0.004134 took: 1.33s\n",
      "Epoch 5, 50% \t train_loss: 0.002072 took: 1.45s\n",
      "Epoch 5, 60% \t train_loss: 0.002133 took: 1.50s\n",
      "Epoch 5, 70% \t train_loss: 0.003802 took: 1.36s\n",
      "Epoch 5, 80% \t train_loss: 0.003446 took: 1.32s\n",
      "Epoch 5, 90% \t train_loss: 0.002574 took: 1.53s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 107.26s\n",
      "Epoch 6, 10% \t train_loss: 0.002176 took: 2.72s\n",
      "Epoch 6, 20% \t train_loss: 0.002048 took: 1.59s\n",
      "Epoch 6, 30% \t train_loss: 0.002793 took: 1.61s\n",
      "Epoch 6, 40% \t train_loss: 0.002887 took: 1.52s\n",
      "Epoch 6, 50% \t train_loss: 0.001455 took: 1.49s\n",
      "Epoch 6, 60% \t train_loss: 0.001373 took: 1.41s\n",
      "Epoch 6, 70% \t train_loss: 0.001538 took: 1.50s\n",
      "Epoch 6, 80% \t train_loss: 0.001195 took: 1.49s\n",
      "Epoch 6, 90% \t train_loss: 0.001418 took: 1.42s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 127.58s\n",
      "Epoch 7, 10% \t train_loss: 0.001192 took: 2.88s\n",
      "Epoch 7, 20% \t train_loss: 0.001036 took: 1.57s\n",
      "Epoch 7, 30% \t train_loss: 0.000892 took: 1.46s\n",
      "Epoch 7, 40% \t train_loss: 0.001078 took: 1.45s\n",
      "Epoch 7, 50% \t train_loss: 0.000781 took: 1.45s\n",
      "Epoch 7, 60% \t train_loss: 0.001750 took: 1.36s\n",
      "Epoch 7, 70% \t train_loss: 0.002063 took: 1.60s\n",
      "Epoch 7, 80% \t train_loss: 0.000717 took: 1.56s\n",
      "Epoch 7, 90% \t train_loss: 0.000697 took: 1.42s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 147.97s\n",
      "Epoch 8, 10% \t train_loss: 0.000707 took: 2.84s\n",
      "Epoch 8, 20% \t train_loss: 0.000694 took: 1.50s\n",
      "Epoch 8, 30% \t train_loss: 0.000645 took: 1.46s\n",
      "Epoch 8, 40% \t train_loss: 0.000428 took: 1.46s\n",
      "Epoch 8, 50% \t train_loss: 0.000450 took: 1.45s\n",
      "Epoch 8, 60% \t train_loss: 0.000354 took: 1.53s\n",
      "Epoch 8, 70% \t train_loss: 0.000410 took: 1.41s\n",
      "Epoch 8, 80% \t train_loss: 0.000494 took: 1.46s\n",
      "Epoch 8, 90% \t train_loss: 0.001523 took: 1.47s\n",
      "Validation loss = 0.06\n",
      "Training finished, took 168.20s\n",
      "Epoch 9, 10% \t train_loss: 0.000706 took: 3.26s\n",
      "Epoch 9, 20% \t train_loss: 0.000416 took: 1.52s\n",
      "Epoch 9, 30% \t train_loss: 0.000456 took: 1.56s\n",
      "Epoch 9, 40% \t train_loss: 0.000339 took: 1.71s\n",
      "Epoch 9, 50% \t train_loss: 0.000283 took: 1.52s\n",
      "Epoch 9, 60% \t train_loss: 0.000391 took: 1.53s\n",
      "Epoch 9, 70% \t train_loss: 0.000336 took: 1.45s\n",
      "Epoch 9, 80% \t train_loss: 0.000328 took: 1.46s\n",
      "Epoch 9, 90% \t train_loss: 0.000249 took: 1.44s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 189.25s\n",
      "Epoch 10, 10% \t train_loss: 0.000236 took: 3.34s\n",
      "Epoch 10, 20% \t train_loss: 0.000257 took: 1.56s\n",
      "Epoch 10, 30% \t train_loss: 0.000277 took: 1.39s\n",
      "Epoch 10, 40% \t train_loss: 0.000260 took: 1.41s\n",
      "Epoch 10, 50% \t train_loss: 0.000520 took: 1.39s\n",
      "Epoch 10, 60% \t train_loss: 0.000154 took: 1.44s\n",
      "Epoch 10, 70% \t train_loss: 0.000136 took: 1.47s\n",
      "Epoch 10, 80% \t train_loss: 0.000195 took: 1.60s\n",
      "Epoch 10, 90% \t train_loss: 0.000239 took: 1.44s\n",
      "Validation loss = 0.06\n",
      "Training finished, took 209.97s\n",
      "[[  2.17500000e+03   2.00000000e+00]\n",
      " [  3.00000000e+02   5.00000000e+02]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 1.242650 took: 3.34s\n",
      "Epoch 1, 20% \t train_loss: 0.303182 took: 1.53s\n",
      "Epoch 1, 30% \t train_loss: 0.217525 took: 1.39s\n",
      "Epoch 1, 40% \t train_loss: 0.172553 took: 1.36s\n",
      "Epoch 1, 50% \t train_loss: 0.141191 took: 1.33s\n",
      "Epoch 1, 60% \t train_loss: 0.110673 took: 1.35s\n",
      "Epoch 1, 70% \t train_loss: 0.090558 took: 1.36s\n",
      "Epoch 1, 80% \t train_loss: 0.063993 took: 1.31s\n",
      "Epoch 1, 90% \t train_loss: 0.056713 took: 1.34s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 19.85s\n",
      "Epoch 2, 10% \t train_loss: 0.039587 took: 2.70s\n",
      "Epoch 2, 20% \t train_loss: 0.034745 took: 1.51s\n",
      "Epoch 2, 30% \t train_loss: 0.031783 took: 1.47s\n",
      "Epoch 2, 40% \t train_loss: 0.029440 took: 1.41s\n",
      "Epoch 2, 50% \t train_loss: 0.022264 took: 1.35s\n",
      "Epoch 2, 60% \t train_loss: 0.025240 took: 1.38s\n",
      "Epoch 2, 70% \t train_loss: 0.022041 took: 1.42s\n",
      "Epoch 2, 80% \t train_loss: 0.015153 took: 1.44s\n",
      "Epoch 2, 90% \t train_loss: 0.011952 took: 1.35s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 39.94s\n",
      "Epoch 3, 10% \t train_loss: 0.012322 took: 2.82s\n",
      "Epoch 3, 20% \t train_loss: 0.009024 took: 1.54s\n",
      "Epoch 3, 30% \t train_loss: 0.009043 took: 1.35s\n",
      "Epoch 3, 40% \t train_loss: 0.008605 took: 1.41s\n",
      "Epoch 3, 50% \t train_loss: 0.009345 took: 1.39s\n",
      "Epoch 3, 60% \t train_loss: 0.006975 took: 1.42s\n",
      "Epoch 3, 70% \t train_loss: 0.005732 took: 1.40s\n",
      "Epoch 3, 80% \t train_loss: 0.005459 took: 1.39s\n",
      "Epoch 3, 90% \t train_loss: 0.009184 took: 1.41s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 59.58s\n",
      "Epoch 4, 10% \t train_loss: 0.005716 took: 3.13s\n",
      "Epoch 4, 20% \t train_loss: 0.004434 took: 1.48s\n",
      "Epoch 4, 30% \t train_loss: 0.006415 took: 1.41s\n",
      "Epoch 4, 40% \t train_loss: 0.003324 took: 1.56s\n",
      "Epoch 4, 50% \t train_loss: 0.003372 took: 1.43s\n",
      "Epoch 4, 60% \t train_loss: 0.003162 took: 1.40s\n",
      "Epoch 4, 70% \t train_loss: 0.005468 took: 1.38s\n",
      "Epoch 4, 80% \t train_loss: 0.002717 took: 1.32s\n",
      "Epoch 4, 90% \t train_loss: 0.001960 took: 1.41s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 79.76s\n",
      "Epoch 5, 10% \t train_loss: 0.002037 took: 3.18s\n",
      "Epoch 5, 20% \t train_loss: 0.002018 took: 1.50s\n",
      "Epoch 5, 30% \t train_loss: 0.002502 took: 1.36s\n",
      "Epoch 5, 40% \t train_loss: 0.001492 took: 1.50s\n",
      "Epoch 5, 50% \t train_loss: 0.001375 took: 1.45s\n",
      "Epoch 5, 60% \t train_loss: 0.001857 took: 1.43s\n",
      "Epoch 5, 70% \t train_loss: 0.002563 took: 1.35s\n",
      "Epoch 5, 80% \t train_loss: 0.001008 took: 1.54s\n",
      "Epoch 5, 90% \t train_loss: 0.003828 took: 1.58s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 100.57s\n",
      "Epoch 6, 10% \t train_loss: 0.001672 took: 3.65s\n",
      "Epoch 6, 20% \t train_loss: 0.000944 took: 1.63s\n",
      "Epoch 6, 30% \t train_loss: 0.000976 took: 1.54s\n",
      "Epoch 6, 40% \t train_loss: 0.000728 took: 1.47s\n",
      "Epoch 6, 50% \t train_loss: 0.000656 took: 1.41s\n",
      "Epoch 6, 60% \t train_loss: 0.000993 took: 1.38s\n",
      "Epoch 6, 70% \t train_loss: 0.000644 took: 1.38s\n",
      "Epoch 6, 80% \t train_loss: 0.002090 took: 1.40s\n",
      "Epoch 6, 90% \t train_loss: 0.002071 took: 1.41s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 121.36s\n",
      "Epoch 7, 10% \t train_loss: 0.001154 took: 3.14s\n",
      "Epoch 7, 20% \t train_loss: 0.000572 took: 1.59s\n",
      "Epoch 7, 30% \t train_loss: 0.000587 took: 1.41s\n",
      "Epoch 7, 40% \t train_loss: 0.000630 took: 1.36s\n",
      "Epoch 7, 50% \t train_loss: 0.000739 took: 1.38s\n",
      "Epoch 7, 60% \t train_loss: 0.000990 took: 1.38s\n",
      "Epoch 7, 70% \t train_loss: 0.000357 took: 1.40s\n",
      "Epoch 7, 80% \t train_loss: 0.000635 took: 1.39s\n",
      "Epoch 7, 90% \t train_loss: 0.000389 took: 1.33s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 141.11s\n",
      "Epoch 8, 10% \t train_loss: 0.000689 took: 2.90s\n",
      "Epoch 8, 20% \t train_loss: 0.000291 took: 1.46s\n",
      "Epoch 8, 30% \t train_loss: 0.000285 took: 1.46s\n",
      "Epoch 8, 40% \t train_loss: 0.000232 took: 1.43s\n",
      "Epoch 8, 50% \t train_loss: 0.000244 took: 1.42s\n",
      "Epoch 8, 60% \t train_loss: 0.000249 took: 1.33s\n",
      "Epoch 8, 70% \t train_loss: 0.000243 took: 1.49s\n",
      "Epoch 8, 80% \t train_loss: 0.001218 took: 1.69s\n",
      "Epoch 8, 90% \t train_loss: 0.000234 took: 1.37s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 161.20s\n",
      "Epoch 9, 10% \t train_loss: 0.000288 took: 2.79s\n",
      "Epoch 9, 20% \t train_loss: 0.000242 took: 1.56s\n",
      "Epoch 9, 30% \t train_loss: 0.000217 took: 1.39s\n",
      "Epoch 9, 40% \t train_loss: 0.000204 took: 1.36s\n",
      "Epoch 9, 50% \t train_loss: 0.000142 took: 1.38s\n",
      "Epoch 9, 60% \t train_loss: 0.000182 took: 1.38s\n",
      "Epoch 9, 70% \t train_loss: 0.000162 took: 1.37s\n",
      "Epoch 9, 80% \t train_loss: 0.000136 took: 1.39s\n",
      "Epoch 9, 90% \t train_loss: 0.000166 took: 1.48s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 181.34s\n",
      "Epoch 10, 10% \t train_loss: 0.000154 took: 2.99s\n",
      "Epoch 10, 20% \t train_loss: 0.000201 took: 1.46s\n",
      "Epoch 10, 30% \t train_loss: 0.000111 took: 1.38s\n",
      "Epoch 10, 40% \t train_loss: 0.000107 took: 1.36s\n",
      "Epoch 10, 50% \t train_loss: 0.000093 took: 1.36s\n",
      "Epoch 10, 60% \t train_loss: 0.000082 took: 1.35s\n",
      "Epoch 10, 70% \t train_loss: 0.000081 took: 1.46s\n",
      "Epoch 10, 80% \t train_loss: 0.000109 took: 1.41s\n",
      "Epoch 10, 90% \t train_loss: 0.000089 took: 1.36s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 201.13s\n",
      "[[  2.71900000e+03   2.00000000e+00]\n",
      " [  3.00000000e+02   7.00000000e+02]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 10% \t train_loss: 1.117325 took: 3.71s\n",
      "Epoch 1, 20% \t train_loss: 0.431910 took: 1.34s\n",
      "Epoch 1, 30% \t train_loss: 0.239685 took: 1.32s\n",
      "Epoch 1, 40% \t train_loss: 0.183068 took: 1.30s\n",
      "Epoch 1, 50% \t train_loss: 0.160724 took: 1.38s\n",
      "Epoch 1, 60% \t train_loss: 0.121515 took: 1.32s\n",
      "Epoch 1, 70% \t train_loss: 0.110246 took: 1.35s\n",
      "Epoch 1, 80% \t train_loss: 0.080216 took: 1.29s\n",
      "Epoch 1, 90% \t train_loss: 0.078324 took: 1.36s\n",
      "Validation loss = 0.06\n",
      "Training finished, took 19.84s\n",
      "Epoch 2, 10% \t train_loss: 0.051761 took: 3.44s\n",
      "Epoch 2, 20% \t train_loss: 0.047831 took: 1.32s\n",
      "Epoch 2, 30% \t train_loss: 0.044277 took: 1.38s\n",
      "Epoch 2, 40% \t train_loss: 0.039589 took: 1.40s\n",
      "Epoch 2, 50% \t train_loss: 0.037968 took: 1.29s\n",
      "Epoch 2, 60% \t train_loss: 0.035935 took: 1.33s\n",
      "Epoch 2, 70% \t train_loss: 0.025880 took: 1.34s\n",
      "Epoch 2, 80% \t train_loss: 0.031142 took: 1.29s\n",
      "Epoch 2, 90% \t train_loss: 0.025827 took: 1.40s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 40.12s\n",
      "Epoch 3, 10% \t train_loss: 0.016158 took: 3.14s\n",
      "Epoch 3, 20% \t train_loss: 0.015544 took: 1.37s\n",
      "Epoch 3, 30% \t train_loss: 0.020069 took: 1.31s\n",
      "Epoch 3, 40% \t train_loss: 0.018915 took: 1.35s\n",
      "Epoch 3, 50% \t train_loss: 0.018639 took: 1.42s\n",
      "Epoch 3, 60% \t train_loss: 0.013689 took: 1.36s\n",
      "Epoch 3, 70% \t train_loss: 0.011185 took: 1.33s\n",
      "Epoch 3, 80% \t train_loss: 0.010311 took: 1.38s\n",
      "Epoch 3, 90% \t train_loss: 0.010559 took: 1.36s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 59.47s\n",
      "Epoch 4, 10% \t train_loss: 0.007925 took: 3.17s\n",
      "Epoch 4, 20% \t train_loss: 0.008675 took: 1.55s\n",
      "Epoch 4, 30% \t train_loss: 0.007433 took: 1.39s\n",
      "Epoch 4, 40% \t train_loss: 0.006878 took: 1.32s\n",
      "Epoch 4, 50% \t train_loss: 0.008516 took: 1.37s\n",
      "Epoch 4, 60% \t train_loss: 0.007849 took: 1.47s\n",
      "Epoch 4, 70% \t train_loss: 0.008062 took: 1.33s\n",
      "Epoch 4, 80% \t train_loss: 0.004907 took: 1.36s\n",
      "Epoch 4, 90% \t train_loss: 0.005451 took: 1.37s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 79.39s\n",
      "Epoch 5, 10% \t train_loss: 0.003795 took: 3.45s\n",
      "Epoch 5, 20% \t train_loss: 0.003380 took: 1.31s\n",
      "Epoch 5, 30% \t train_loss: 0.004022 took: 1.38s\n",
      "Epoch 5, 40% \t train_loss: 0.004108 took: 1.37s\n",
      "Epoch 5, 50% \t train_loss: 0.006134 took: 1.30s\n",
      "Epoch 5, 60% \t train_loss: 0.003208 took: 1.34s\n",
      "Epoch 5, 70% \t train_loss: 0.004187 took: 1.44s\n",
      "Epoch 5, 80% \t train_loss: 0.003682 took: 1.36s\n",
      "Epoch 5, 90% \t train_loss: 0.008244 took: 1.32s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 99.45s\n",
      "Epoch 6, 10% \t train_loss: 0.002131 took: 2.89s\n",
      "Epoch 6, 20% \t train_loss: 0.002881 took: 1.46s\n",
      "Epoch 6, 30% \t train_loss: 0.002453 took: 1.36s\n",
      "Epoch 6, 40% \t train_loss: 0.002075 took: 1.46s\n",
      "Epoch 6, 50% \t train_loss: 0.002259 took: 1.43s\n",
      "Epoch 6, 60% \t train_loss: 0.002646 took: 1.36s\n",
      "Epoch 6, 70% \t train_loss: 0.006678 took: 1.42s\n",
      "Epoch 6, 80% \t train_loss: 0.002094 took: 1.40s\n",
      "Epoch 6, 90% \t train_loss: 0.002465 took: 1.53s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 119.21s\n",
      "Epoch 7, 10% \t train_loss: 0.002651 took: 3.15s\n",
      "Epoch 7, 20% \t train_loss: 0.001719 took: 1.60s\n",
      "Epoch 7, 30% \t train_loss: 0.001365 took: 1.36s\n",
      "Epoch 7, 40% \t train_loss: 0.001308 took: 1.36s\n",
      "Epoch 7, 50% \t train_loss: 0.001356 took: 1.43s\n",
      "Epoch 7, 60% \t train_loss: 0.001570 took: 1.38s\n",
      "Epoch 7, 70% \t train_loss: 0.002021 took: 1.37s\n",
      "Epoch 7, 80% \t train_loss: 0.004252 took: 1.38s\n",
      "Epoch 7, 90% \t train_loss: 0.001691 took: 1.42s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 139.46s\n",
      "Epoch 8, 10% \t train_loss: 0.001100 took: 3.13s\n",
      "Epoch 8, 20% \t train_loss: 0.000969 took: 1.51s\n",
      "Epoch 8, 30% \t train_loss: 0.001528 took: 1.38s\n",
      "Epoch 8, 40% \t train_loss: 0.000734 took: 1.45s\n",
      "Epoch 8, 50% \t train_loss: 0.001287 took: 1.52s\n",
      "Epoch 8, 60% \t train_loss: 0.001514 took: 1.42s\n",
      "Epoch 8, 70% \t train_loss: 0.000931 took: 1.48s\n",
      "Epoch 8, 80% \t train_loss: 0.003301 took: 1.39s\n",
      "Epoch 8, 90% \t train_loss: 0.001013 took: 1.45s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 159.80s\n",
      "Epoch 9, 10% \t train_loss: 0.000723 took: 3.12s\n",
      "Epoch 9, 20% \t train_loss: 0.001011 took: 1.56s\n",
      "Epoch 9, 30% \t train_loss: 0.000661 took: 1.47s\n",
      "Epoch 9, 40% \t train_loss: 0.002451 took: 1.57s\n",
      "Epoch 9, 50% \t train_loss: 0.000721 took: 1.37s\n",
      "Epoch 9, 60% \t train_loss: 0.000581 took: 1.53s\n",
      "Epoch 9, 70% \t train_loss: 0.000526 took: 1.43s\n",
      "Epoch 9, 80% \t train_loss: 0.000786 took: 1.36s\n",
      "Epoch 9, 90% \t train_loss: 0.000684 took: 1.45s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 180.65s\n",
      "Epoch 10, 10% \t train_loss: 0.001815 took: 3.22s\n",
      "Epoch 10, 20% \t train_loss: 0.000446 took: 1.57s\n",
      "Epoch 10, 30% \t train_loss: 0.000700 took: 1.42s\n",
      "Epoch 10, 40% \t train_loss: 0.000364 took: 1.41s\n",
      "Epoch 10, 50% \t train_loss: 0.000485 took: 1.40s\n",
      "Epoch 10, 60% \t train_loss: 0.000405 took: 1.37s\n",
      "Epoch 10, 70% \t train_loss: 0.000380 took: 1.40s\n",
      "Epoch 10, 80% \t train_loss: 0.000581 took: 1.45s\n",
      "Epoch 10, 90% \t train_loss: 0.000502 took: 1.43s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 200.89s\n",
      "[[  3.26300000e+03   2.00000000e+00]\n",
      " [  3.00000000e+02   9.00000000e+02]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 2.528680 took: 3.25s\n",
      "Epoch 1, 20% \t train_loss: 0.441720 took: 1.37s\n",
      "Epoch 1, 30% \t train_loss: 0.269785 took: 1.27s\n",
      "Epoch 1, 40% \t train_loss: 0.178063 took: 1.36s\n",
      "Epoch 1, 50% \t train_loss: 0.145691 took: 1.32s\n",
      "Epoch 1, 60% \t train_loss: 0.116247 took: 1.29s\n",
      "Epoch 1, 70% \t train_loss: 0.086720 took: 1.34s\n",
      "Epoch 1, 80% \t train_loss: 0.074122 took: 1.41s\n",
      "Epoch 1, 90% \t train_loss: 0.061177 took: 1.29s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 19.46s\n",
      "Epoch 2, 10% \t train_loss: 0.040108 took: 2.85s\n",
      "Epoch 2, 20% \t train_loss: 0.032793 took: 1.59s\n",
      "Epoch 2, 30% \t train_loss: 0.040083 took: 1.42s\n",
      "Epoch 2, 40% \t train_loss: 0.029870 took: 1.44s\n",
      "Epoch 2, 50% \t train_loss: 0.028728 took: 1.40s\n",
      "Epoch 2, 60% \t train_loss: 0.021669 took: 1.42s\n",
      "Epoch 2, 70% \t train_loss: 0.018658 took: 1.37s\n",
      "Epoch 2, 80% \t train_loss: 0.017327 took: 1.39s\n",
      "Epoch 2, 90% \t train_loss: 0.017373 took: 1.40s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 39.69s\n",
      "Epoch 3, 10% \t train_loss: 0.011090 took: 2.72s\n",
      "Epoch 3, 20% \t train_loss: 0.014174 took: 1.56s\n",
      "Epoch 3, 30% \t train_loss: 0.013092 took: 1.41s\n",
      "Epoch 3, 40% \t train_loss: 0.012884 took: 1.37s\n",
      "Epoch 3, 50% \t train_loss: 0.008289 took: 1.35s\n",
      "Epoch 3, 60% \t train_loss: 0.008199 took: 1.36s\n",
      "Epoch 3, 70% \t train_loss: 0.006533 took: 1.39s\n",
      "Epoch 3, 80% \t train_loss: 0.007268 took: 1.32s\n",
      "Epoch 3, 90% \t train_loss: 0.007141 took: 1.37s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 58.95s\n",
      "Epoch 4, 10% \t train_loss: 0.006392 took: 3.37s\n",
      "Epoch 4, 20% \t train_loss: 0.003913 took: 1.42s\n",
      "Epoch 4, 30% \t train_loss: 0.004812 took: 1.39s\n",
      "Epoch 4, 40% \t train_loss: 0.006239 took: 1.56s\n",
      "Epoch 4, 50% \t train_loss: 0.006960 took: 1.43s\n",
      "Epoch 4, 60% \t train_loss: 0.004206 took: 1.32s\n",
      "Epoch 4, 70% \t train_loss: 0.002320 took: 1.42s\n",
      "Epoch 4, 80% \t train_loss: 0.003103 took: 1.43s\n",
      "Epoch 4, 90% \t train_loss: 0.002523 took: 1.55s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 79.54s\n",
      "Epoch 5, 10% \t train_loss: 0.002330 took: 2.91s\n",
      "Epoch 5, 20% \t train_loss: 0.002788 took: 1.61s\n",
      "Epoch 5, 30% \t train_loss: 0.004345 took: 1.42s\n",
      "Epoch 5, 40% \t train_loss: 0.001638 took: 1.41s\n",
      "Epoch 5, 50% \t train_loss: 0.002001 took: 1.47s\n",
      "Epoch 5, 60% \t train_loss: 0.001689 took: 1.38s\n",
      "Epoch 5, 70% \t train_loss: 0.001722 took: 1.41s\n",
      "Epoch 5, 80% \t train_loss: 0.001197 took: 1.36s\n",
      "Epoch 5, 90% \t train_loss: 0.001224 took: 1.45s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 99.53s\n",
      "Epoch 6, 10% \t train_loss: 0.001381 took: 3.46s\n",
      "Epoch 6, 20% \t train_loss: 0.002220 took: 1.47s\n",
      "Epoch 6, 30% \t train_loss: 0.001075 took: 1.36s\n",
      "Epoch 6, 40% \t train_loss: 0.000945 took: 1.41s\n",
      "Epoch 6, 50% \t train_loss: 0.001013 took: 1.40s\n",
      "Epoch 6, 60% \t train_loss: 0.002758 took: 1.38s\n",
      "Epoch 6, 70% \t train_loss: 0.000760 took: 1.43s\n",
      "Epoch 6, 80% \t train_loss: 0.001156 took: 1.44s\n",
      "Epoch 6, 90% \t train_loss: 0.000678 took: 1.40s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 119.75s\n",
      "Epoch 7, 10% \t train_loss: 0.001072 took: 3.08s\n",
      "Epoch 7, 20% \t train_loss: 0.000658 took: 1.60s\n",
      "Epoch 7, 30% \t train_loss: 0.000862 took: 1.43s\n",
      "Epoch 7, 40% \t train_loss: 0.000510 took: 1.46s\n",
      "Epoch 7, 50% \t train_loss: 0.000543 took: 1.38s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, 60% \t train_loss: 0.000573 took: 1.40s\n",
      "Epoch 7, 70% \t train_loss: 0.000441 took: 1.43s\n",
      "Epoch 7, 80% \t train_loss: 0.001999 took: 1.38s\n",
      "Epoch 7, 90% \t train_loss: 0.000548 took: 1.40s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 139.86s\n",
      "Epoch 8, 10% \t train_loss: 0.000422 took: 3.30s\n",
      "Epoch 8, 20% \t train_loss: 0.000306 took: 1.37s\n",
      "Epoch 8, 30% \t train_loss: 0.000370 took: 1.42s\n",
      "Epoch 8, 40% \t train_loss: 0.000500 took: 1.40s\n",
      "Epoch 8, 50% \t train_loss: 0.000352 took: 1.39s\n",
      "Epoch 8, 60% \t train_loss: 0.000889 took: 1.37s\n",
      "Epoch 8, 70% \t train_loss: 0.000399 took: 1.41s\n",
      "Epoch 8, 80% \t train_loss: 0.000275 took: 1.43s\n",
      "Epoch 8, 90% \t train_loss: 0.000258 took: 1.44s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 159.92s\n",
      "Epoch 9, 10% \t train_loss: 0.000257 took: 3.30s\n",
      "Epoch 9, 20% \t train_loss: 0.000208 took: 1.54s\n",
      "Epoch 9, 30% \t train_loss: 0.000188 took: 1.49s\n",
      "Epoch 9, 40% \t train_loss: 0.000500 took: 1.42s\n",
      "Epoch 9, 50% \t train_loss: 0.000253 took: 1.38s\n",
      "Epoch 9, 60% \t train_loss: 0.000183 took: 1.46s\n",
      "Epoch 9, 70% \t train_loss: 0.000175 took: 1.44s\n",
      "Epoch 9, 80% \t train_loss: 0.000222 took: 1.40s\n",
      "Epoch 9, 90% \t train_loss: 0.000150 took: 1.48s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 180.47s\n",
      "Epoch 10, 10% \t train_loss: 0.000318 took: 2.91s\n",
      "Epoch 10, 20% \t train_loss: 0.000105 took: 1.56s\n",
      "Epoch 10, 30% \t train_loss: 0.000130 took: 1.42s\n",
      "Epoch 10, 40% \t train_loss: 0.000104 took: 1.45s\n",
      "Epoch 10, 50% \t train_loss: 0.000164 took: 1.44s\n",
      "Epoch 10, 60% \t train_loss: 0.000119 took: 1.44s\n",
      "Epoch 10, 70% \t train_loss: 0.000153 took: 1.39s\n",
      "Epoch 10, 80% \t train_loss: 0.000116 took: 1.47s\n",
      "Epoch 10, 90% \t train_loss: 0.000084 took: 1.43s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 201.16s\n",
      "[[  3.80700000e+03   2.00000000e+00]\n",
      " [  4.00000000e+02   1.00000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 0.955617 took: 3.94s\n",
      "Epoch 1, 20% \t train_loss: 0.353114 took: 1.55s\n",
      "Epoch 1, 30% \t train_loss: 0.244279 took: 1.36s\n",
      "Epoch 1, 40% \t train_loss: 0.174213 took: 1.37s\n",
      "Epoch 1, 50% \t train_loss: 0.142413 took: 1.34s\n",
      "Epoch 1, 60% \t train_loss: 0.098471 took: 1.35s\n",
      "Epoch 1, 70% \t train_loss: 0.070874 took: 1.32s\n",
      "Epoch 1, 80% \t train_loss: 0.075767 took: 1.39s\n",
      "Epoch 1, 90% \t train_loss: 0.061673 took: 1.33s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 20.56s\n",
      "Epoch 2, 10% \t train_loss: 0.043624 took: 2.72s\n",
      "Epoch 2, 20% \t train_loss: 0.036005 took: 1.53s\n",
      "Epoch 2, 30% \t train_loss: 0.028374 took: 1.37s\n",
      "Epoch 2, 40% \t train_loss: 0.027211 took: 1.36s\n",
      "Epoch 2, 50% \t train_loss: 0.027275 took: 1.38s\n",
      "Epoch 2, 60% \t train_loss: 0.019995 took: 1.38s\n",
      "Epoch 2, 70% \t train_loss: 0.018689 took: 1.43s\n",
      "Epoch 2, 80% \t train_loss: 0.013994 took: 1.34s\n",
      "Epoch 2, 90% \t train_loss: 0.013677 took: 1.40s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 40.05s\n",
      "Epoch 3, 10% \t train_loss: 0.011241 took: 3.35s\n",
      "Epoch 3, 20% \t train_loss: 0.009858 took: 1.38s\n",
      "Epoch 3, 30% \t train_loss: 0.008217 took: 1.34s\n",
      "Epoch 3, 40% \t train_loss: 0.006973 took: 1.41s\n",
      "Epoch 3, 50% \t train_loss: 0.012174 took: 1.37s\n",
      "Epoch 3, 60% \t train_loss: 0.006970 took: 1.42s\n",
      "Epoch 3, 70% \t train_loss: 0.006932 took: 1.40s\n",
      "Epoch 3, 80% \t train_loss: 0.007666 took: 1.55s\n",
      "Epoch 3, 90% \t train_loss: 0.008472 took: 1.42s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 60.28s\n",
      "Epoch 4, 10% \t train_loss: 0.003946 took: 2.85s\n",
      "Epoch 4, 20% \t train_loss: 0.003503 took: 1.60s\n",
      "Epoch 4, 30% \t train_loss: 0.002976 took: 1.42s\n",
      "Epoch 4, 40% \t train_loss: 0.007769 took: 1.36s\n",
      "Epoch 4, 50% \t train_loss: 0.004027 took: 1.38s\n",
      "Epoch 4, 60% \t train_loss: 0.004005 took: 1.45s\n",
      "Epoch 4, 70% \t train_loss: 0.004076 took: 1.64s\n",
      "Epoch 4, 80% \t train_loss: 0.004186 took: 1.34s\n",
      "Epoch 4, 90% \t train_loss: 0.002935 took: 1.55s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 80.86s\n",
      "Epoch 5, 10% \t train_loss: 0.003952 took: 3.19s\n",
      "Epoch 5, 20% \t train_loss: 0.002703 took: 1.59s\n",
      "Epoch 5, 30% \t train_loss: 0.002568 took: 1.33s\n",
      "Epoch 5, 40% \t train_loss: 0.001448 took: 1.44s\n",
      "Epoch 5, 50% \t train_loss: 0.002124 took: 1.60s\n",
      "Epoch 5, 60% \t train_loss: 0.001456 took: 1.44s\n",
      "Epoch 5, 70% \t train_loss: 0.002332 took: 1.47s\n",
      "Epoch 5, 80% \t train_loss: 0.001182 took: 1.38s\n",
      "Epoch 5, 90% \t train_loss: 0.002302 took: 1.48s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 101.50s\n",
      "Epoch 6, 10% \t train_loss: 0.003032 took: 2.82s\n",
      "Epoch 6, 20% \t train_loss: 0.001073 took: 1.54s\n",
      "Epoch 6, 30% \t train_loss: 0.001289 took: 1.51s\n",
      "Epoch 6, 40% \t train_loss: 0.000998 took: 1.60s\n",
      "Epoch 6, 50% \t train_loss: 0.001524 took: 1.54s\n",
      "Epoch 6, 60% \t train_loss: 0.000789 took: 1.32s\n",
      "Epoch 6, 70% \t train_loss: 0.000711 took: 1.48s\n",
      "Epoch 6, 80% \t train_loss: 0.000741 took: 1.46s\n",
      "Epoch 6, 90% \t train_loss: 0.000539 took: 1.43s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 121.70s\n",
      "Epoch 7, 10% \t train_loss: 0.000625 took: 2.94s\n",
      "Epoch 7, 20% \t train_loss: 0.000822 took: 1.55s\n",
      "Epoch 7, 30% \t train_loss: 0.000643 took: 1.38s\n",
      "Epoch 7, 40% \t train_loss: 0.000520 took: 1.37s\n",
      "Epoch 7, 50% \t train_loss: 0.000717 took: 1.42s\n",
      "Epoch 7, 60% \t train_loss: 0.000429 took: 1.39s\n",
      "Epoch 7, 70% \t train_loss: 0.000783 took: 1.39s\n",
      "Epoch 7, 80% \t train_loss: 0.001333 took: 1.41s\n",
      "Epoch 7, 90% \t train_loss: 0.000350 took: 1.42s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 141.62s\n",
      "Epoch 8, 10% \t train_loss: 0.000286 took: 2.75s\n",
      "Epoch 8, 20% \t train_loss: 0.000260 took: 1.57s\n",
      "Epoch 8, 30% \t train_loss: 0.000542 took: 1.41s\n",
      "Epoch 8, 40% \t train_loss: 0.000395 took: 1.43s\n",
      "Epoch 8, 50% \t train_loss: 0.000642 took: 1.38s\n",
      "Epoch 8, 60% \t train_loss: 0.000273 took: 1.49s\n",
      "Epoch 8, 70% \t train_loss: 0.000337 took: 1.39s\n",
      "Epoch 8, 80% \t train_loss: 0.000395 took: 1.48s\n",
      "Epoch 8, 90% \t train_loss: 0.000350 took: 1.35s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 161.48s\n",
      "Epoch 9, 10% \t train_loss: 0.000197 took: 3.21s\n",
      "Epoch 9, 20% \t train_loss: 0.000164 took: 1.45s\n",
      "Epoch 9, 30% \t train_loss: 0.000269 took: 1.41s\n",
      "Epoch 9, 40% \t train_loss: 0.000337 took: 1.40s\n",
      "Epoch 9, 50% \t train_loss: 0.000214 took: 1.41s\n",
      "Epoch 9, 60% \t train_loss: 0.000219 took: 1.37s\n",
      "Epoch 9, 70% \t train_loss: 0.000089 took: 1.36s\n",
      "Epoch 9, 80% \t train_loss: 0.000129 took: 1.39s\n",
      "Epoch 9, 90% \t train_loss: 0.000096 took: 1.41s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 181.40s\n",
      "Epoch 10, 10% \t train_loss: 0.000117 took: 3.22s\n",
      "Epoch 10, 20% \t train_loss: 0.000120 took: 1.61s\n",
      "Epoch 10, 30% \t train_loss: 0.000106 took: 1.48s\n",
      "Epoch 10, 40% \t train_loss: 0.000087 took: 1.62s\n",
      "Epoch 10, 50% \t train_loss: 0.000164 took: 1.37s\n",
      "Epoch 10, 60% \t train_loss: 0.000131 took: 1.42s\n",
      "Epoch 10, 70% \t train_loss: 0.000128 took: 1.39s\n",
      "Epoch 10, 80% \t train_loss: 0.000073 took: 1.46s\n",
      "Epoch 10, 90% \t train_loss: 0.000120 took: 1.53s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 202.26s\n",
      "[[  4.35100000e+03   3.00000000e+00]\n",
      " [  4.00000000e+02   1.20000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 2.017724 took: 3.52s\n",
      "Epoch 1, 20% \t train_loss: 0.491815 took: 1.49s\n",
      "Epoch 1, 30% \t train_loss: 0.328439 took: 1.35s\n",
      "Epoch 1, 40% \t train_loss: 0.262024 took: 1.37s\n",
      "Epoch 1, 50% \t train_loss: 0.207120 took: 1.31s\n",
      "Epoch 1, 60% \t train_loss: 0.186363 took: 1.34s\n",
      "Epoch 1, 70% \t train_loss: 0.131315 took: 1.33s\n",
      "Epoch 1, 80% \t train_loss: 0.103178 took: 1.33s\n",
      "Epoch 1, 90% \t train_loss: 0.090601 took: 1.36s\n",
      "Validation loss = 0.06\n",
      "Training finished, took 19.98s\n",
      "Epoch 2, 10% \t train_loss: 0.055971 took: 3.20s\n",
      "Epoch 2, 20% \t train_loss: 0.049182 took: 1.42s\n",
      "Epoch 2, 30% \t train_loss: 0.041491 took: 1.37s\n",
      "Epoch 2, 40% \t train_loss: 0.050073 took: 1.35s\n",
      "Epoch 2, 50% \t train_loss: 0.037443 took: 1.32s\n",
      "Epoch 2, 60% \t train_loss: 0.027554 took: 1.36s\n",
      "Epoch 2, 70% \t train_loss: 0.030332 took: 1.36s\n",
      "Epoch 2, 80% \t train_loss: 0.024055 took: 1.39s\n",
      "Epoch 2, 90% \t train_loss: 0.021694 took: 1.33s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 39.57s\n",
      "Epoch 3, 10% \t train_loss: 0.015576 took: 2.76s\n",
      "Epoch 3, 20% \t train_loss: 0.016550 took: 1.43s\n",
      "Epoch 3, 30% \t train_loss: 0.013933 took: 1.43s\n",
      "Epoch 3, 40% \t train_loss: 0.013198 took: 1.46s\n",
      "Epoch 3, 50% \t train_loss: 0.012016 took: 1.40s\n",
      "Epoch 3, 60% \t train_loss: 0.012437 took: 1.35s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, 70% \t train_loss: 0.009789 took: 1.43s\n",
      "Epoch 3, 80% \t train_loss: 0.009077 took: 1.39s\n",
      "Epoch 3, 90% \t train_loss: 0.013261 took: 1.46s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 59.37s\n",
      "Epoch 4, 10% \t train_loss: 0.006216 took: 2.76s\n",
      "Epoch 4, 20% \t train_loss: 0.005166 took: 1.55s\n",
      "Epoch 4, 30% \t train_loss: 0.004946 took: 1.37s\n",
      "Epoch 4, 40% \t train_loss: 0.004537 took: 1.41s\n",
      "Epoch 4, 50% \t train_loss: 0.004314 took: 1.37s\n",
      "Epoch 4, 60% \t train_loss: 0.004563 took: 1.41s\n",
      "Epoch 4, 70% \t train_loss: 0.010424 took: 1.33s\n",
      "Epoch 4, 80% \t train_loss: 0.006870 took: 1.41s\n",
      "Epoch 4, 90% \t train_loss: 0.003595 took: 1.34s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 78.67s\n",
      "Epoch 5, 10% \t train_loss: 0.002997 took: 2.75s\n",
      "Epoch 5, 20% \t train_loss: 0.002698 took: 1.54s\n",
      "Epoch 5, 30% \t train_loss: 0.003831 took: 1.33s\n",
      "Epoch 5, 40% \t train_loss: 0.003200 took: 1.37s\n",
      "Epoch 5, 50% \t train_loss: 0.004385 took: 1.39s\n",
      "Epoch 5, 60% \t train_loss: 0.003355 took: 1.38s\n",
      "Epoch 5, 70% \t train_loss: 0.002164 took: 1.32s\n",
      "Epoch 5, 80% \t train_loss: 0.001374 took: 1.36s\n",
      "Epoch 5, 90% \t train_loss: 0.006334 took: 1.35s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 98.31s\n",
      "Epoch 6, 10% \t train_loss: 0.001414 took: 2.99s\n",
      "Epoch 6, 20% \t train_loss: 0.001630 took: 1.53s\n",
      "Epoch 6, 30% \t train_loss: 0.001311 took: 1.38s\n",
      "Epoch 6, 40% \t train_loss: 0.001690 took: 1.34s\n",
      "Epoch 6, 50% \t train_loss: 0.001857 took: 1.37s\n",
      "Epoch 6, 60% \t train_loss: 0.001044 took: 1.37s\n",
      "Epoch 6, 70% \t train_loss: 0.002272 took: 1.38s\n",
      "Epoch 6, 80% \t train_loss: 0.005028 took: 1.35s\n",
      "Epoch 6, 90% \t train_loss: 0.000997 took: 1.34s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 118.17s\n",
      "Epoch 7, 10% \t train_loss: 0.000775 took: 3.31s\n",
      "Epoch 7, 20% \t train_loss: 0.004196 took: 1.48s\n",
      "Epoch 7, 30% \t train_loss: 0.000885 took: 1.29s\n",
      "Epoch 7, 40% \t train_loss: 0.000861 took: 1.33s\n",
      "Epoch 7, 50% \t train_loss: 0.000764 took: 1.38s\n",
      "Epoch 7, 60% \t train_loss: 0.001157 took: 1.33s\n",
      "Epoch 7, 70% \t train_loss: 0.001895 took: 1.43s\n",
      "Epoch 7, 80% \t train_loss: 0.000813 took: 1.39s\n",
      "Epoch 7, 90% \t train_loss: 0.000821 took: 1.34s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 138.07s\n",
      "Epoch 8, 10% \t train_loss: 0.003219 took: 3.28s\n",
      "Epoch 8, 20% \t train_loss: 0.000728 took: 1.48s\n",
      "Epoch 8, 30% \t train_loss: 0.001072 took: 1.40s\n",
      "Epoch 8, 40% \t train_loss: 0.000427 took: 1.33s\n",
      "Epoch 8, 50% \t train_loss: 0.000764 took: 1.35s\n",
      "Epoch 8, 60% \t train_loss: 0.000553 took: 1.40s\n",
      "Epoch 8, 70% \t train_loss: 0.000573 took: 1.35s\n",
      "Epoch 8, 80% \t train_loss: 0.000737 took: 1.41s\n",
      "Epoch 8, 90% \t train_loss: 0.000381 took: 1.39s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 158.00s\n",
      "Epoch 9, 10% \t train_loss: 0.002186 took: 3.37s\n",
      "Epoch 9, 20% \t train_loss: 0.000781 took: 1.44s\n",
      "Epoch 9, 30% \t train_loss: 0.000382 took: 1.36s\n",
      "Epoch 9, 40% \t train_loss: 0.000405 took: 1.37s\n",
      "Epoch 9, 50% \t train_loss: 0.000449 took: 1.33s\n",
      "Epoch 9, 60% \t train_loss: 0.000512 took: 1.42s\n",
      "Epoch 9, 70% \t train_loss: 0.000422 took: 1.37s\n",
      "Epoch 9, 80% \t train_loss: 0.000281 took: 1.38s\n",
      "Epoch 9, 90% \t train_loss: 0.000243 took: 1.32s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 177.89s\n",
      "Epoch 10, 10% \t train_loss: 0.000331 took: 2.85s\n",
      "Epoch 10, 20% \t train_loss: 0.000215 took: 1.46s\n",
      "Epoch 10, 30% \t train_loss: 0.000249 took: 1.47s\n",
      "Epoch 10, 40% \t train_loss: 0.000364 took: 1.42s\n",
      "Epoch 10, 50% \t train_loss: 0.000212 took: 1.39s\n",
      "Epoch 10, 60% \t train_loss: 0.000388 took: 1.34s\n",
      "Epoch 10, 70% \t train_loss: 0.000281 took: 1.37s\n",
      "Epoch 10, 80% \t train_loss: 0.000182 took: 1.39s\n",
      "Epoch 10, 90% \t train_loss: 0.000372 took: 1.35s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 197.51s\n",
      "[[  4.89500000e+03   3.00000000e+00]\n",
      " [  4.00000000e+02   1.40000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 1.677645 took: 3.66s\n",
      "Epoch 1, 20% \t train_loss: 0.418460 took: 1.46s\n",
      "Epoch 1, 30% \t train_loss: 0.294786 took: 1.35s\n",
      "Epoch 1, 40% \t train_loss: 0.214606 took: 1.38s\n",
      "Epoch 1, 50% \t train_loss: 0.174428 took: 1.32s\n",
      "Epoch 1, 60% \t train_loss: 0.130075 took: 1.42s\n",
      "Epoch 1, 70% \t train_loss: 0.119529 took: 1.36s\n",
      "Epoch 1, 80% \t train_loss: 0.083782 took: 1.33s\n",
      "Epoch 1, 90% \t train_loss: 0.069910 took: 1.39s\n",
      "Validation loss = 0.06\n",
      "Training finished, took 20.23s\n",
      "Epoch 2, 10% \t train_loss: 0.056924 took: 2.91s\n",
      "Epoch 2, 20% \t train_loss: 0.053176 took: 1.50s\n",
      "Epoch 2, 30% \t train_loss: 0.040385 took: 1.48s\n",
      "Epoch 2, 40% \t train_loss: 0.042586 took: 1.39s\n",
      "Epoch 2, 50% \t train_loss: 0.029851 took: 1.41s\n",
      "Epoch 2, 60% \t train_loss: 0.027568 took: 1.41s\n",
      "Epoch 2, 70% \t train_loss: 0.031591 took: 1.36s\n",
      "Epoch 2, 80% \t train_loss: 0.025838 took: 1.39s\n",
      "Epoch 2, 90% \t train_loss: 0.028944 took: 1.42s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 40.03s\n",
      "Epoch 3, 10% \t train_loss: 0.018566 took: 2.86s\n",
      "Epoch 3, 20% \t train_loss: 0.016469 took: 1.71s\n",
      "Epoch 3, 30% \t train_loss: 0.017485 took: 1.65s\n",
      "Epoch 3, 40% \t train_loss: 0.014845 took: 1.35s\n",
      "Epoch 3, 50% \t train_loss: 0.013450 took: 1.40s\n",
      "Epoch 3, 60% \t train_loss: 0.013841 took: 1.40s\n",
      "Epoch 3, 70% \t train_loss: 0.011475 took: 1.38s\n",
      "Epoch 3, 80% \t train_loss: 0.015098 took: 1.41s\n",
      "Epoch 3, 90% \t train_loss: 0.006833 took: 1.43s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 60.02s\n",
      "Epoch 4, 10% \t train_loss: 0.008989 took: 3.28s\n",
      "Epoch 4, 20% \t train_loss: 0.009310 took: 1.45s\n",
      "Epoch 4, 30% \t train_loss: 0.006977 took: 1.38s\n",
      "Epoch 4, 40% \t train_loss: 0.006005 took: 1.42s\n",
      "Epoch 4, 50% \t train_loss: 0.006900 took: 1.53s\n",
      "Epoch 4, 60% \t train_loss: 0.007399 took: 1.41s\n",
      "Epoch 4, 70% \t train_loss: 0.003487 took: 1.42s\n",
      "Epoch 4, 80% \t train_loss: 0.006228 took: 1.49s\n",
      "Epoch 4, 90% \t train_loss: 0.004484 took: 1.60s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 80.67s\n",
      "Epoch 5, 10% \t train_loss: 0.004408 took: 3.06s\n",
      "Epoch 5, 20% \t train_loss: 0.003066 took: 1.55s\n",
      "Epoch 5, 30% \t train_loss: 0.003244 took: 1.39s\n",
      "Epoch 5, 40% \t train_loss: 0.003128 took: 1.38s\n",
      "Epoch 5, 50% \t train_loss: 0.002451 took: 1.43s\n",
      "Epoch 5, 60% \t train_loss: 0.003123 took: 1.42s\n",
      "Epoch 5, 70% \t train_loss: 0.005059 took: 1.39s\n",
      "Epoch 5, 80% \t train_loss: 0.004943 took: 1.36s\n",
      "Epoch 5, 90% \t train_loss: 0.003653 took: 1.39s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 100.48s\n",
      "Epoch 6, 10% \t train_loss: 0.004292 took: 2.78s\n",
      "Epoch 6, 20% \t train_loss: 0.002151 took: 1.53s\n",
      "Epoch 6, 30% \t train_loss: 0.001994 took: 1.48s\n",
      "Epoch 6, 40% \t train_loss: 0.003071 took: 1.48s\n",
      "Epoch 6, 50% \t train_loss: 0.001778 took: 1.47s\n",
      "Epoch 6, 60% \t train_loss: 0.001512 took: 1.54s\n",
      "Epoch 6, 70% \t train_loss: 0.002214 took: 1.47s\n",
      "Epoch 6, 80% \t train_loss: 0.001388 took: 1.47s\n",
      "Epoch 6, 90% \t train_loss: 0.000979 took: 1.40s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 120.72s\n",
      "Epoch 7, 10% \t train_loss: 0.001125 took: 3.39s\n",
      "Epoch 7, 20% \t train_loss: 0.001368 took: 1.55s\n",
      "Epoch 7, 30% \t train_loss: 0.001781 took: 1.40s\n",
      "Epoch 7, 40% \t train_loss: 0.001589 took: 1.42s\n",
      "Epoch 7, 50% \t train_loss: 0.000996 took: 1.40s\n",
      "Epoch 7, 60% \t train_loss: 0.000999 took: 1.46s\n",
      "Epoch 7, 70% \t train_loss: 0.000887 took: 1.44s\n",
      "Epoch 7, 80% \t train_loss: 0.002151 took: 1.39s\n",
      "Epoch 7, 90% \t train_loss: 0.001278 took: 1.42s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 141.20s\n",
      "Epoch 8, 10% \t train_loss: 0.001224 took: 3.39s\n",
      "Epoch 8, 20% \t train_loss: 0.000839 took: 1.48s\n",
      "Epoch 8, 30% \t train_loss: 0.000798 took: 1.39s\n",
      "Epoch 8, 40% \t train_loss: 0.000704 took: 1.41s\n",
      "Epoch 8, 50% \t train_loss: 0.000782 took: 1.42s\n",
      "Epoch 8, 60% \t train_loss: 0.001340 took: 1.60s\n",
      "Epoch 8, 70% \t train_loss: 0.000637 took: 1.45s\n",
      "Epoch 8, 80% \t train_loss: 0.000543 took: 1.59s\n",
      "Epoch 8, 90% \t train_loss: 0.000559 took: 1.62s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 162.30s\n",
      "Epoch 9, 10% \t train_loss: 0.000413 took: 3.42s\n",
      "Epoch 9, 20% \t train_loss: 0.000458 took: 1.70s\n",
      "Epoch 9, 30% \t train_loss: 0.000699 took: 1.59s\n",
      "Epoch 9, 40% \t train_loss: 0.000528 took: 1.58s\n",
      "Epoch 9, 50% \t train_loss: 0.000371 took: 1.57s\n",
      "Epoch 9, 60% \t train_loss: 0.000498 took: 1.68s\n",
      "Epoch 9, 70% \t train_loss: 0.000257 took: 1.61s\n",
      "Epoch 9, 80% \t train_loss: 0.000425 took: 1.62s\n",
      "Epoch 9, 90% \t train_loss: 0.000473 took: 1.52s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 184.44s\n",
      "Epoch 10, 10% \t train_loss: 0.000319 took: 3.50s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, 20% \t train_loss: 0.000251 took: 1.76s\n",
      "Epoch 10, 30% \t train_loss: 0.000455 took: 1.72s\n",
      "Epoch 10, 40% \t train_loss: 0.000302 took: 1.84s\n",
      "Epoch 10, 50% \t train_loss: 0.000259 took: 1.66s\n",
      "Epoch 10, 60% \t train_loss: 0.000419 took: 1.60s\n",
      "Epoch 10, 70% \t train_loss: 0.000279 took: 1.64s\n",
      "Epoch 10, 80% \t train_loss: 0.000228 took: 1.64s\n",
      "Epoch 10, 90% \t train_loss: 0.000224 took: 1.52s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 207.10s\n",
      "[[  5.43900000e+03   3.00000000e+00]\n",
      " [  4.00000000e+02   1.60000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 1.858491 took: 4.31s\n",
      "Epoch 1, 20% \t train_loss: 0.541699 took: 1.41s\n",
      "Epoch 1, 30% \t train_loss: 0.312238 took: 1.45s\n",
      "Epoch 1, 40% \t train_loss: 0.233038 took: 1.51s\n",
      "Epoch 1, 50% \t train_loss: 0.178298 took: 1.61s\n",
      "Epoch 1, 60% \t train_loss: 0.150395 took: 1.55s\n",
      "Epoch 1, 70% \t train_loss: 0.119245 took: 1.39s\n",
      "Epoch 1, 80% \t train_loss: 0.096775 took: 1.72s\n",
      "Epoch 1, 90% \t train_loss: 0.071875 took: 1.53s\n",
      "Validation loss = 0.07\n",
      "Training finished, took 22.27s\n",
      "Epoch 2, 10% \t train_loss: 0.055351 took: 2.95s\n",
      "Epoch 2, 20% \t train_loss: 0.046730 took: 1.60s\n",
      "Epoch 2, 30% \t train_loss: 0.039902 took: 1.65s\n",
      "Epoch 2, 40% \t train_loss: 0.037854 took: 1.55s\n",
      "Epoch 2, 50% \t train_loss: 0.029246 took: 1.63s\n",
      "Epoch 2, 60% \t train_loss: 0.026696 took: 1.65s\n",
      "Epoch 2, 70% \t train_loss: 0.023345 took: 1.73s\n",
      "Epoch 2, 80% \t train_loss: 0.020449 took: 1.83s\n",
      "Epoch 2, 90% \t train_loss: 0.017622 took: 1.74s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 44.71s\n",
      "Epoch 3, 10% \t train_loss: 0.014149 took: 3.01s\n",
      "Epoch 3, 20% \t train_loss: 0.010919 took: 1.66s\n",
      "Epoch 3, 30% \t train_loss: 0.011630 took: 1.73s\n",
      "Epoch 3, 40% \t train_loss: 0.012412 took: 1.74s\n",
      "Epoch 3, 50% \t train_loss: 0.008963 took: 1.66s\n",
      "Epoch 3, 60% \t train_loss: 0.011071 took: 1.73s\n",
      "Epoch 3, 70% \t train_loss: 0.010533 took: 1.62s\n",
      "Epoch 3, 80% \t train_loss: 0.009517 took: 1.51s\n",
      "Epoch 3, 90% \t train_loss: 0.007032 took: 1.68s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 66.88s\n",
      "Epoch 4, 10% \t train_loss: 0.005777 took: 3.36s\n",
      "Epoch 4, 20% \t train_loss: 0.005401 took: 1.76s\n",
      "Epoch 4, 30% \t train_loss: 0.004157 took: 1.68s\n",
      "Epoch 4, 40% \t train_loss: 0.008659 took: 1.61s\n",
      "Epoch 4, 50% \t train_loss: 0.004991 took: 1.76s\n",
      "Epoch 4, 60% \t train_loss: 0.005072 took: 1.56s\n",
      "Epoch 4, 70% \t train_loss: 0.003979 took: 1.57s\n",
      "Epoch 4, 80% \t train_loss: 0.002736 took: 1.63s\n",
      "Epoch 4, 90% \t train_loss: 0.004232 took: 1.65s\n",
      "Validation loss = 0.04\n",
      "Training finished, took 89.22s\n",
      "Epoch 5, 10% \t train_loss: 0.002577 took: 2.75s\n",
      "Epoch 5, 20% \t train_loss: 0.002623 took: 1.71s\n",
      "Epoch 5, 30% \t train_loss: 0.002503 took: 1.66s\n",
      "Epoch 5, 40% \t train_loss: 0.003697 took: 1.55s\n",
      "Epoch 5, 50% \t train_loss: 0.005541 took: 1.67s\n",
      "Epoch 5, 60% \t train_loss: 0.002097 took: 1.69s\n",
      "Epoch 5, 70% \t train_loss: 0.002079 took: 1.90s\n",
      "Epoch 5, 80% \t train_loss: 0.001303 took: 1.71s\n",
      "Epoch 5, 90% \t train_loss: 0.001578 took: 1.59s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 111.27s\n",
      "Epoch 6, 10% \t train_loss: 0.001765 took: 3.17s\n",
      "Epoch 6, 20% \t train_loss: 0.001266 took: 1.70s\n",
      "Epoch 6, 30% \t train_loss: 0.004422 took: 1.63s\n",
      "Epoch 6, 40% \t train_loss: 0.001776 took: 1.51s\n",
      "Epoch 6, 50% \t train_loss: 0.001200 took: 1.45s\n",
      "Epoch 6, 60% \t train_loss: 0.001430 took: 1.53s\n",
      "Epoch 6, 70% \t train_loss: 0.000911 took: 1.55s\n",
      "Epoch 6, 80% \t train_loss: 0.000853 took: 1.50s\n",
      "Epoch 6, 90% \t train_loss: 0.000980 took: 1.38s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 133.14s\n",
      "Epoch 7, 10% \t train_loss: 0.000923 took: 2.86s\n",
      "Epoch 7, 20% \t train_loss: 0.000700 took: 1.62s\n",
      "Epoch 7, 30% \t train_loss: 0.001555 took: 1.66s\n",
      "Epoch 7, 40% \t train_loss: 0.001177 took: 1.52s\n",
      "Epoch 7, 50% \t train_loss: 0.002113 took: 1.56s\n",
      "Epoch 7, 60% \t train_loss: 0.000710 took: 1.44s\n",
      "Epoch 7, 70% \t train_loss: 0.000648 took: 1.55s\n",
      "Epoch 7, 80% \t train_loss: 0.000604 took: 1.55s\n",
      "Epoch 7, 90% \t train_loss: 0.000513 took: 1.51s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 154.65s\n",
      "Epoch 8, 10% \t train_loss: 0.000364 took: 2.98s\n",
      "Epoch 8, 20% \t train_loss: 0.000673 took: 1.50s\n",
      "Epoch 8, 30% \t train_loss: 0.000576 took: 1.65s\n",
      "Epoch 8, 40% \t train_loss: 0.000359 took: 1.54s\n",
      "Epoch 8, 50% \t train_loss: 0.000466 took: 1.65s\n",
      "Epoch 8, 60% \t train_loss: 0.000412 took: 1.59s\n",
      "Epoch 8, 70% \t train_loss: 0.001587 took: 1.69s\n",
      "Epoch 8, 80% \t train_loss: 0.000385 took: 1.52s\n",
      "Epoch 8, 90% \t train_loss: 0.000396 took: 1.60s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 175.92s\n",
      "Epoch 9, 10% \t train_loss: 0.000320 took: 3.04s\n",
      "Epoch 9, 20% \t train_loss: 0.000393 took: 1.55s\n",
      "Epoch 9, 30% \t train_loss: 0.000260 took: 1.60s\n",
      "Epoch 9, 40% \t train_loss: 0.000257 took: 1.53s\n",
      "Epoch 9, 50% \t train_loss: 0.000218 took: 1.55s\n",
      "Epoch 9, 60% \t train_loss: 0.000795 took: 1.57s\n",
      "Epoch 9, 70% \t train_loss: 0.000333 took: 1.55s\n",
      "Epoch 9, 80% \t train_loss: 0.000282 took: 1.49s\n",
      "Epoch 9, 90% \t train_loss: 0.000218 took: 1.56s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 197.55s\n",
      "Epoch 10, 10% \t train_loss: 0.000230 took: 2.94s\n",
      "Epoch 10, 20% \t train_loss: 0.000230 took: 1.68s\n",
      "Epoch 10, 30% \t train_loss: 0.000202 took: 1.68s\n",
      "Epoch 10, 40% \t train_loss: 0.000138 took: 1.43s\n",
      "Epoch 10, 50% \t train_loss: 0.000398 took: 1.64s\n",
      "Epoch 10, 60% \t train_loss: 0.000167 took: 1.69s\n",
      "Epoch 10, 70% \t train_loss: 0.000167 took: 1.66s\n",
      "Epoch 10, 80% \t train_loss: 0.000113 took: 1.58s\n",
      "Epoch 10, 90% \t train_loss: 0.000121 took: 1.61s\n",
      "Validation loss = 0.06\n",
      "Training finished, took 219.63s\n",
      "[[  5.98300000e+03   3.00000000e+00]\n",
      " [  5.00000000e+02   1.70000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 2.691319 took: 4.46s\n",
      "Epoch 1, 20% \t train_loss: 0.382048 took: 1.64s\n",
      "Epoch 1, 30% \t train_loss: 0.234694 took: 1.55s\n",
      "Epoch 1, 40% \t train_loss: 0.177399 took: 1.54s\n",
      "Epoch 1, 50% \t train_loss: 0.153101 took: 1.55s\n",
      "Epoch 1, 60% \t train_loss: 0.100940 took: 1.39s\n",
      "Epoch 1, 70% \t train_loss: 0.099923 took: 1.54s\n",
      "Epoch 1, 80% \t train_loss: 0.082508 took: 1.50s\n",
      "Epoch 1, 90% \t train_loss: 0.071843 took: 1.53s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 22.27s\n",
      "Epoch 2, 10% \t train_loss: 0.049470 took: 3.01s\n",
      "Epoch 2, 20% \t train_loss: 0.055026 took: 1.62s\n",
      "Epoch 2, 30% \t train_loss: 0.037950 took: 1.52s\n",
      "Epoch 2, 40% \t train_loss: 0.032830 took: 1.52s\n",
      "Epoch 2, 50% \t train_loss: 0.026345 took: 1.39s\n",
      "Epoch 2, 60% \t train_loss: 0.028139 took: 1.55s\n",
      "Epoch 2, 70% \t train_loss: 0.025828 took: 1.52s\n",
      "Epoch 2, 80% \t train_loss: 0.026893 took: 1.57s\n",
      "Epoch 2, 90% \t train_loss: 0.019117 took: 1.56s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 43.82s\n",
      "Epoch 3, 10% \t train_loss: 0.013572 took: 2.91s\n",
      "Epoch 3, 20% \t train_loss: 0.014883 took: 1.56s\n",
      "Epoch 3, 30% \t train_loss: 0.009965 took: 1.60s\n",
      "Epoch 3, 40% \t train_loss: 0.012779 took: 1.61s\n",
      "Epoch 3, 50% \t train_loss: 0.010031 took: 1.60s\n",
      "Epoch 3, 60% \t train_loss: 0.012122 took: 1.47s\n",
      "Epoch 3, 70% \t train_loss: 0.008470 took: 1.58s\n",
      "Epoch 3, 80% \t train_loss: 0.006493 took: 1.57s\n",
      "Epoch 3, 90% \t train_loss: 0.008409 took: 1.57s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 65.17s\n",
      "Epoch 4, 10% \t train_loss: 0.004539 took: 2.85s\n",
      "Epoch 4, 20% \t train_loss: 0.004867 took: 1.71s\n",
      "Epoch 4, 30% \t train_loss: 0.004681 took: 1.59s\n",
      "Epoch 4, 40% \t train_loss: 0.006409 took: 1.59s\n",
      "Epoch 4, 50% \t train_loss: 0.004140 took: 1.54s\n",
      "Epoch 4, 60% \t train_loss: 0.003591 took: 1.54s\n",
      "Epoch 4, 70% \t train_loss: 0.003601 took: 1.42s\n",
      "Epoch 4, 80% \t train_loss: 0.003975 took: 1.54s\n",
      "Epoch 4, 90% \t train_loss: 0.002389 took: 1.52s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 86.50s\n",
      "Epoch 5, 10% \t train_loss: 0.002921 took: 2.67s\n",
      "Epoch 5, 20% \t train_loss: 0.002216 took: 1.72s\n",
      "Epoch 5, 30% \t train_loss: 0.002520 took: 1.65s\n",
      "Epoch 5, 40% \t train_loss: 0.003200 took: 1.62s\n",
      "Epoch 5, 50% \t train_loss: 0.003332 took: 1.60s\n",
      "Epoch 5, 60% \t train_loss: 0.001340 took: 1.58s\n",
      "Epoch 5, 70% \t train_loss: 0.001219 took: 1.44s\n",
      "Epoch 5, 80% \t train_loss: 0.001298 took: 1.59s\n",
      "Epoch 5, 90% \t train_loss: 0.001257 took: 1.59s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 108.02s\n",
      "Epoch 6, 10% \t train_loss: 0.001086 took: 3.20s\n",
      "Epoch 6, 20% \t train_loss: 0.001690 took: 1.68s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, 30% \t train_loss: 0.001159 took: 1.61s\n",
      "Epoch 6, 40% \t train_loss: 0.001322 took: 1.45s\n",
      "Epoch 6, 50% \t train_loss: 0.000645 took: 1.55s\n",
      "Epoch 6, 60% \t train_loss: 0.000846 took: 1.62s\n",
      "Epoch 6, 70% \t train_loss: 0.001368 took: 1.58s\n",
      "Epoch 6, 80% \t train_loss: 0.000833 took: 1.60s\n",
      "Epoch 6, 90% \t train_loss: 0.001766 took: 1.41s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 129.34s\n",
      "Epoch 7, 10% \t train_loss: 0.000782 took: 3.05s\n",
      "Epoch 7, 20% \t train_loss: 0.000578 took: 1.71s\n",
      "Epoch 7, 30% \t train_loss: 0.000583 took: 1.67s\n",
      "Epoch 7, 40% \t train_loss: 0.000884 took: 1.59s\n",
      "Epoch 7, 50% \t train_loss: 0.000871 took: 1.51s\n",
      "Epoch 7, 60% \t train_loss: 0.000695 took: 1.57s\n",
      "Epoch 7, 70% \t train_loss: 0.000583 took: 1.62s\n",
      "Epoch 7, 80% \t train_loss: 0.000399 took: 1.59s\n",
      "Epoch 7, 90% \t train_loss: 0.000493 took: 1.56s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 150.84s\n",
      "Epoch 8, 10% \t train_loss: 0.000326 took: 3.06s\n",
      "Epoch 8, 20% \t train_loss: 0.000334 took: 1.68s\n",
      "Epoch 8, 30% \t train_loss: 0.000417 took: 1.73s\n",
      "Epoch 8, 40% \t train_loss: 0.000276 took: 1.57s\n",
      "Epoch 8, 50% \t train_loss: 0.000420 took: 1.39s\n",
      "Epoch 8, 60% \t train_loss: 0.000395 took: 1.59s\n",
      "Epoch 8, 70% \t train_loss: 0.000395 took: 1.59s\n",
      "Epoch 8, 80% \t train_loss: 0.000367 took: 1.64s\n",
      "Epoch 8, 90% \t train_loss: 0.000286 took: 1.63s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 173.00s\n",
      "Epoch 9, 10% \t train_loss: 0.000232 took: 3.22s\n",
      "Epoch 9, 20% \t train_loss: 0.000236 took: 1.63s\n",
      "Epoch 9, 30% \t train_loss: 0.000292 took: 1.50s\n",
      "Epoch 9, 40% \t train_loss: 0.000196 took: 1.63s\n",
      "Epoch 9, 50% \t train_loss: 0.000169 took: 1.65s\n",
      "Epoch 9, 60% \t train_loss: 0.000168 took: 1.55s\n",
      "Epoch 9, 70% \t train_loss: 0.000187 took: 1.58s\n",
      "Epoch 9, 80% \t train_loss: 0.000213 took: 1.46s\n",
      "Epoch 9, 90% \t train_loss: 0.000106 took: 1.55s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 194.65s\n",
      "Epoch 10, 10% \t train_loss: 0.000102 took: 2.79s\n",
      "Epoch 10, 20% \t train_loss: 0.000132 took: 1.70s\n",
      "Epoch 10, 30% \t train_loss: 0.000127 took: 1.52s\n",
      "Epoch 10, 40% \t train_loss: 0.000189 took: 1.52s\n",
      "Epoch 10, 50% \t train_loss: 0.000100 took: 1.63s\n",
      "Epoch 10, 60% \t train_loss: 0.000092 took: 1.58s\n",
      "Epoch 10, 70% \t train_loss: 0.000134 took: 1.60s\n",
      "Epoch 10, 80% \t train_loss: 0.000062 took: 1.54s\n",
      "Epoch 10, 90% \t train_loss: 0.000091 took: 1.46s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 216.03s\n",
      "[[  6.52800000e+03   3.00000000e+00]\n",
      " [  5.00000000e+02   1.90000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 1.832626 took: 4.27s\n",
      "Epoch 1, 20% \t train_loss: 0.470455 took: 1.68s\n",
      "Epoch 1, 30% \t train_loss: 0.330037 took: 1.63s\n",
      "Epoch 1, 40% \t train_loss: 0.243910 took: 1.50s\n",
      "Epoch 1, 50% \t train_loss: 0.179306 took: 1.69s\n",
      "Epoch 1, 60% \t train_loss: 0.158057 took: 1.68s\n",
      "Epoch 1, 70% \t train_loss: 0.117669 took: 1.61s\n",
      "Epoch 1, 80% \t train_loss: 0.104302 took: 1.68s\n",
      "Epoch 1, 90% \t train_loss: 0.089653 took: 1.72s\n",
      "Validation loss = 0.06\n",
      "Training finished, took 23.43s\n",
      "Epoch 2, 10% \t train_loss: 0.068493 took: 2.92s\n",
      "Epoch 2, 20% \t train_loss: 0.046404 took: 1.76s\n",
      "Epoch 2, 30% \t train_loss: 0.052134 took: 1.62s\n",
      "Epoch 2, 40% \t train_loss: 0.042618 took: 1.62s\n",
      "Epoch 2, 50% \t train_loss: 0.042100 took: 1.64s\n",
      "Epoch 2, 60% \t train_loss: 0.032026 took: 1.55s\n",
      "Epoch 2, 70% \t train_loss: 0.031862 took: 1.60s\n",
      "Epoch 2, 80% \t train_loss: 0.024747 took: 1.65s\n",
      "Epoch 2, 90% \t train_loss: 0.021771 took: 1.64s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 45.28s\n",
      "Epoch 3, 10% \t train_loss: 0.018007 took: 3.24s\n",
      "Epoch 3, 20% \t train_loss: 0.017282 took: 1.64s\n",
      "Epoch 3, 30% \t train_loss: 0.015013 took: 1.51s\n",
      "Epoch 3, 40% \t train_loss: 0.011337 took: 1.51s\n",
      "Epoch 3, 50% \t train_loss: 0.009754 took: 1.56s\n",
      "Epoch 3, 60% \t train_loss: 0.011475 took: 1.64s\n",
      "Epoch 3, 70% \t train_loss: 0.009373 took: 1.69s\n",
      "Epoch 3, 80% \t train_loss: 0.009578 took: 1.56s\n",
      "Epoch 3, 90% \t train_loss: 0.012867 took: 1.70s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 67.55s\n",
      "Epoch 4, 10% \t train_loss: 0.007781 took: 3.00s\n",
      "Epoch 4, 20% \t train_loss: 0.008449 took: 1.81s\n",
      "Epoch 4, 30% \t train_loss: 0.003480 took: 1.65s\n",
      "Epoch 4, 40% \t train_loss: 0.005039 took: 1.64s\n",
      "Epoch 4, 50% \t train_loss: 0.004187 took: 1.66s\n",
      "Epoch 4, 60% \t train_loss: 0.006364 took: 1.66s\n",
      "Epoch 4, 70% \t train_loss: 0.005041 took: 1.57s\n",
      "Epoch 4, 80% \t train_loss: 0.007852 took: 1.52s\n",
      "Epoch 4, 90% \t train_loss: 0.004664 took: 1.65s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 89.72s\n",
      "Epoch 5, 10% \t train_loss: 0.004428 took: 3.51s\n",
      "Epoch 5, 20% \t train_loss: 0.002271 took: 1.76s\n",
      "Epoch 5, 30% \t train_loss: 0.002954 took: 1.71s\n",
      "Epoch 5, 40% \t train_loss: 0.002996 took: 1.61s\n",
      "Epoch 5, 50% \t train_loss: 0.006718 took: 1.67s\n",
      "Epoch 5, 60% \t train_loss: 0.001735 took: 1.73s\n",
      "Epoch 5, 70% \t train_loss: 0.001768 took: 1.52s\n",
      "Epoch 5, 80% \t train_loss: 0.003219 took: 1.67s\n",
      "Epoch 5, 90% \t train_loss: 0.002718 took: 1.72s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 112.78s\n",
      "Epoch 6, 10% \t train_loss: 0.002536 took: 3.29s\n",
      "Epoch 6, 20% \t train_loss: 0.001426 took: 1.74s\n",
      "Epoch 6, 30% \t train_loss: 0.001932 took: 1.60s\n",
      "Epoch 6, 40% \t train_loss: 0.004577 took: 1.53s\n",
      "Epoch 6, 50% \t train_loss: 0.001158 took: 1.52s\n",
      "Epoch 6, 60% \t train_loss: 0.002279 took: 1.63s\n",
      "Epoch 6, 70% \t train_loss: 0.001584 took: 1.74s\n",
      "Epoch 6, 80% \t train_loss: 0.001050 took: 1.81s\n",
      "Epoch 6, 90% \t train_loss: 0.000747 took: 1.64s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 134.94s\n",
      "Epoch 7, 10% \t train_loss: 0.000968 took: 3.10s\n",
      "Epoch 7, 20% \t train_loss: 0.001325 took: 1.61s\n",
      "Epoch 7, 30% \t train_loss: 0.000754 took: 1.64s\n",
      "Epoch 7, 40% \t train_loss: 0.000688 took: 1.58s\n",
      "Epoch 7, 50% \t train_loss: 0.000753 took: 1.68s\n",
      "Epoch 7, 60% \t train_loss: 0.001288 took: 1.74s\n",
      "Epoch 7, 70% \t train_loss: 0.000607 took: 1.77s\n",
      "Epoch 7, 80% \t train_loss: 0.001026 took: 1.61s\n",
      "Epoch 7, 90% \t train_loss: 0.000616 took: 1.47s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 157.05s\n",
      "Epoch 8, 10% \t train_loss: 0.000795 took: 2.97s\n",
      "Epoch 8, 20% \t train_loss: 0.000462 took: 1.67s\n",
      "Epoch 8, 30% \t train_loss: 0.000432 took: 1.69s\n",
      "Epoch 8, 40% \t train_loss: 0.000561 took: 1.67s\n",
      "Epoch 8, 50% \t train_loss: 0.000509 took: 1.42s\n",
      "Epoch 8, 60% \t train_loss: 0.000537 took: 1.60s\n",
      "Epoch 8, 70% \t train_loss: 0.002074 took: 1.55s\n",
      "Epoch 8, 80% \t train_loss: 0.000559 took: 1.54s\n",
      "Epoch 8, 90% \t train_loss: 0.000383 took: 1.58s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 178.29s\n",
      "Epoch 9, 10% \t train_loss: 0.000436 took: 2.81s\n",
      "Epoch 9, 20% \t train_loss: 0.000286 took: 1.68s\n",
      "Epoch 9, 30% \t train_loss: 0.000620 took: 1.68s\n",
      "Epoch 9, 40% \t train_loss: 0.000223 took: 1.44s\n",
      "Epoch 9, 50% \t train_loss: 0.000272 took: 1.55s\n",
      "Epoch 9, 60% \t train_loss: 0.000311 took: 1.66s\n",
      "Epoch 9, 70% \t train_loss: 0.000290 took: 1.60s\n",
      "Epoch 9, 80% \t train_loss: 0.001272 took: 1.67s\n",
      "Epoch 9, 90% \t train_loss: 0.000374 took: 1.65s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 199.77s\n",
      "Epoch 10, 10% \t train_loss: 0.000221 took: 3.18s\n",
      "Epoch 10, 20% \t train_loss: 0.000198 took: 1.68s\n",
      "Epoch 10, 30% \t train_loss: 0.000273 took: 1.70s\n",
      "Epoch 10, 40% \t train_loss: 0.000147 took: 1.71s\n",
      "Epoch 10, 50% \t train_loss: 0.000260 took: 1.59s\n",
      "Epoch 10, 60% \t train_loss: 0.000192 took: 1.63s\n",
      "Epoch 10, 70% \t train_loss: 0.000166 took: 1.55s\n",
      "Epoch 10, 80% \t train_loss: 0.000212 took: 1.55s\n",
      "Epoch 10, 90% \t train_loss: 0.000199 took: 1.64s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 221.59s\n",
      "[[  7.07200000e+03   3.00000000e+00]\n",
      " [  6.00000000e+02   2.00000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 1.412514 took: 4.66s\n",
      "Epoch 1, 20% \t train_loss: 0.425498 took: 1.47s\n",
      "Epoch 1, 30% \t train_loss: 0.231061 took: 1.57s\n",
      "Epoch 1, 40% \t train_loss: 0.159132 took: 1.51s\n",
      "Epoch 1, 50% \t train_loss: 0.122989 took: 1.54s\n",
      "Epoch 1, 60% \t train_loss: 0.094414 took: 1.66s\n",
      "Epoch 1, 70% \t train_loss: 0.074203 took: 1.38s\n",
      "Epoch 1, 80% \t train_loss: 0.062794 took: 1.50s\n",
      "Epoch 1, 90% \t train_loss: 0.043906 took: 1.64s\n",
      "Validation loss = 0.06\n",
      "Training finished, took 22.70s\n",
      "Epoch 2, 10% \t train_loss: 0.043326 took: 2.77s\n",
      "Epoch 2, 20% \t train_loss: 0.030331 took: 1.64s\n",
      "Epoch 2, 30% \t train_loss: 0.032102 took: 1.80s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, 40% \t train_loss: 0.021673 took: 1.59s\n",
      "Epoch 2, 50% \t train_loss: 0.020440 took: 1.65s\n",
      "Epoch 2, 60% \t train_loss: 0.016345 took: 1.63s\n",
      "Epoch 2, 70% \t train_loss: 0.016281 took: 1.50s\n",
      "Epoch 2, 80% \t train_loss: 0.013909 took: 1.51s\n",
      "Epoch 2, 90% \t train_loss: 0.010159 took: 1.61s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 44.12s\n",
      "Epoch 3, 10% \t train_loss: 0.008763 took: 2.98s\n",
      "Epoch 3, 20% \t train_loss: 0.009603 took: 1.46s\n",
      "Epoch 3, 30% \t train_loss: 0.007635 took: 1.57s\n",
      "Epoch 3, 40% \t train_loss: 0.007399 took: 1.54s\n",
      "Epoch 3, 50% \t train_loss: 0.006277 took: 1.54s\n",
      "Epoch 3, 60% \t train_loss: 0.010471 took: 1.54s\n",
      "Epoch 3, 70% \t train_loss: 0.005874 took: 1.50s\n",
      "Epoch 3, 80% \t train_loss: 0.007506 took: 1.65s\n",
      "Epoch 3, 90% \t train_loss: 0.004483 took: 1.53s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 65.02s\n",
      "Epoch 4, 10% \t train_loss: 0.003568 took: 3.09s\n",
      "Epoch 4, 20% \t train_loss: 0.006287 took: 1.52s\n",
      "Epoch 4, 30% \t train_loss: 0.003017 took: 1.65s\n",
      "Epoch 4, 40% \t train_loss: 0.002749 took: 1.58s\n",
      "Epoch 4, 50% \t train_loss: 0.003497 took: 1.55s\n",
      "Epoch 4, 60% \t train_loss: 0.001781 took: 1.63s\n",
      "Epoch 4, 70% \t train_loss: 0.004138 took: 1.60s\n",
      "Epoch 4, 80% \t train_loss: 0.005283 took: 1.60s\n",
      "Epoch 4, 90% \t train_loss: 0.001924 took: 1.56s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 86.67s\n",
      "Epoch 5, 10% \t train_loss: 0.001636 took: 2.81s\n",
      "Epoch 5, 20% \t train_loss: 0.002613 took: 1.66s\n",
      "Epoch 5, 30% \t train_loss: 0.001612 took: 1.44s\n",
      "Epoch 5, 40% \t train_loss: 0.001393 took: 1.54s\n",
      "Epoch 5, 50% \t train_loss: 0.001587 took: 1.61s\n",
      "Epoch 5, 60% \t train_loss: 0.001199 took: 1.67s\n",
      "Epoch 5, 70% \t train_loss: 0.003762 took: 1.60s\n",
      "Epoch 5, 80% \t train_loss: 0.002771 took: 1.46s\n",
      "Epoch 5, 90% \t train_loss: 0.000890 took: 1.75s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 108.00s\n",
      "Epoch 6, 10% \t train_loss: 0.000778 took: 3.37s\n",
      "Epoch 6, 20% \t train_loss: 0.000786 took: 1.65s\n",
      "Epoch 6, 30% \t train_loss: 0.000840 took: 1.62s\n",
      "Epoch 6, 40% \t train_loss: 0.001157 took: 1.68s\n",
      "Epoch 6, 50% \t train_loss: 0.000830 took: 1.45s\n",
      "Epoch 6, 60% \t train_loss: 0.003203 took: 1.59s\n",
      "Epoch 6, 70% \t train_loss: 0.000822 took: 1.61s\n",
      "Epoch 6, 80% \t train_loss: 0.000606 took: 1.58s\n",
      "Epoch 6, 90% \t train_loss: 0.000619 took: 1.67s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 130.15s\n",
      "Epoch 7, 10% \t train_loss: 0.000629 took: 3.17s\n",
      "Epoch 7, 20% \t train_loss: 0.000424 took: 1.72s\n",
      "Epoch 7, 30% \t train_loss: 0.000705 took: 1.67s\n",
      "Epoch 7, 40% \t train_loss: 0.000337 took: 1.55s\n",
      "Epoch 7, 50% \t train_loss: 0.001603 took: 1.55s\n",
      "Epoch 7, 60% \t train_loss: 0.000449 took: 1.70s\n",
      "Epoch 7, 70% \t train_loss: 0.000314 took: 1.64s\n",
      "Epoch 7, 80% \t train_loss: 0.000901 took: 1.45s\n",
      "Epoch 7, 90% \t train_loss: 0.000309 took: 1.52s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 152.00s\n",
      "Epoch 8, 10% \t train_loss: 0.000259 took: 3.00s\n",
      "Epoch 8, 20% \t train_loss: 0.000283 took: 1.63s\n",
      "Epoch 8, 30% \t train_loss: 0.000261 took: 1.72s\n",
      "Epoch 8, 40% \t train_loss: 0.000245 took: 1.48s\n",
      "Epoch 8, 50% \t train_loss: 0.000264 took: 1.74s\n",
      "Epoch 8, 60% \t train_loss: 0.000232 took: 1.71s\n",
      "Epoch 8, 70% \t train_loss: 0.000759 took: 1.74s\n",
      "Epoch 8, 80% \t train_loss: 0.000268 took: 1.67s\n",
      "Epoch 8, 90% \t train_loss: 0.000196 took: 1.60s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 174.43s\n",
      "Epoch 9, 10% \t train_loss: 0.000211 took: 3.23s\n",
      "Epoch 9, 20% \t train_loss: 0.000156 took: 1.53s\n",
      "Epoch 9, 30% \t train_loss: 0.000191 took: 1.59s\n",
      "Epoch 9, 40% \t train_loss: 0.000180 took: 1.59s\n",
      "Epoch 9, 50% \t train_loss: 0.000101 took: 1.59s\n",
      "Epoch 9, 60% \t train_loss: 0.000380 took: 1.81s\n",
      "Epoch 9, 70% \t train_loss: 0.000133 took: 1.60s\n",
      "Epoch 9, 80% \t train_loss: 0.000116 took: 1.42s\n",
      "Epoch 9, 90% \t train_loss: 0.000189 took: 1.55s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 196.11s\n",
      "Epoch 10, 10% \t train_loss: 0.000144 took: 3.50s\n",
      "Epoch 10, 20% \t train_loss: 0.000147 took: 1.67s\n",
      "Epoch 10, 30% \t train_loss: 0.000089 took: 1.74s\n",
      "Epoch 10, 40% \t train_loss: 0.000098 took: 1.57s\n",
      "Epoch 10, 50% \t train_loss: 0.000069 took: 1.85s\n",
      "Epoch 10, 60% \t train_loss: 0.000177 took: 1.51s\n",
      "Epoch 10, 70% \t train_loss: 0.000095 took: 1.62s\n",
      "Epoch 10, 80% \t train_loss: 0.000097 took: 1.65s\n",
      "Epoch 10, 90% \t train_loss: 0.000095 took: 1.61s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 218.65s\n",
      "[[  7.61600000e+03   3.00000000e+00]\n",
      " [  8.00000000e+02   2.00000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 1.545740 took: 4.13s\n",
      "Epoch 1, 20% \t train_loss: 0.508954 took: 1.51s\n",
      "Epoch 1, 30% \t train_loss: 0.366896 took: 1.56s\n",
      "Epoch 1, 40% \t train_loss: 0.283449 took: 1.50s\n",
      "Epoch 1, 50% \t train_loss: 0.219100 took: 1.49s\n",
      "Epoch 1, 60% \t train_loss: 0.162290 took: 1.56s\n",
      "Epoch 1, 70% \t train_loss: 0.134120 took: 1.62s\n",
      "Epoch 1, 80% \t train_loss: 0.114345 took: 1.57s\n",
      "Epoch 1, 90% \t train_loss: 0.102097 took: 1.68s\n",
      "Validation loss = 0.07\n",
      "Training finished, took 22.35s\n",
      "Epoch 2, 10% \t train_loss: 0.075143 took: 3.13s\n",
      "Epoch 2, 20% \t train_loss: 0.062279 took: 1.60s\n",
      "Epoch 2, 30% \t train_loss: 0.057638 took: 1.53s\n",
      "Epoch 2, 40% \t train_loss: 0.043182 took: 1.51s\n",
      "Epoch 2, 50% \t train_loss: 0.041577 took: 1.58s\n",
      "Epoch 2, 60% \t train_loss: 0.035674 took: 1.44s\n",
      "Epoch 2, 70% \t train_loss: 0.035035 took: 1.55s\n",
      "Epoch 2, 80% \t train_loss: 0.030097 took: 1.48s\n",
      "Epoch 2, 90% \t train_loss: 0.027975 took: 1.52s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 43.21s\n",
      "Epoch 3, 10% \t train_loss: 0.018616 took: 3.28s\n",
      "Epoch 3, 20% \t train_loss: 0.021726 took: 1.60s\n",
      "Epoch 3, 30% \t train_loss: 0.019675 took: 1.61s\n",
      "Epoch 3, 40% \t train_loss: 0.016299 took: 1.54s\n",
      "Epoch 3, 50% \t train_loss: 0.010899 took: 1.52s\n",
      "Epoch 3, 60% \t train_loss: 0.016243 took: 1.52s\n",
      "Epoch 3, 70% \t train_loss: 0.011074 took: 1.64s\n",
      "Epoch 3, 80% \t train_loss: 0.013824 took: 1.54s\n",
      "Epoch 3, 90% \t train_loss: 0.014731 took: 1.57s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 65.23s\n",
      "Epoch 4, 10% \t train_loss: 0.006547 took: 3.11s\n",
      "Epoch 4, 20% \t train_loss: 0.007635 took: 1.49s\n",
      "Epoch 4, 30% \t train_loss: 0.014088 took: 1.56s\n",
      "Epoch 4, 40% \t train_loss: 0.007017 took: 1.47s\n",
      "Epoch 4, 50% \t train_loss: 0.008155 took: 1.49s\n",
      "Epoch 4, 60% \t train_loss: 0.005733 took: 1.50s\n",
      "Epoch 4, 70% \t train_loss: 0.008430 took: 1.41s\n",
      "Epoch 4, 80% \t train_loss: 0.004597 took: 1.57s\n",
      "Epoch 4, 90% \t train_loss: 0.006385 took: 1.66s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 86.16s\n",
      "Epoch 5, 10% \t train_loss: 0.005372 took: 3.16s\n",
      "Epoch 5, 20% \t train_loss: 0.004103 took: 1.53s\n",
      "Epoch 5, 30% \t train_loss: 0.005189 took: 1.58s\n",
      "Epoch 5, 40% \t train_loss: 0.002723 took: 1.57s\n",
      "Epoch 5, 50% \t train_loss: 0.003224 took: 1.55s\n",
      "Epoch 5, 60% \t train_loss: 0.006928 took: 1.53s\n",
      "Epoch 5, 70% \t train_loss: 0.003189 took: 1.51s\n",
      "Epoch 5, 80% \t train_loss: 0.003144 took: 1.50s\n",
      "Epoch 5, 90% \t train_loss: 0.004419 took: 1.60s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 107.32s\n",
      "Epoch 6, 10% \t train_loss: 0.002424 took: 3.10s\n",
      "Epoch 6, 20% \t train_loss: 0.002440 took: 1.50s\n",
      "Epoch 6, 30% \t train_loss: 0.002175 took: 1.70s\n",
      "Epoch 6, 40% \t train_loss: 0.003079 took: 1.55s\n",
      "Epoch 6, 50% \t train_loss: 0.004092 took: 1.61s\n",
      "Epoch 6, 60% \t train_loss: 0.003696 took: 1.56s\n",
      "Epoch 6, 70% \t train_loss: 0.001431 took: 1.63s\n",
      "Epoch 6, 80% \t train_loss: 0.001502 took: 1.46s\n",
      "Epoch 6, 90% \t train_loss: 0.002489 took: 1.57s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 128.83s\n",
      "Epoch 7, 10% \t train_loss: 0.001679 took: 2.82s\n",
      "Epoch 7, 20% \t train_loss: 0.001270 took: 1.51s\n",
      "Epoch 7, 30% \t train_loss: 0.001557 took: 1.66s\n",
      "Epoch 7, 40% \t train_loss: 0.001444 took: 1.83s\n",
      "Epoch 7, 50% \t train_loss: 0.000833 took: 1.64s\n",
      "Epoch 7, 60% \t train_loss: 0.002445 took: 1.52s\n",
      "Epoch 7, 70% \t train_loss: 0.002446 took: 1.58s\n",
      "Epoch 7, 80% \t train_loss: 0.001254 took: 1.58s\n",
      "Epoch 7, 90% \t train_loss: 0.001425 took: 1.48s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 150.39s\n",
      "Epoch 8, 10% \t train_loss: 0.000726 took: 2.92s\n",
      "Epoch 8, 20% \t train_loss: 0.000957 took: 1.65s\n",
      "Epoch 8, 30% \t train_loss: 0.000859 took: 1.59s\n",
      "Epoch 8, 40% \t train_loss: 0.000730 took: 1.41s\n",
      "Epoch 8, 50% \t train_loss: 0.000700 took: 1.55s\n",
      "Epoch 8, 60% \t train_loss: 0.001239 took: 1.60s\n",
      "Epoch 8, 70% \t train_loss: 0.000988 took: 1.57s\n",
      "Epoch 8, 80% \t train_loss: 0.001308 took: 1.58s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, 90% \t train_loss: 0.000599 took: 1.40s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 171.31s\n",
      "Epoch 9, 10% \t train_loss: 0.000735 took: 2.77s\n",
      "Epoch 9, 20% \t train_loss: 0.000545 took: 1.56s\n",
      "Epoch 9, 30% \t train_loss: 0.000593 took: 1.51s\n",
      "Epoch 9, 40% \t train_loss: 0.000804 took: 1.61s\n",
      "Epoch 9, 50% \t train_loss: 0.000462 took: 1.49s\n",
      "Epoch 9, 60% \t train_loss: 0.000623 took: 1.57s\n",
      "Epoch 9, 70% \t train_loss: 0.000880 took: 1.57s\n",
      "Epoch 9, 80% \t train_loss: 0.000438 took: 1.51s\n",
      "Epoch 9, 90% \t train_loss: 0.000623 took: 1.54s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 192.26s\n",
      "Epoch 10, 10% \t train_loss: 0.000324 took: 3.09s\n",
      "Epoch 10, 20% \t train_loss: 0.000268 took: 1.65s\n",
      "Epoch 10, 30% \t train_loss: 0.000449 took: 1.56s\n",
      "Epoch 10, 40% \t train_loss: 0.000382 took: 1.47s\n",
      "Epoch 10, 50% \t train_loss: 0.000334 took: 1.62s\n",
      "Epoch 10, 60% \t train_loss: 0.000594 took: 1.54s\n",
      "Epoch 10, 70% \t train_loss: 0.000735 took: 1.60s\n",
      "Epoch 10, 80% \t train_loss: 0.000344 took: 1.59s\n",
      "Epoch 10, 90% \t train_loss: 0.000287 took: 1.44s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 213.45s\n",
      "[[  8.16000000e+03   3.00000000e+00]\n",
      " [  8.00000000e+02   2.20000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 0.867877 took: 4.01s\n",
      "Epoch 1, 20% \t train_loss: 0.302467 took: 1.50s\n",
      "Epoch 1, 30% \t train_loss: 0.189555 took: 1.51s\n",
      "Epoch 1, 40% \t train_loss: 0.156553 took: 1.52s\n",
      "Epoch 1, 50% \t train_loss: 0.111458 took: 1.64s\n",
      "Epoch 1, 60% \t train_loss: 0.086657 took: 1.52s\n",
      "Epoch 1, 70% \t train_loss: 0.082369 took: 1.48s\n",
      "Epoch 1, 80% \t train_loss: 0.063976 took: 1.58s\n",
      "Epoch 1, 90% \t train_loss: 0.052817 took: 1.73s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 22.69s\n",
      "Epoch 2, 10% \t train_loss: 0.036738 took: 3.36s\n",
      "Epoch 2, 20% \t train_loss: 0.031305 took: 1.64s\n",
      "Epoch 2, 30% \t train_loss: 0.024352 took: 1.56s\n",
      "Epoch 2, 40% \t train_loss: 0.019429 took: 1.44s\n",
      "Epoch 2, 50% \t train_loss: 0.027032 took: 1.56s\n",
      "Epoch 2, 60% \t train_loss: 0.017681 took: 1.59s\n",
      "Epoch 2, 70% \t train_loss: 0.017104 took: 1.61s\n",
      "Epoch 2, 80% \t train_loss: 0.012910 took: 1.55s\n",
      "Epoch 2, 90% \t train_loss: 0.010192 took: 1.49s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 44.29s\n",
      "Epoch 3, 10% \t train_loss: 0.009556 took: 3.28s\n",
      "Epoch 3, 20% \t train_loss: 0.010583 took: 1.72s\n",
      "Epoch 3, 30% \t train_loss: 0.007559 took: 1.72s\n",
      "Epoch 3, 40% \t train_loss: 0.006460 took: 1.71s\n",
      "Epoch 3, 50% \t train_loss: 0.006777 took: 1.64s\n",
      "Epoch 3, 60% \t train_loss: 0.008166 took: 1.63s\n",
      "Epoch 3, 70% \t train_loss: 0.007644 took: 1.50s\n",
      "Epoch 3, 80% \t train_loss: 0.004613 took: 1.58s\n",
      "Epoch 3, 90% \t train_loss: 0.006107 took: 1.61s\n",
      "Validation loss = 0.04\n",
      "Training finished, took 66.38s\n",
      "Epoch 4, 10% \t train_loss: 0.003871 took: 2.74s\n",
      "Epoch 4, 20% \t train_loss: 0.004100 took: 1.63s\n",
      "Epoch 4, 30% \t train_loss: 0.004784 took: 1.57s\n",
      "Epoch 4, 40% \t train_loss: 0.005147 took: 1.56s\n",
      "Epoch 4, 50% \t train_loss: 0.003169 took: 1.54s\n",
      "Epoch 4, 60% \t train_loss: 0.002342 took: 1.47s\n",
      "Epoch 4, 70% \t train_loss: 0.002015 took: 1.56s\n",
      "Epoch 4, 80% \t train_loss: 0.002133 took: 1.68s\n",
      "Epoch 4, 90% \t train_loss: 0.002675 took: 1.52s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 87.28s\n",
      "Epoch 5, 10% \t train_loss: 0.001429 took: 2.78s\n",
      "Epoch 5, 20% \t train_loss: 0.001770 took: 1.67s\n",
      "Epoch 5, 30% \t train_loss: 0.001166 took: 1.56s\n",
      "Epoch 5, 40% \t train_loss: 0.002248 took: 1.59s\n",
      "Epoch 5, 50% \t train_loss: 0.001708 took: 1.61s\n",
      "Epoch 5, 60% \t train_loss: 0.001539 took: 1.46s\n",
      "Epoch 5, 70% \t train_loss: 0.002279 took: 1.55s\n",
      "Epoch 5, 80% \t train_loss: 0.000907 took: 1.58s\n",
      "Epoch 5, 90% \t train_loss: 0.002878 took: 1.57s\n",
      "Validation loss = 0.04\n",
      "Training finished, took 108.27s\n",
      "Epoch 6, 10% \t train_loss: 0.000920 took: 2.80s\n",
      "Epoch 6, 20% \t train_loss: 0.000854 took: 1.68s\n",
      "Epoch 6, 30% \t train_loss: 0.000869 took: 1.60s\n",
      "Epoch 6, 40% \t train_loss: 0.000892 took: 1.59s\n",
      "Epoch 6, 50% \t train_loss: 0.000542 took: 1.58s\n",
      "Epoch 6, 60% \t train_loss: 0.000581 took: 1.42s\n",
      "Epoch 6, 70% \t train_loss: 0.000833 took: 1.58s\n",
      "Epoch 6, 80% \t train_loss: 0.000888 took: 1.58s\n",
      "Epoch 6, 90% \t train_loss: 0.001885 took: 1.68s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 129.60s\n",
      "Epoch 7, 10% \t train_loss: 0.000538 took: 3.34s\n",
      "Epoch 7, 20% \t train_loss: 0.000546 took: 1.52s\n",
      "Epoch 7, 30% \t train_loss: 0.000565 took: 1.53s\n",
      "Epoch 7, 40% \t train_loss: 0.000372 took: 1.59s\n",
      "Epoch 7, 50% \t train_loss: 0.000383 took: 1.56s\n",
      "Epoch 7, 60% \t train_loss: 0.000894 took: 1.54s\n",
      "Epoch 7, 70% \t train_loss: 0.000492 took: 1.52s\n",
      "Epoch 7, 80% \t train_loss: 0.000244 took: 1.69s\n",
      "Epoch 7, 90% \t train_loss: 0.000539 took: 1.58s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 151.31s\n",
      "Epoch 8, 10% \t train_loss: 0.000436 took: 3.08s\n",
      "Epoch 8, 20% \t train_loss: 0.000300 took: 1.84s\n",
      "Epoch 8, 30% \t train_loss: 0.000336 took: 1.76s\n",
      "Epoch 8, 40% \t train_loss: 0.000263 took: 1.49s\n",
      "Epoch 8, 50% \t train_loss: 0.000287 took: 1.60s\n",
      "Epoch 8, 60% \t train_loss: 0.000310 took: 1.60s\n",
      "Epoch 8, 70% \t train_loss: 0.000239 took: 1.58s\n",
      "Epoch 8, 80% \t train_loss: 0.000231 took: 1.63s\n",
      "Epoch 8, 90% \t train_loss: 0.000190 took: 1.60s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 173.71s\n",
      "Epoch 9, 10% \t train_loss: 0.000148 took: 2.93s\n",
      "Epoch 9, 20% \t train_loss: 0.000158 took: 1.84s\n",
      "Epoch 9, 30% \t train_loss: 0.000152 took: 1.62s\n",
      "Epoch 9, 40% \t train_loss: 0.000160 took: 1.57s\n",
      "Epoch 9, 50% \t train_loss: 0.000135 took: 1.68s\n",
      "Epoch 9, 60% \t train_loss: 0.000214 took: 1.59s\n",
      "Epoch 9, 70% \t train_loss: 0.000119 took: 1.51s\n",
      "Epoch 9, 80% \t train_loss: 0.000119 took: 1.56s\n",
      "Epoch 9, 90% \t train_loss: 0.000173 took: 1.54s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 195.54s\n",
      "Epoch 10, 10% \t train_loss: 0.000099 took: 3.27s\n",
      "Epoch 10, 20% \t train_loss: 0.000100 took: 1.67s\n",
      "Epoch 10, 30% \t train_loss: 0.000092 took: 1.56s\n",
      "Epoch 10, 40% \t train_loss: 0.000078 took: 1.49s\n",
      "Epoch 10, 50% \t train_loss: 0.000090 took: 1.61s\n",
      "Epoch 10, 60% \t train_loss: 0.000054 took: 1.60s\n",
      "Epoch 10, 70% \t train_loss: 0.000058 took: 1.64s\n",
      "Epoch 10, 80% \t train_loss: 0.000047 took: 1.63s\n",
      "Epoch 10, 90% \t train_loss: 0.000038 took: 1.59s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 217.25s\n",
      "[[  8.70500000e+03   3.00000000e+00]\n",
      " [  9.00000000e+02   2.30000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 5.473310 took: 4.70s\n",
      "Epoch 1, 20% \t train_loss: 0.412351 took: 1.57s\n",
      "Epoch 1, 30% \t train_loss: 0.256472 took: 1.57s\n",
      "Epoch 1, 40% \t train_loss: 0.198019 took: 1.41s\n",
      "Epoch 1, 50% \t train_loss: 0.158492 took: 1.50s\n",
      "Epoch 1, 60% \t train_loss: 0.113537 took: 1.54s\n",
      "Epoch 1, 70% \t train_loss: 0.097345 took: 1.57s\n",
      "Epoch 1, 80% \t train_loss: 0.072915 took: 1.58s\n",
      "Epoch 1, 90% \t train_loss: 0.063609 took: 1.40s\n",
      "Validation loss = 0.06\n",
      "Training finished, took 22.94s\n",
      "Epoch 2, 10% \t train_loss: 0.045289 took: 2.93s\n",
      "Epoch 2, 20% \t train_loss: 0.036882 took: 1.69s\n",
      "Epoch 2, 30% \t train_loss: 0.032198 took: 1.59s\n",
      "Epoch 2, 40% \t train_loss: 0.030123 took: 1.49s\n",
      "Epoch 2, 50% \t train_loss: 0.023749 took: 1.42s\n",
      "Epoch 2, 60% \t train_loss: 0.020949 took: 1.51s\n",
      "Epoch 2, 70% \t train_loss: 0.017868 took: 1.52s\n",
      "Epoch 2, 80% \t train_loss: 0.016179 took: 1.47s\n",
      "Epoch 2, 90% \t train_loss: 0.015588 took: 1.37s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 43.54s\n",
      "Epoch 3, 10% \t train_loss: 0.011063 took: 3.30s\n",
      "Epoch 3, 20% \t train_loss: 0.010907 took: 1.65s\n",
      "Epoch 3, 30% \t train_loss: 0.013984 took: 1.58s\n",
      "Epoch 3, 40% \t train_loss: 0.010389 took: 1.44s\n",
      "Epoch 3, 50% \t train_loss: 0.008140 took: 1.52s\n",
      "Epoch 3, 60% \t train_loss: 0.006879 took: 1.56s\n",
      "Epoch 3, 70% \t train_loss: 0.007413 took: 1.58s\n",
      "Epoch 3, 80% \t train_loss: 0.005661 took: 1.54s\n",
      "Epoch 3, 90% \t train_loss: 0.004007 took: 1.38s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 64.79s\n",
      "Epoch 4, 10% \t train_loss: 0.004690 took: 2.87s\n",
      "Epoch 4, 20% \t train_loss: 0.003320 took: 1.63s\n",
      "Epoch 4, 30% \t train_loss: 0.005204 took: 1.66s\n",
      "Epoch 4, 40% \t train_loss: 0.004595 took: 1.44s\n",
      "Epoch 4, 50% \t train_loss: 0.002908 took: 1.56s\n",
      "Epoch 4, 60% \t train_loss: 0.005018 took: 1.58s\n",
      "Epoch 4, 70% \t train_loss: 0.002633 took: 1.55s\n",
      "Epoch 4, 80% \t train_loss: 0.004762 took: 1.60s\n",
      "Epoch 4, 90% \t train_loss: 0.002958 took: 1.47s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.02\n",
      "Training finished, took 86.22s\n",
      "Epoch 5, 10% \t train_loss: 0.002398 took: 3.15s\n",
      "Epoch 5, 20% \t train_loss: 0.002584 took: 1.63s\n",
      "Epoch 5, 30% \t train_loss: 0.001815 took: 1.58s\n",
      "Epoch 5, 40% \t train_loss: 0.001152 took: 1.51s\n",
      "Epoch 5, 50% \t train_loss: 0.002319 took: 1.42s\n",
      "Epoch 5, 60% \t train_loss: 0.002595 took: 1.61s\n",
      "Epoch 5, 70% \t train_loss: 0.001090 took: 1.48s\n",
      "Epoch 5, 80% \t train_loss: 0.003506 took: 1.62s\n",
      "Epoch 5, 90% \t train_loss: 0.001991 took: 1.51s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 107.26s\n",
      "Epoch 6, 10% \t train_loss: 0.001352 took: 3.15s\n",
      "Epoch 6, 20% \t train_loss: 0.001420 took: 1.65s\n",
      "Epoch 6, 30% \t train_loss: 0.001136 took: 1.60s\n",
      "Epoch 6, 40% \t train_loss: 0.000882 took: 1.61s\n",
      "Epoch 6, 50% \t train_loss: 0.001563 took: 1.42s\n",
      "Epoch 6, 60% \t train_loss: 0.002203 took: 1.65s\n",
      "Epoch 6, 70% \t train_loss: 0.000796 took: 1.59s\n",
      "Epoch 6, 80% \t train_loss: 0.000759 took: 1.50s\n",
      "Epoch 6, 90% \t train_loss: 0.000699 took: 1.58s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 128.54s\n",
      "Epoch 7, 10% \t train_loss: 0.000948 took: 2.70s\n",
      "Epoch 7, 20% \t train_loss: 0.000525 took: 1.59s\n",
      "Epoch 7, 30% \t train_loss: 0.000629 took: 1.61s\n",
      "Epoch 7, 40% \t train_loss: 0.000471 took: 1.50s\n",
      "Epoch 7, 50% \t train_loss: 0.000820 took: 1.52s\n",
      "Epoch 7, 60% \t train_loss: 0.000558 took: 1.59s\n",
      "Epoch 7, 70% \t train_loss: 0.000943 took: 1.60s\n",
      "Epoch 7, 80% \t train_loss: 0.000642 took: 1.57s\n",
      "Epoch 7, 90% \t train_loss: 0.000307 took: 1.50s\n",
      "Validation loss = 0.04\n",
      "Training finished, took 150.00s\n",
      "Epoch 8, 10% \t train_loss: 0.000351 took: 2.92s\n",
      "Epoch 8, 20% \t train_loss: 0.000304 took: 1.68s\n",
      "Epoch 8, 30% \t train_loss: 0.000452 took: 1.67s\n",
      "Epoch 8, 40% \t train_loss: 0.000681 took: 1.51s\n",
      "Epoch 8, 50% \t train_loss: 0.000375 took: 1.68s\n",
      "Epoch 8, 60% \t train_loss: 0.000382 took: 1.45s\n",
      "Epoch 8, 70% \t train_loss: 0.000255 took: 1.60s\n",
      "Epoch 8, 80% \t train_loss: 0.000304 took: 1.58s\n",
      "Epoch 8, 90% \t train_loss: 0.000321 took: 1.54s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 171.34s\n",
      "Epoch 9, 10% \t train_loss: 0.000223 took: 3.31s\n",
      "Epoch 9, 20% \t train_loss: 0.000176 took: 1.48s\n",
      "Epoch 9, 30% \t train_loss: 0.000351 took: 1.50s\n",
      "Epoch 9, 40% \t train_loss: 0.000147 took: 1.58s\n",
      "Epoch 9, 50% \t train_loss: 0.000275 took: 1.56s\n",
      "Epoch 9, 60% \t train_loss: 0.000139 took: 1.53s\n",
      "Epoch 9, 70% \t train_loss: 0.000165 took: 1.67s\n",
      "Epoch 9, 80% \t train_loss: 0.000178 took: 1.60s\n",
      "Epoch 9, 90% \t train_loss: 0.000166 took: 1.58s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 192.80s\n",
      "Epoch 10, 10% \t train_loss: 0.000171 took: 2.88s\n",
      "Epoch 10, 20% \t train_loss: 0.000119 took: 1.55s\n",
      "Epoch 10, 30% \t train_loss: 0.000129 took: 1.60s\n",
      "Epoch 10, 40% \t train_loss: 0.000099 took: 1.62s\n",
      "Epoch 10, 50% \t train_loss: 0.000126 took: 1.60s\n",
      "Epoch 10, 60% \t train_loss: 0.000085 took: 1.58s\n",
      "Epoch 10, 70% \t train_loss: 0.000077 took: 1.43s\n",
      "Epoch 10, 80% \t train_loss: 0.000102 took: 1.64s\n",
      "Epoch 10, 90% \t train_loss: 0.000094 took: 1.63s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 214.08s\n",
      "[[  9.24900000e+03   3.00000000e+00]\n",
      " [  1.00000000e+03   2.40000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 2.060427 took: 4.08s\n",
      "Epoch 1, 20% \t train_loss: 0.542687 took: 1.66s\n",
      "Epoch 1, 30% \t train_loss: 0.361200 took: 1.49s\n",
      "Epoch 1, 40% \t train_loss: 0.255761 took: 1.52s\n",
      "Epoch 1, 50% \t train_loss: 0.198429 took: 1.50s\n",
      "Epoch 1, 60% \t train_loss: 0.152281 took: 1.37s\n",
      "Epoch 1, 70% \t train_loss: 0.123701 took: 1.64s\n",
      "Epoch 1, 80% \t train_loss: 0.102945 took: 1.56s\n",
      "Epoch 1, 90% \t train_loss: 0.077790 took: 1.54s\n",
      "Validation loss = 0.06\n",
      "Training finished, took 22.49s\n",
      "Epoch 2, 10% \t train_loss: 0.061258 took: 2.70s\n",
      "Epoch 2, 20% \t train_loss: 0.055408 took: 1.68s\n",
      "Epoch 2, 30% \t train_loss: 0.037175 took: 1.59s\n",
      "Epoch 2, 40% \t train_loss: 0.035020 took: 1.52s\n",
      "Epoch 2, 50% \t train_loss: 0.034682 took: 1.60s\n",
      "Epoch 2, 60% \t train_loss: 0.034682 took: 1.43s\n",
      "Epoch 2, 70% \t train_loss: 0.028405 took: 1.52s\n",
      "Epoch 2, 80% \t train_loss: 0.021775 took: 1.58s\n",
      "Epoch 2, 90% \t train_loss: 0.018199 took: 1.64s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 43.45s\n",
      "Epoch 3, 10% \t train_loss: 0.019340 took: 2.80s\n",
      "Epoch 3, 20% \t train_loss: 0.014978 took: 1.65s\n",
      "Epoch 3, 30% \t train_loss: 0.014469 took: 1.62s\n",
      "Epoch 3, 40% \t train_loss: 0.013046 took: 1.61s\n",
      "Epoch 3, 50% \t train_loss: 0.012577 took: 1.61s\n",
      "Epoch 3, 60% \t train_loss: 0.008894 took: 1.41s\n",
      "Epoch 3, 70% \t train_loss: 0.010001 took: 1.59s\n",
      "Epoch 3, 80% \t train_loss: 0.009603 took: 1.56s\n",
      "Epoch 3, 90% \t train_loss: 0.007713 took: 1.59s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 65.17s\n",
      "Epoch 4, 10% \t train_loss: 0.005902 took: 3.00s\n",
      "Epoch 4, 20% \t train_loss: 0.003989 took: 1.69s\n",
      "Epoch 4, 30% \t train_loss: 0.005774 took: 1.46s\n",
      "Epoch 4, 40% \t train_loss: 0.004062 took: 1.68s\n",
      "Epoch 4, 50% \t train_loss: 0.006230 took: 1.55s\n",
      "Epoch 4, 60% \t train_loss: 0.004143 took: 1.55s\n",
      "Epoch 4, 70% \t train_loss: 0.006317 took: 1.63s\n",
      "Epoch 4, 80% \t train_loss: 0.003507 took: 1.41s\n",
      "Epoch 4, 90% \t train_loss: 0.009158 took: 1.62s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 86.36s\n",
      "Epoch 5, 10% \t train_loss: 0.002940 took: 2.88s\n",
      "Epoch 5, 20% \t train_loss: 0.003644 took: 1.80s\n",
      "Epoch 5, 30% \t train_loss: 0.002337 took: 1.95s\n",
      "Epoch 5, 40% \t train_loss: 0.003128 took: 1.49s\n",
      "Epoch 5, 50% \t train_loss: 0.002078 took: 1.56s\n",
      "Epoch 5, 60% \t train_loss: 0.006959 took: 1.56s\n",
      "Epoch 5, 70% \t train_loss: 0.002325 took: 1.71s\n",
      "Epoch 5, 80% \t train_loss: 0.001748 took: 1.71s\n",
      "Epoch 5, 90% \t train_loss: 0.002219 took: 1.63s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 108.29s\n",
      "Epoch 6, 10% \t train_loss: 0.002101 took: 3.08s\n",
      "Epoch 6, 20% \t train_loss: 0.001636 took: 1.68s\n",
      "Epoch 6, 30% \t train_loss: 0.001555 took: 1.65s\n",
      "Epoch 6, 40% \t train_loss: 0.001167 took: 1.55s\n",
      "Epoch 6, 50% \t train_loss: 0.001642 took: 1.53s\n",
      "Epoch 6, 60% \t train_loss: 0.004488 took: 1.40s\n",
      "Epoch 6, 70% \t train_loss: 0.001207 took: 1.61s\n",
      "Epoch 6, 80% \t train_loss: 0.001710 took: 1.57s\n",
      "Epoch 6, 90% \t train_loss: 0.001076 took: 1.58s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 129.71s\n",
      "Epoch 7, 10% \t train_loss: 0.001329 took: 2.92s\n",
      "Epoch 7, 20% \t train_loss: 0.001523 took: 1.70s\n",
      "Epoch 7, 30% \t train_loss: 0.001123 took: 1.67s\n",
      "Epoch 7, 40% \t train_loss: 0.000777 took: 1.66s\n",
      "Epoch 7, 50% \t train_loss: 0.002672 took: 1.59s\n",
      "Epoch 7, 60% \t train_loss: 0.000732 took: 1.66s\n",
      "Epoch 7, 70% \t train_loss: 0.000589 took: 1.47s\n",
      "Epoch 7, 80% \t train_loss: 0.000645 took: 1.56s\n",
      "Epoch 7, 90% \t train_loss: 0.000581 took: 1.61s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 151.31s\n",
      "Epoch 8, 10% \t train_loss: 0.000550 took: 2.95s\n",
      "Epoch 8, 20% \t train_loss: 0.000508 took: 1.54s\n",
      "Epoch 8, 30% \t train_loss: 0.000408 took: 1.77s\n",
      "Epoch 8, 40% \t train_loss: 0.000557 took: 1.60s\n",
      "Epoch 8, 50% \t train_loss: 0.000457 took: 1.63s\n",
      "Epoch 8, 60% \t train_loss: 0.000435 took: 1.57s\n",
      "Epoch 8, 70% \t train_loss: 0.000733 took: 1.57s\n",
      "Epoch 8, 80% \t train_loss: 0.000361 took: 1.41s\n",
      "Epoch 8, 90% \t train_loss: 0.000393 took: 1.52s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 172.59s\n",
      "Epoch 9, 10% \t train_loss: 0.000326 took: 2.93s\n",
      "Epoch 9, 20% \t train_loss: 0.000247 took: 1.48s\n",
      "Epoch 9, 30% \t train_loss: 0.000320 took: 1.72s\n",
      "Epoch 9, 40% \t train_loss: 0.000310 took: 1.55s\n",
      "Epoch 9, 50% \t train_loss: 0.000223 took: 1.56s\n",
      "Epoch 9, 60% \t train_loss: 0.000994 took: 1.51s\n",
      "Epoch 9, 70% \t train_loss: 0.000306 took: 1.47s\n",
      "Epoch 9, 80% \t train_loss: 0.000324 took: 1.52s\n",
      "Epoch 9, 90% \t train_loss: 0.000247 took: 1.67s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 194.12s\n",
      "Epoch 10, 10% \t train_loss: 0.000226 took: 3.03s\n",
      "Epoch 10, 20% \t train_loss: 0.000148 took: 1.66s\n",
      "Epoch 10, 30% \t train_loss: 0.000186 took: 1.50s\n",
      "Epoch 10, 40% \t train_loss: 0.000210 took: 1.48s\n",
      "Epoch 10, 50% \t train_loss: 0.000208 took: 1.50s\n",
      "Epoch 10, 60% \t train_loss: 0.000148 took: 1.60s\n",
      "Epoch 10, 70% \t train_loss: 0.000198 took: 1.60s\n",
      "Epoch 10, 80% \t train_loss: 0.000114 took: 1.53s\n",
      "Epoch 10, 90% \t train_loss: 0.000424 took: 1.49s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 215.11s\n",
      "[[  9.79300000e+03   3.00000000e+00]\n",
      " [  1.00000000e+03   2.60000000e+03]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 4\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 10% \t train_loss: 0.953505 took: 4.62s\n",
      "Epoch 1, 20% \t train_loss: 0.321400 took: 1.45s\n",
      "Epoch 1, 30% \t train_loss: 0.222959 took: 1.37s\n",
      "Epoch 1, 40% \t train_loss: 0.157762 took: 1.45s\n",
      "Epoch 1, 50% \t train_loss: 0.104716 took: 1.52s\n",
      "Epoch 1, 60% \t train_loss: 0.087606 took: 1.55s\n",
      "Epoch 1, 70% \t train_loss: 0.073008 took: 1.36s\n",
      "Epoch 1, 80% \t train_loss: 0.056609 took: 1.54s\n",
      "Epoch 1, 90% \t train_loss: 0.061276 took: 1.53s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 22.00s\n",
      "Epoch 2, 10% \t train_loss: 0.033457 took: 2.75s\n",
      "Epoch 2, 20% \t train_loss: 0.031489 took: 1.69s\n",
      "Epoch 2, 30% \t train_loss: 0.025737 took: 1.60s\n",
      "Epoch 2, 40% \t train_loss: 0.023351 took: 1.56s\n",
      "Epoch 2, 50% \t train_loss: 0.018994 took: 1.52s\n",
      "Epoch 2, 60% \t train_loss: 0.017009 took: 1.43s\n",
      "Epoch 2, 70% \t train_loss: 0.013928 took: 1.52s\n",
      "Epoch 2, 80% \t train_loss: 0.013693 took: 1.54s\n",
      "Epoch 2, 90% \t train_loss: 0.011890 took: 1.52s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 43.21s\n",
      "Epoch 3, 10% \t train_loss: 0.009527 took: 2.70s\n",
      "Epoch 3, 20% \t train_loss: 0.008407 took: 1.59s\n",
      "Epoch 3, 30% \t train_loss: 0.007575 took: 1.63s\n",
      "Epoch 3, 40% \t train_loss: 0.008085 took: 1.47s\n",
      "Epoch 3, 50% \t train_loss: 0.005060 took: 1.32s\n",
      "Epoch 3, 60% \t train_loss: 0.007988 took: 1.42s\n",
      "Epoch 3, 70% \t train_loss: 0.005189 took: 1.38s\n",
      "Epoch 3, 80% \t train_loss: 0.004043 took: 1.41s\n",
      "Epoch 3, 90% \t train_loss: 0.008670 took: 1.35s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 63.39s\n",
      "Epoch 4, 10% \t train_loss: 0.005490 took: 3.42s\n",
      "Epoch 4, 20% \t train_loss: 0.002910 took: 1.51s\n",
      "Epoch 4, 30% \t train_loss: 0.002682 took: 1.40s\n",
      "Epoch 4, 40% \t train_loss: 0.004567 took: 1.31s\n",
      "Epoch 4, 50% \t train_loss: 0.002130 took: 1.43s\n",
      "Epoch 4, 60% \t train_loss: 0.002157 took: 1.50s\n",
      "Epoch 4, 70% \t train_loss: 0.004703 took: 1.44s\n",
      "Epoch 4, 80% \t train_loss: 0.002827 took: 1.42s\n",
      "Epoch 4, 90% \t train_loss: 0.001652 took: 1.43s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 83.69s\n",
      "Epoch 5, 10% \t train_loss: 0.001411 took: 3.08s\n",
      "Epoch 5, 20% \t train_loss: 0.001457 took: 1.37s\n",
      "Epoch 5, 30% \t train_loss: 0.004526 took: 1.42s\n",
      "Epoch 5, 40% \t train_loss: 0.001649 took: 1.35s\n",
      "Epoch 5, 50% \t train_loss: 0.002328 took: 1.31s\n",
      "Epoch 5, 60% \t train_loss: 0.001171 took: 1.44s\n",
      "Epoch 5, 70% \t train_loss: 0.001090 took: 1.38s\n",
      "Epoch 5, 80% \t train_loss: 0.000886 took: 1.40s\n",
      "Epoch 5, 90% \t train_loss: 0.001369 took: 1.34s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 103.29s\n",
      "Epoch 6, 10% \t train_loss: 0.000761 took: 2.91s\n",
      "Epoch 6, 20% \t train_loss: 0.000735 took: 1.44s\n",
      "Epoch 6, 30% \t train_loss: 0.001163 took: 1.41s\n",
      "Epoch 6, 40% \t train_loss: 0.001697 took: 1.45s\n",
      "Epoch 6, 50% \t train_loss: 0.001834 took: 1.45s\n",
      "Epoch 6, 60% \t train_loss: 0.000557 took: 1.41s\n",
      "Epoch 6, 70% \t train_loss: 0.000650 took: 1.40s\n",
      "Epoch 6, 80% \t train_loss: 0.000688 took: 1.45s\n",
      "Epoch 6, 90% \t train_loss: 0.000575 took: 1.48s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 123.22s\n",
      "Epoch 7, 10% \t train_loss: 0.000449 took: 3.21s\n",
      "Epoch 7, 20% \t train_loss: 0.000501 took: 1.54s\n",
      "Epoch 7, 30% \t train_loss: 0.000432 took: 1.40s\n",
      "Epoch 7, 40% \t train_loss: 0.000694 took: 1.35s\n",
      "Epoch 7, 50% \t train_loss: 0.000336 took: 1.38s\n",
      "Epoch 7, 60% \t train_loss: 0.000353 took: 1.38s\n",
      "Epoch 7, 70% \t train_loss: 0.001330 took: 1.42s\n",
      "Epoch 7, 80% \t train_loss: 0.000304 took: 1.31s\n",
      "Epoch 7, 90% \t train_loss: 0.000254 took: 1.34s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 143.13s\n",
      "Epoch 8, 10% \t train_loss: 0.000331 took: 2.75s\n",
      "Epoch 8, 20% \t train_loss: 0.000279 took: 1.57s\n",
      "Epoch 8, 30% \t train_loss: 0.000324 took: 1.40s\n",
      "Epoch 8, 40% \t train_loss: 0.000383 took: 1.44s\n",
      "Epoch 8, 50% \t train_loss: 0.000342 took: 1.35s\n",
      "Epoch 8, 60% \t train_loss: 0.000408 took: 1.38s\n",
      "Epoch 8, 70% \t train_loss: 0.000190 took: 1.48s\n",
      "Epoch 8, 80% \t train_loss: 0.000204 took: 1.43s\n",
      "Epoch 8, 90% \t train_loss: 0.000209 took: 1.40s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 162.97s\n",
      "Epoch 9, 10% \t train_loss: 0.000127 took: 2.71s\n",
      "Epoch 9, 20% \t train_loss: 0.000165 took: 1.47s\n",
      "Epoch 9, 30% \t train_loss: 0.000230 took: 1.35s\n",
      "Epoch 9, 40% \t train_loss: 0.000210 took: 1.39s\n",
      "Epoch 9, 50% \t train_loss: 0.000201 took: 1.41s\n",
      "Epoch 9, 60% \t train_loss: 0.000163 took: 1.35s\n",
      "Epoch 9, 70% \t train_loss: 0.000133 took: 1.42s\n",
      "Epoch 9, 80% \t train_loss: 0.000115 took: 1.42s\n",
      "Epoch 9, 90% \t train_loss: 0.000287 took: 1.41s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 182.48s\n",
      "Epoch 10, 10% \t train_loss: 0.000094 took: 3.38s\n",
      "Epoch 10, 20% \t train_loss: 0.000083 took: 1.49s\n",
      "Epoch 10, 30% \t train_loss: 0.000081 took: 1.43s\n",
      "Epoch 10, 40% \t train_loss: 0.000105 took: 1.35s\n",
      "Epoch 10, 50% \t train_loss: 0.000099 took: 1.41s\n",
      "Epoch 10, 60% \t train_loss: 0.000111 took: 1.42s\n",
      "Epoch 10, 70% \t train_loss: 0.000110 took: 1.42s\n",
      "Epoch 10, 80% \t train_loss: 0.000084 took: 1.38s\n",
      "Epoch 10, 90% \t train_loss: 0.000139 took: 1.40s\n",
      "Validation loss = 0.00\n",
      "Training finished, took 203.09s\n",
      "[[  1.03380000e+04   3.00000000e+00]\n",
      " [  1.00000000e+03   2.80000000e+03]]\n",
      "[[  1.03380000e+04   3.00000000e+00]\n",
      " [  1.00000000e+03   2.80000000e+03]]\n",
      "accuracy:  0.929071494237\n",
      "sensitivity: 0.736842105263\n",
      "specificity: 0.99970989266\n",
      "precision: 0.998929718159\n",
      "F1: 0.848099348781\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "k_fold = 19\n",
    "train_sampler_list, test_sampler_list, train = split_train_test_kfold(train_percent, k_fold=k_fold)\n",
    "\n",
    "cm = np.zeros((2,2))\n",
    "for train_sampler, test_sampler in zip(train_sampler_list, test_sampler_list):\n",
    "    CNN = SimpleCNN()\n",
    "    trainNet(CNN, train_sampler, batch_size=4, n_epochs=10, learning_rate=1e-6)#1e-6\n",
    "    cm += calculate_cm(test_sampler)\n",
    "    print(cm)\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "sen = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "prec = tp/(tp+fp)\n",
    "F1 = 2 * (prec * sen) / (prec + sen)\n",
    "\n",
    "print(cm)\n",
    "print('accuracy: ',acc)\n",
    "print('sensitivity:', sen)\n",
    "print('specificity:', spec)\n",
    "print('precision:', prec)\n",
    "print('F1:', F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.03380000e+04   3.00000000e+00]\n",
      " [  1.00000000e+03   2.80000000e+03]]\n",
      "accuracy:  0.998747470855\n",
      "sensitivity: 0.736842105263\n",
      "specificity: 0.99970989266\n",
      "precision: 0.903225806452\n",
      "F1: 0.811594202899\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "fn=fn/100\n",
    "tp=tp/100\n",
    "acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "sen = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "prec = tp/(tp+fp)\n",
    "F1 = 2 * (prec * sen) / (prec + sen)\n",
    "\n",
    "print(cm)\n",
    "print('accuracy: ',acc)\n",
    "print('sensitivity:', sen)\n",
    "print('specificity:', spec)\n",
    "print('precision:', prec)\n",
    "print('F1:', F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cm(test_sampler):\n",
    "    test_loader = torch.utils.data.DataLoader(train, sampler=test_sampler, batch_size=2)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data          \n",
    "            outputs = CNN(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            for x,y in zip(predicted, labels):\n",
    "                y_true.append(int(y.numpy()))\n",
    "                y_pred.append(int(x.numpy()))\n",
    "    return confusion_matrix(np.array(y_true), np.array(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99 %\n",
      "[[1551    1]\n",
      " [   2    4]]\n",
      "accuracy:  0.998074454429\n",
      "sensitivity: 0.666666666667\n",
      "specificity: 0.999355670103\n",
      "precision: 0.8\n",
      "F1: 0.727272727273\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(np.array(y_true), np.array(y_pred))\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "cm = np.zeros((2,2))\n",
    "total=0\n",
    "correct=0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data          \n",
    "        outputs = CNN(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        for x,y in zip(predicted, labels):\n",
    "            y_true.append(int(y.numpy()))\n",
    "            y_pred.append(int(x.numpy()))\n",
    "            \n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "cm = confusion_matrix(np.array(y_true), np.array(y_pred))\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "sen = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "prec = tp/(tp+fp)\n",
    "F1 = 2 * (prec * sen) / (prec + sen)\n",
    "\n",
    "print(cm)\n",
    "print('accuracy: ',acc)\n",
    "print('sensitivity:', sen)\n",
    "print('specificity:', spec)\n",
    "print('precision:', prec)\n",
    "print('F1:', F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "limit = 10e10\n",
    "for i, data in enumerate(test_loader):\n",
    "    if i > limit:\n",
    "        break\n",
    "    images, labels = data\n",
    "    for x,y in zip(images,labels):\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        outputs = CNN(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = predicted.numpy()[0]\n",
    "        if y==1:\n",
    "            x=np.mean(x,axis=0)\n",
    "\n",
    "            plt.pcolormesh(np.arange(x.shape[1])*0.875,np.arange(x.shape[0]),x)\n",
    "            plt.ylabel('Frequency [Hz]')\n",
    "            plt.xlabel('Time [sec]')\n",
    "            plt.colorbar()\n",
    "            if y:\n",
    "                plt.title('Seizure Spectrogram (dB)')\n",
    "            else:\n",
    "                plt.title('Normal Spectrogram (dB)')\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
