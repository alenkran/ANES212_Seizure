{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test CNN on spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0d39ccc7b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize data into something useable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_kfold(feature_filename, target_filename, k_fold=1):\n",
    "    features_og = np.load(feature_filename)\n",
    "    targets_og = np.load(target_filename)\n",
    "    print(sum(targets_og),targets_og.shape)\n",
    "\n",
    "    # Split data into train and test\n",
    "    split = np.arange(len(targets_og))\n",
    "    np.random.shuffle(split)\n",
    "    features_og = features_og[split]\n",
    "    targets_og = targets_og[split]\n",
    "    features_og_0 = features_og[np.where(targets_og==0)[0]]\n",
    "    features_og_1 = features_og[np.where(targets_og==1)[0]]\n",
    "    targets_og_0 = targets_og[np.where(targets_og==0)[0]]\n",
    "    targets_og_1 = targets_og[np.where(targets_og==1)[0]]\n",
    "    N_0 = len(targets_og_0)\n",
    "    N_1 = len(targets_og_1)\n",
    "    \n",
    "    features_og_train = np.vstack([features_og_0,features_og_1])\n",
    "    targets_og_train = np.vstack([targets_og_0,targets_og_1])\n",
    "    sample_list = []\n",
    "    \n",
    "    num_seizure_per = float(N_1)//k_fold\n",
    "    num_remainder = float(N_1) % k_fold\n",
    "    for i in range(k_fold):\n",
    "        if (i+1)==k_fold:\n",
    "            temp1 = np.arange(i*N_0//k_fold,N_0, dtype=np.int64)\n",
    "            temp2 = np.arange(i*N_1//k_fold,N_1, dtype=np.int64)+N_0\n",
    "        else:\n",
    "            temp1 = np.arange(i*N_0//k_fold,(i+1)*N_0//k_fold, dtype=np.int64)\n",
    "            temp2 = np.arange(i*N_1//k_fold,(i+1)*N_1//k_fold, dtype=np.int64)+N_0\n",
    "        temp2 = np.repeat(temp2,250)\n",
    "        sample_list.append(np.hstack([temp1,temp2]))\n",
    "    \n",
    "    train_sampler_list = []\n",
    "    test_sampler_list = []\n",
    "    for i in range(k_fold):\n",
    "        temp = np.where(np.arange(len(sample_list), dtype=np.int64) != i)[0]\n",
    "        train =  np.hstack([sample_list[x] for x in temp])\n",
    "        test = sample_list[i]\n",
    "        train_sampler_list.append(SubsetRandomSampler(train))\n",
    "        test_sampler_list.append(SubsetRandomSampler(test))\n",
    "        \n",
    "    # Convert data to tensor dataset\n",
    "    features = torch.from_numpy(features_og_train).float()\n",
    "    targets = torch.from_numpy(targets_og_train).long()\n",
    "    targets = torch.squeeze(targets)\n",
    "    train = data_utils.TensorDataset(features, targets)\n",
    "    \n",
    "    return train_sampler_list, test_sampler_list, train\n",
    "    \n",
    "def split_train_test(train_percent, k_fold=0):\n",
    "    features_og = np.load('features_nonsliding_ch.npy')\n",
    "    targets_og = np.load('targets_nonsliding_ch.npy')\n",
    "    print(sum(targets_og),targets_og.shape)\n",
    "\n",
    "    # Split data into train and test\n",
    "    split = np.arange(len(targets_og))\n",
    "    np.random.shuffle(split)\n",
    "    features_og = features_og[split]\n",
    "    targets_og = targets_og[split]\n",
    "    features_og_0 = features_og[np.where(targets_og==0)[0]]\n",
    "    features_og_1 = features_og[np.where(targets_og==1)[0]]\n",
    "    targets_og_0 = targets_og[np.where(targets_og==0)[0]]\n",
    "    targets_og_1 = targets_og[np.where(targets_og==1)[0]]\n",
    "    N_0 = len(targets_og_0)\n",
    "    N_1 = len(targets_og_1)\n",
    "    \n",
    "    features_og_train = np.vstack([features_og_0[:int(train_percent*N_0)],features_og_1[:int(train_percent*N_1)]])\n",
    "    targets_og_train = np.vstack([targets_og_0[:int(train_percent*N_0)],targets_og_1[:int(train_percent*N_1)]])\n",
    "    features_og_test = np.vstack([features_og_0[int(train_percent*N_0):],features_og_1[int(train_percent*N_1):]])\n",
    "    targets_og_test = np.vstack([targets_og_0[int(train_percent*N_0):],targets_og_1[int(train_percent*N_1):]])\n",
    "    print(sum(targets_og_train), sum(targets_og_test))\n",
    "    \n",
    "\n",
    "    # Balance dataset\n",
    "    # ~1/4000 seizure events\n",
    "    idx = np.hstack([np.where(targets_og_train == 0)[0], \n",
    "                     np.repeat(np.where(targets_og_train == 1)[0], 100)]) # Oversample\n",
    "    features = features_og_train[idx]\n",
    "    targets = targets_og_train[idx]\n",
    "\n",
    "    # Convert data to tensor dataset\n",
    "    features = torch.from_numpy(features).float()\n",
    "    targets = torch.from_numpy(targets).long()\n",
    "    targets = torch.squeeze(targets)\n",
    "    train = data_utils.TensorDataset(features, targets)\n",
    "\n",
    "    N = features.size()[0]\n",
    "    sample_list = np.arange(N, dtype=np.int64)\n",
    "    np.random.shuffle(sample_list)\n",
    "    percent_train = 1.0\n",
    "\n",
    "    #Training\n",
    "    n_training_samples = int(N*percent_train)\n",
    "    train_sampler = SubsetRandomSampler(sample_list[:n_training_samples])\n",
    "\n",
    "    #Validation\n",
    "    val_sampler = SubsetRandomSampler(sample_list[:n_training_samples])\n",
    "\n",
    "    #Test data\n",
    "    features = torch.from_numpy(features_og_test).float()\n",
    "    targets = torch.from_numpy(targets_og_test).long()\n",
    "    targets = torch.squeeze(targets)\n",
    "    test = data_utils.TensorDataset(features, targets)\n",
    "    return train, test, train_sampler, val_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(torch.nn.Module):\n",
    "    \n",
    "    #Our batch shape for input x is (3, 32, 32)\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        #Input channels = 1, output channels = 18\n",
    "        self.conv1 = torch.nn.Conv2d(23, 18, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        #4608 input features, 64 output features (see sizing flow below)\n",
    "        self.fc1 = torch.nn.Linear(18 * 16 * 16, 64)\n",
    "        \n",
    "        #64 input features, 10 output features for our 10 defined classes\n",
    "        self.fc2 = torch.nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #Computes the activation of the first convolution\n",
    "        #Size changes from (3, 32, 32) to (18, 32, 32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        #Size changes from (18, 32, 32) to (18, 16, 16)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #Reshape data to input to the input layer of the neural net\n",
    "        #Size changes from (18, 16, 16) to (1, 4608)\n",
    "        #Recall that the -1 infers this dimension from the other given dimension\n",
    "        x = x.view(-1, 18 * 16 *16)\n",
    "        \n",
    "        #Computes the activation of the first fully connected layer\n",
    "        #Size changes from (1, 4608) to (1, 64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #Computes the second fully connected layer (activation applied later)\n",
    "        #Size changes from (1, 64) to (1, 10)\n",
    "        x = self.fc2(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader takes in a dataset and a sampler for loading (num_workers deals with system level memory) \n",
    "def get_train_loader(batch_size, train_sampler):\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers=2)\n",
    "    return(train_loader)\n",
    "\n",
    "#Test and validation loaders have constant batch sizes, so we can define them directly\n",
    "\n",
    "def trainNet(net, train_sampler, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = get_train_loader(batch_size, train_sampler)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.data[0]\n",
    "            total_train_loss += loss_size.data[0]\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.6f} took: {:.2f}s\".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        for inputs, labels in val_loader:            \n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data[0]\n",
    "            \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / max(1,len(val_loader))))\n",
    "        \n",
    "\n",
    "        print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
    "        \n",
    "def calculate_cm(test_sampler):\n",
    "    test_loader = torch.utils.data.DataLoader(train, sampler=test_sampler, batch_size=2)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data          \n",
    "            outputs = CNN(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            for x,y in zip(predicted, labels):\n",
    "                y_true.append(int(y.numpy()))\n",
    "                y_pred.append(int(x.numpy()))\n",
    "    return confusion_matrix(np.array(y_true), np.array(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 38.] (10379, 1)\n",
      "(10379, 23, 64, 16)\n",
      "(10379, 1)\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 32\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Christian/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:55: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/Christian/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:56: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 10% \t train_loss: 4.211040 took: 1.45s\n",
      "Epoch 1, 20% \t train_loss: 2.013771 took: 0.61s\n",
      "Epoch 1, 30% \t train_loss: 1.198637 took: 0.61s\n",
      "Epoch 1, 40% \t train_loss: 0.809367 took: 0.63s\n",
      "Epoch 1, 50% \t train_loss: 0.636083 took: 0.61s\n",
      "Epoch 1, 60% \t train_loss: 0.504578 took: 0.61s\n",
      "Epoch 1, 70% \t train_loss: 0.462832 took: 0.64s\n",
      "Epoch 1, 80% \t train_loss: 0.400315 took: 0.62s\n",
      "Epoch 1, 90% \t train_loss: 0.370664 took: 0.61s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Christian/anaconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:75: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss = 0.31\n",
      "Training finished, took 11.34s\n",
      "Epoch 2, 10% \t train_loss: 0.298562 took: 1.46s\n",
      "Epoch 2, 20% \t train_loss: 0.277432 took: 0.60s\n",
      "Epoch 2, 30% \t train_loss: 0.241240 took: 0.61s\n",
      "Epoch 2, 40% \t train_loss: 0.215489 took: 0.61s\n",
      "Epoch 2, 50% \t train_loss: 0.199394 took: 0.64s\n",
      "Epoch 2, 60% \t train_loss: 0.186847 took: 0.61s\n",
      "Epoch 2, 70% \t train_loss: 0.163653 took: 0.61s\n",
      "Epoch 2, 80% \t train_loss: 0.161275 took: 0.61s\n",
      "Epoch 2, 90% \t train_loss: 0.155828 took: 0.61s\n",
      "Validation loss = 0.13\n",
      "Training finished, took 22.64s\n",
      "Epoch 3, 10% \t train_loss: 0.123987 took: 1.43s\n",
      "Epoch 3, 20% \t train_loss: 0.122875 took: 0.61s\n",
      "Epoch 3, 30% \t train_loss: 0.113584 took: 0.62s\n",
      "Epoch 3, 40% \t train_loss: 0.113597 took: 0.61s\n",
      "Epoch 3, 50% \t train_loss: 0.107041 took: 0.61s\n",
      "Epoch 3, 60% \t train_loss: 0.098871 took: 0.62s\n",
      "Epoch 3, 70% \t train_loss: 0.088949 took: 0.61s\n",
      "Epoch 3, 80% \t train_loss: 0.084781 took: 0.61s\n",
      "Epoch 3, 90% \t train_loss: 0.084938 took: 0.61s\n",
      "Validation loss = 0.08\n",
      "Training finished, took 33.87s\n",
      "Epoch 4, 10% \t train_loss: 0.073283 took: 1.45s\n",
      "Epoch 4, 20% \t train_loss: 0.074582 took: 0.60s\n",
      "Epoch 4, 30% \t train_loss: 0.068598 took: 0.61s\n",
      "Epoch 4, 40% \t train_loss: 0.061187 took: 0.61s\n",
      "Epoch 4, 50% \t train_loss: 0.056159 took: 0.65s\n",
      "Epoch 4, 60% \t train_loss: 0.064540 took: 0.61s\n",
      "Epoch 4, 70% \t train_loss: 0.065411 took: 0.62s\n",
      "Epoch 4, 80% \t train_loss: 0.059957 took: 0.61s\n",
      "Epoch 4, 90% \t train_loss: 0.055723 took: 0.61s\n",
      "Validation loss = 0.05\n",
      "Training finished, took 45.15s\n",
      "Epoch 5, 10% \t train_loss: 0.051025 took: 1.44s\n",
      "Epoch 5, 20% \t train_loss: 0.044065 took: 0.61s\n",
      "Epoch 5, 30% \t train_loss: 0.046819 took: 0.60s\n",
      "Epoch 5, 40% \t train_loss: 0.039243 took: 0.61s\n",
      "Epoch 5, 50% \t train_loss: 0.040483 took: 0.61s\n",
      "Epoch 5, 60% \t train_loss: 0.036960 took: 0.61s\n",
      "Epoch 5, 70% \t train_loss: 0.041055 took: 0.61s\n",
      "Epoch 5, 80% \t train_loss: 0.048496 took: 0.62s\n",
      "Epoch 5, 90% \t train_loss: 0.039602 took: 0.60s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 56.32s\n",
      "Epoch 6, 10% \t train_loss: 0.039562 took: 1.45s\n",
      "Epoch 6, 20% \t train_loss: 0.032373 took: 0.60s\n",
      "Epoch 6, 30% \t train_loss: 0.033274 took: 0.61s\n",
      "Epoch 6, 40% \t train_loss: 0.031664 took: 0.61s\n",
      "Epoch 6, 50% \t train_loss: 0.027159 took: 0.61s\n",
      "Epoch 6, 60% \t train_loss: 0.026070 took: 0.62s\n",
      "Epoch 6, 70% \t train_loss: 0.027175 took: 0.61s\n",
      "Epoch 6, 80% \t train_loss: 0.031866 took: 0.60s\n",
      "Epoch 6, 90% \t train_loss: 0.031986 took: 0.63s\n",
      "Validation loss = 0.03\n",
      "Training finished, took 67.58s\n",
      "Epoch 7, 10% \t train_loss: 0.028809 took: 1.45s\n",
      "Epoch 7, 20% \t train_loss: 0.022386 took: 0.62s\n",
      "Epoch 7, 30% \t train_loss: 0.024742 took: 0.62s\n",
      "Epoch 7, 40% \t train_loss: 0.024287 took: 0.61s\n",
      "Epoch 7, 50% \t train_loss: 0.022581 took: 0.61s\n",
      "Epoch 7, 60% \t train_loss: 0.021602 took: 0.61s\n",
      "Epoch 7, 70% \t train_loss: 0.022808 took: 0.62s\n",
      "Epoch 7, 80% \t train_loss: 0.020195 took: 0.61s\n",
      "Epoch 7, 90% \t train_loss: 0.022974 took: 0.60s\n",
      "Validation loss = 0.02\n",
      "Training finished, took 78.85s\n",
      "Epoch 8, 10% \t train_loss: 0.022234 took: 1.46s\n",
      "Epoch 8, 20% \t train_loss: 0.017120 took: 0.60s\n",
      "Epoch 8, 30% \t train_loss: 0.017011 took: 0.62s\n",
      "Epoch 8, 40% \t train_loss: 0.017343 took: 0.61s\n",
      "Epoch 8, 50% \t train_loss: 0.022757 took: 0.64s\n",
      "Epoch 8, 60% \t train_loss: 0.013738 took: 0.61s\n",
      "Epoch 8, 70% \t train_loss: 0.015994 took: 0.61s\n",
      "Epoch 8, 80% \t train_loss: 0.019021 took: 0.62s\n",
      "Epoch 8, 90% \t train_loss: 0.015142 took: 0.61s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 90.14s\n",
      "Epoch 9, 10% \t train_loss: 0.017009 took: 1.48s\n",
      "Epoch 9, 20% \t train_loss: 0.014163 took: 0.63s\n",
      "Epoch 9, 30% \t train_loss: 0.013650 took: 0.61s\n",
      "Epoch 9, 40% \t train_loss: 0.013022 took: 0.61s\n",
      "Epoch 9, 50% \t train_loss: 0.012505 took: 0.61s\n",
      "Epoch 9, 60% \t train_loss: 0.014354 took: 0.66s\n",
      "Epoch 9, 70% \t train_loss: 0.014612 took: 0.64s\n",
      "Epoch 9, 80% \t train_loss: 0.011682 took: 0.62s\n",
      "Epoch 9, 90% \t train_loss: 0.012567 took: 0.61s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 101.56s\n",
      "Epoch 10, 10% \t train_loss: 0.011080 took: 1.46s\n",
      "Epoch 10, 20% \t train_loss: 0.011213 took: 0.60s\n",
      "Epoch 10, 30% \t train_loss: 0.010271 took: 0.61s\n",
      "Epoch 10, 40% \t train_loss: 0.011916 took: 0.61s\n",
      "Epoch 10, 50% \t train_loss: 0.009961 took: 0.61s\n",
      "Epoch 10, 60% \t train_loss: 0.009531 took: 0.61s\n",
      "Epoch 10, 70% \t train_loss: 0.010906 took: 0.62s\n",
      "Epoch 10, 80% \t train_loss: 0.010283 took: 0.61s\n",
      "Epoch 10, 90% \t train_loss: 0.011939 took: 0.61s\n",
      "Validation loss = 0.01\n",
      "Training finished, took 112.86s\n",
      "[[ 2577.     8.]\n",
      " [  750.  1500.]]\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size= 32\n",
      "epochs= 10\n",
      "learning_rate= 1e-06\n",
      "==============================\n",
      "Epoch 1, 10% \t train_loss: 4.978230 took: 1.42s\n",
      "Epoch 1, 20% \t train_loss: 2.556051 took: 0.62s\n",
      "Epoch 1, 30% \t train_loss: 1.312615 took: 0.65s\n",
      "Epoch 1, 40% \t train_loss: 0.783086 took: 0.64s\n",
      "Epoch 1, 50% \t train_loss: 0.698050 took: 0.63s\n",
      "Epoch 1, 61% \t train_loss: 0.550686 took: 0.61s\n",
      "Epoch 1, 71% \t train_loss: 0.443801 took: 0.61s\n",
      "Epoch 1, 81% \t train_loss: 0.425496 took: 0.61s\n",
      "Epoch 1, 91% \t train_loss: 0.378314 took: 0.62s\n",
      "Validation loss = 0.30\n",
      "Training finished, took 11.19s\n",
      "Epoch 2, 10% \t train_loss: 0.289602 took: 1.45s\n",
      "Epoch 2, 20% \t train_loss: 0.291691 took: 0.61s\n",
      "Epoch 2, 30% \t train_loss: 0.233893 took: 0.63s\n",
      "Epoch 2, 40% \t train_loss: 0.213160 took: 0.62s\n",
      "Epoch 2, 50% \t train_loss: 0.192946 took: 0.62s\n",
      "Epoch 2, 61% \t train_loss: 0.166279 took: 0.63s\n",
      "Epoch 2, 71% \t train_loss: 0.168799 took: 0.62s\n",
      "Epoch 2, 81% \t train_loss: 0.159197 took: 0.63s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1347:\n",
      "Process Process-1348:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/Christian/anaconda2/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8a6cca4b6962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mCNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrainNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# 2,5,1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mcm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalculate_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-77bffaf0699d>\u001b[0m in \u001b[0;36mtrainNet\u001b[0;34m(net, train_sampler, batch_size, n_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mloss_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "k_fold = 4\n",
    "batch_size = 32\n",
    "feature_filename = 'features_nonsliding_ch.npy' #stft vs spect\n",
    "target_filename = 'targets_nonsliding_ch.npy'\n",
    "\n",
    "train_sampler_list, test_sampler_list, train = split_train_test_kfold(feature_filename, target_filename, k_fold=k_fold)\n",
    "\n",
    "cm = np.zeros((2,2))\n",
    "for train_sampler, test_sampler in zip(train_sampler_list, test_sampler_list):\n",
    "    CNN = SimpleCNN()\n",
    "    val_loader = torch.utils.data.DataLoader(train, batch_size=32, sampler=train_sampler, num_workers=2)\n",
    "    trainNet(CNN, train_sampler, batch_size=batch_size, n_epochs=10, learning_rate=1e-6)# 2,5,1e-6 \n",
    "    cm += calculate_cm(test_sampler)\n",
    "    print(cm)\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "fn=fn/250\n",
    "tp=tp/250\n",
    "acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "sen = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "prec = tp/(tp+fp)\n",
    "F1 = 2 * (prec * sen) / (prec + sen)\n",
    "\n",
    "print(cm)\n",
    "print('accuracy: ',acc)\n",
    "print('sensitivity:', sen)\n",
    "print('specificity:', spec)\n",
    "print('precision:', prec)\n",
    "print('F1:', F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "fn=fn/250\n",
    "tp=tp/250\n",
    "acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "sen = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "prec = tp/(tp+fp)\n",
    "F1 = 2 * (prec * sen) / (prec + sen)\n",
    "\n",
    "print(cm)\n",
    "print('accuracy: ',acc)\n",
    "print('sensitivity:', sen)\n",
    "print('specificity:', spec)\n",
    "print('precision:', prec)\n",
    "print('F1:', F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.array(y_true), np.array(y_pred))\n",
    "    \n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "cm = np.zeros((2,2))\n",
    "total=0\n",
    "correct=0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data          \n",
    "        outputs = CNN(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        for x,y in zip(predicted, labels):\n",
    "            y_true.append(int(y.numpy()))\n",
    "            y_pred.append(int(x.numpy()))\n",
    "            \n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "cm = confusion_matrix(np.array(y_true), np.array(y_pred))\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "acc = (tp+tn)/(tn+fp+fn+tp)\n",
    "sen = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "prec = tp/(tp+fp)\n",
    "F1 = 2 * (prec * sen) / (prec + sen)\n",
    "\n",
    "print(cm)\n",
    "print('accuracy: ',acc)\n",
    "print('sensitivity:', sen)\n",
    "print('specificity:', spec)\n",
    "print('precision:', prec)\n",
    "print('F1:', F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize CNN running on an actual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {}\n",
    "df['1'] = a # Figure out how to do python 2 wrapp\n",
    "width = 8\n",
    "\n",
    "feature, target = df_to_spectrogram_FT(df, sliding=True, avg=True, sliding_ft=False, width=8, stft=True)\n",
    "\n",
    "# Convert feature to tensor\n",
    "# Run it into the CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some example of CNN labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 10e10\n",
    "for i, data in enumerate(test_loader):\n",
    "    if i > limit:\n",
    "        break\n",
    "    images, labels = data\n",
    "    for x,y in zip(images,labels):\n",
    "        x = x.numpy()\n",
    "        y = y.numpy()\n",
    "        outputs = CNN(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted = predicted.numpy()[0]\n",
    "        if y==1:\n",
    "            x=np.mean(x,axis=0)\n",
    "\n",
    "            plt.pcolormesh(np.arange(x.shape[1])*0.875,np.arange(x.shape[0]),x)\n",
    "            plt.ylabel('Frequency [Hz]')\n",
    "            plt.xlabel('Time [sec]')\n",
    "            plt.colorbar()\n",
    "            if y:\n",
    "                plt.title('Seizure Spectrogram (dB)')\n",
    "            else:\n",
    "                plt.title('Normal Spectrogram (dB)')\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
